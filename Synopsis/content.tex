\pdfbookmark{Общая характеристика работы}{characteristic}             % Закладка pdf
\section*{Общая характеристика работы}

\newcommand{\actuality}{\pdfbookmark[1]{Актуальность}{actuality}\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\pdfbookmark[1]{Разработанность темы}{progress}\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\pdfbookmark[1]{Цели}{aim}\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\pdfbookmark[1]{Задачи}{tasks}\underline{\textbf{\tasksTXT}}}
\newcommand{\aimtasks}{\pdfbookmark[1]{Цели и задачи}{aimtasks}\aimtasksTXT}
\newcommand{\novelty}{\pdfbookmark[1]{Научная новизна}{novelty}\underline{\textbf{\noveltyTXT}}}
\newcommand{\influence}{\pdfbookmark[1]{Практическая значимость}{influence}\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\pdfbookmark[1]{Методология и методы исследования}{methods}\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\pdfbookmark[1]{Положения, выносимые на защиту}{defpositions}\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\pdfbookmark[1]{Достоверность}{reliability}\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\pdfbookmark[1]{Апробация}{probation}\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\pdfbookmark[1]{Личный вклад}{contribution}\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\pdfbookmark[1]{Публикации}{publications}\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.

\pdfbookmark{Содержание работы}{description}                          % Закладка pdf
\section*{Содержание работы}
Во \underline{\textbf{введении}} обоснована актуальность диссертационной работы, сформулирована цель и аргументирована научная новизна исследований и представлены выносимые на защиту положения. Описываются распространенные постановки задачи минимизации, вводится понятие оракульной сложности, упоминаются возможные несоответствия между практическим временем работы и получаемой оценкой. Также описываются причины, благодаря которым, сейчас наиболее популярны методы градиентного типа. 

\underline{\textbf{Первая глава}} посвящена обзору литературы по теме выпуклой оптимизации. Описываются различные постановки задачи минимизации и ряд известных методов, широко использующихся на практике. Описываются классические градиентные методы и адаптивные квазиньютоновские. Описывается пример постановки и работы с задачей распределенной оптимизации, а также со специальной функцией, не обладающей выпуклостью, описывающей энергию молекулы белка. В качестве иллюстрации приводится опыт работы с нестандартным невыпуклым функционалом, для которого нет хороших вычислительных гарантий, что позволяет применить методы нулевого порядка и провести некоторый технический анализ.

\underline{\textbf{В параграфе 1.1}} приводится обзор методов первого порядка. Вводится общая постановка задачи, приводятся оптимальные оценки важных классов задач. Также рассматриваются популярные постановки задач, например задача минимизации квадратичной формы. Проводятся пояснения о стандартных способах улучшения оценок скорости сходимости для методов, таких как механизм рестартов, который в дальнейшем будет использован в 3й главе. Упоминаются ограничения и <<тонкие>> места, возникающие при работе с квазиньютоновскими методами, что имеет существенное значение в анализе задачи минимизации энергии белка. Многие из представленных в данной главе методов будут использованы для анализа функционала упомянутой задачи.

\underline{\textbf{Параграф 1.2}} описывает постановку задачи для распределенной минимизации
  \begin{equation} \label{raspr_task}
    \min_{x \in \mathbb{R}^n}\left\{f(x)=\frac{1}{d} \sum_{i=1}^d f_i(x)\right\},
  \end{equation}
  где $d$ соответствует количеству исполнителей или узлов или вычислителей. 

  Во введении уже обсуждалась связь эффективности применения метода и используемого исполнительного устройства или топологии устройств. В связи с ростом популярности машинного обучения и значительного времени, которое требуется для обучения современных моделей (дни, недели), задачи параллелизации и распределения исполнения по различным устройствам для эффективного их использования выходят на первый план.

  Будем предполагать, что информация, касающаяся функции $f_i$, хранится только на узле $i$. Также зачастую каждую из $f_i$ рассматривают как
  $$
    f_i(x) = \frac{1}{m} \sum_{j=1}^m f_{ij}(x).
  $$ 
  Значимым параметром, влияющим на скорость сходимости, является дороговизна пересылки информации между узлами. Данный аспект является так называемым бутылочным горлышком для подобного типа задач. Потому методы, нацеленные на распределенную постановку, как правило, стараются заменить глобальный шаг (обновление всех компонент вектора $x$) на локальные аналоги. Это основная идея, которая лежит в основе таких методов как Local-SGD \cite{Stich2019LocalSC}. Также популярным подходом является уменьшение передаваемой информации между узлами путем выбора наиболее значимых компонент или более сложных механизмов компрессии. Подобные подходы используются в работах \cite{qlsgd, qsgd, err_fdbk}. Отметим, что многие методы предполагают полносвязную топологию доступных узлов или же star-топологию, когда есть один вычисляющий центр и зависимые дополнительные мощности. 
  В работе \cite{GorbunovKMR20}, например, предлагается следующий метод
  $$
  \begin{aligned} 
    x^{k+1} &=x^k-\frac{1}{d} \sum_{i=1}^d v_i^k, \\ 
    e_i^{k+1} &=e_i^k + \gamma g_i^k - v_i^k, \quad i=1,2, \ldots, d . 
  \end{aligned}
  $$
  Здесь $x^k$ отвечает за значение аргумента на итерации $k$, $v_i^k$ --- это результат вычисленный на узле $i$ на итерации $k$, $g_i^k$ --- это оценка $\nabla f_i(x^k)$ полученная узлом $i$, $\gamma$ --- это фиксированный размер шага и $e_i^k$ --- это накопленная к итерации $k$ на узле $i$ ошибка.  Подобная постановка задачи рассматривалась также и в \cite{err_fdbk}, где был проведен схожий анализ, но со своими особенностями, учитывающий отложенное вычисление градиента, компрессию и размеры обновляемой части градиента.
  В упомянутой работе функция компрессии $C()$ представлена так:
  $$
    v_i^k = C(e_i^k + \gamma g_i^k).
  $$
  Данная постановка, как и было показано в \cite{GorbunovKMR20} моими соавторами, обобщает и развивает ряд методов, таких как Error Compensated Stochastic Gradiend Descent (EC-SGD), EC-SGD-DIANA и EC-LSVRG. Теоретические результаты для данного ряда методов также были получены моими соавторами. Однако многие практические результаты были получены автором. В упомянутой статье рассматривается задача линейной регрессии для различных наборов данных
  $$
    \min_{x \in \mathbb{R}^n}\left\{ f(x) = \frac{1}{N} \sum_{i=1}^N \log \left(1+\exp \left(- y_i \cdot(A x)_i\right)\right) + \frac{\mu}{2}\|x\|^2 \right\},
  $$
  где $N$ --- это количество параметров модели, $x$ --- это веса модели, $A$ --- матрица параметров, а $y \in {\{-1,1\}}^N$ --- набор ожидаемых решений. Подобный тип задач имеет широкое распространение в машинном обучении. Важным аспектом являются используемые функции компрессии, а именно 
  \begin{enumerate}
    \item функция $TopK()$ --- $k$ компонентов, отсортированных по убыванию абсолютных значений в векторе ошибок $e_i$,
    \item $RandK()$ --- $k$ случайных компонентов,
    \item $l2-quant()$ --- выделение некоторого подпространства в $\mathbb{R}^n$ на основе вероятности, полученной при помощи нормализации входного вектора. 
  \end{enumerate}

  Очевидно, что подобные преобразования понижают допустимую точность для методов и также появляется шум, связанный с компрессией. Поскольку здесь присутствует достаточно много параметров, которые варьируются для многих методов и различных функций компрессии, то мы приведем один из тестов в качестве иллюстрации влияния компрессии на различные методы --- см. рис. \ref{compr}. Здесь mushrooms --- это наименование использованного набора данных, по горизонтальной оси отложено количество итераций. Видно, что различные методы реагируют на внедрение функции компрессии по-разному. Некоторое методы более устойчивы к компрессионному шуму и позволяют эффективно повышать точность с ростом количества итераций, а именно EC-L-SVRG-DIANA.
  \begin{figure}
    \begin{center}
      \includegraphics[height=0.24\paperheight]{mushrooms_20.png}
    \end{center}
    \caption{Компрессионный шум для различных методов.}
    \label{compr}
  \end{figure}
  В ряде задач требуется сочетания подходов, применяемых для распределенной оптимизации, и методов, использующих свойство относительной гладкости \cite{distrib_relative}. 

  К примеру, если в исходной постановке задачи распределенной оптимизации \eqref{raspr_task} $f_{ij}(x)$ являются $\mu$-сильно выпуклыми в 2-норме гладкими функциями, то можно предложить следующий дизайн градиентного спуска. Предположим, что есть централизованная архитектура с $d \ll m$ узлами. Первый узел является центральным. Это также называют star-топологией. Поместим в первый узел случайно отобранные $\tilde{m}(\widetilde{m} \ll m)$ слагаемых из суммы. Остальные слагаемые распределим по остальным узлам. Обозначим соответствующую первому узлу нормированную подсумму через $\widetilde{f_0}(x)$. Рассмотрим градиентный спуск, выполняемый на центральном узле $(k=0,1,2, \ldots)$ :
  $$
    x^{k+1}=\underset{x \in \mathbb{R}^n}{\arg \min }\left\{\left\langle\nabla f_0\left(x^k\right), x - x^k\right\rangle+L V\left(x, x^k\right)\right\},
  $$
  где
  $$
    V(y, x)=d(y)-d(x)-\langle\nabla d(x), y-x\rangle,\;\;\;d(x)=\widetilde{f_0}(x)+\frac{\gamma}{2}\|x\|_2^2, \;\;\; \gamma>0 .
  $$
  Каждая итерация такого градиентного спуска должна отвечать коммуникации центрального узла с остальными, чтобы в итоге получилось $\nabla f\left(x^k\right)$.
  Для данного метода можно показать, что после $N$ итераций справедливо
  $$
    f(\hat{x})-f_* \leqslant V\left(x_*, x^0\right)\left(1-\frac{\mu}{\mu+2 \gamma}\right)^N.
  $$
  Откуда могут быть получены соответствующие оценки, однако мы не будем углубляться в данном направлении. Аккуратное доказательство для градиентного спуска в схожей постановке представлено в \cite{distrib_relative}.

\underline{\textbf{Параграф 1.3}} содержит анализ и предпринятые шаги по работе с практической задачей, возникшей в области вычислительной биологии. Приводится опыт работы с функционалом, не обладающим достаточной выпуклостью и основанным на ряде физических свойств объекта (что показано на рис. \ref{fig1D}). 

\begin{figure}
\begin{center}
    \includegraphics[height=0.24\paperheight]{1DSearch.jpg}
\end{center}
\caption{Отсутствие выпуклости задачи минимизации OPLS force field}
\label{fig1D}
\end{figure}
     
Сам пункт имеет разбиение на ряд подпунктов которые отвечают за различные этапы анализа и работы с задачей. В первом подпункте 1.3.1 описывается проблематика задачи и мотивация для ее решения. Данная задача относится к типу задач большой размерности:  $n_{ }\left(n\sim 10^4\right)$. 

Второй подпункт 1.3.2 описывает физические основания, стоящие за структурой функционала. Энергия белка основана на взаиморасположении атомов в молекуле и их различных взаимодействиях. Каждый атом в данном случае описывается тремя пространственными координатами, а самый простой белок состоит из $100$ атомов. Что позволяет оценить размерность пространства возможных конфигураций молекулы. Для подобного функционала вычисление градиента является весьма трудоемкой задачей, что объясняется сложностью и большим количеством параметров. Выпуклость пропадает в силу специфики ряда физических взаимодействий. Также стоит отметить, что в задаче необходимо найти ближайший устойчивый локальный минимум, поскольку в естественных условиях стремление к глобальному минимуму отсутствует. 

Третий пункт 1.3.3 описывает обсуждения и мотивацию, стоящую за выбором методов. Оптимизируемый функционал может рассматриваться как гладкий только локально. Константа Липшица не является равномерно ограниченной в достаточно большой окрестности точки старта. Но в таком случае для минимизации такого функционала нет гарантий даже локальной сходимости \cite{ghadimi2015generalized}, \cite{nesterov2017random}.
   
Для решения таких задач глобальной оптимизации теория рекомендует использовать безградиентные методы типа simulated annealing или марковского поиска \cite{zhigljavsky2007stochastic}, \cite{zhigljavsky2012theory}.
Были проведены эксперименты с методами нулевого порядка (покоординатный спуск в различных вариациях). В основе предлагаемого подхода -- <<шевеление>> на каждой итерации только одного, случайно выбранного, атома при <<замороженных>> остальных. Заметим, что при изменении положения одного атома пересчет некоторых частей функционала будет стоить $\mathcal{O}\left( 1 \right)$ с точки зрения сложности вычислений, поскольку затрагивает только <<соседние>> по химическим связям атомы. Здесь под <<соседними>> имеются в виду не только непосредственные соседи, но и соседи через две и даже три химические связи. Обычно число таких <<соседей>> не больше 15. 

Поскольку задача является практической основным приоритетом было качество и скорость работы используемого метода. В итоге возникло интересное противопоставление - безградиентного метода со значительной параллелизацией и более линейного метода сопряженных градиентов. Однако во время работы с покоординатным спуском было обнаружено, что можно получить значительное ускорение при использовании гладкости вдоль каждой из координатных осей. Это было одним из сигналов, что градиентные методы могут быть успешно применены и для данной задачи. Эксперименты это и подтвердили, как итог финальная реализация основывалась на методах сопряженных градиентов.

\begin{table}[h]
\caption{Сравнение характеристик методов}
\label{tabular:timesandtenses}
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
        \hline
        \fontsize{12pt}{12pt}\selectfont {\bfseries Показатели} & {\bfseries <<Шевеление>> атома} & {\bfseries Адаптивный градиентный спуск} \\
        \hline
        \fontsize{12pt}{12pt}\selectfont Время 1 итерации, секунды    & 1.2075 & 122.9814 \\
        \hline
        \fontsize{12pt}{12pt}\selectfont Энергия 1 итерации, кДж/моль & 0.0225 & 2.7081 \\
        \hline
        \fontsize{12pt}{12pt}\selectfont $\sim\Delta$Энергии к 300 минуте, кДж/моль & $- 347$ & $- 413$ \\
        \hline
    \end{tabular}
}
\end{table}

Пункт 1.3.4 и 1.3.5 содержат детали реализации и описание проблем, возникших во время работы. Более подробно описываются модификации квазиньютоновских методов и методы одномерного поиска, адаптированные к структуре задачи.

Пункт 1.3.6 описывает результаты экспериментов, как оценивалось качество полученных решений и какие дополнительные параметры рассматривались. Ускоренный градиентный спуск и метод сопряженных градиентов показали лучшие результаты на тестовых наборах данных. Однако, отметим, что для получения данных результатов пришлось ввести ряд поправок для соответствия структуре задаче. 

\underline{\textbf{Во второй главе}} представлено исследование методов первого порядка для ряда классов вариационных неравенств с операторами, удовлетворяющими предлагаемому аналогу условия относительной сильной выпуклости с аналогом ограниченности (относительная ограниченность), а также с аналогом условия Липшица (относительная гладкость). Также приводится некоторое уточнение известной оптимальной оценки для субградиентного метода \cite{Bach_2012} на сильно-выпуклый случай. 

\underline{\textbf{В параграфе 2.1}} вводятся понятия относительной гладкости и определяется дивергенция Брэгмана. Также происходит переход от классической постановки к слабому решению вариационного неравенства. 

Для выпуклых относительно гладких задач оценки сходимости обычных (неускоренных) методов градиентного типа оптимальны с точностью до умножения на константу, не зависящую от размерности и параметров метода (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также имеющиеся в них ссылки). В работе \cite{Lu_Nesterov_2018} введено понятие относительной сильной выпуклости функции, позволившее расширить класс выпуклых оптимизационных задач, для которых можно доказать линейную скорость сходимости (сходимость со скоростью геометрической прогрессии) метода градиентного типа, причём соответствующая оценка не содержит параметров размерности задачи. В главе 2 мы развиваем этот подход и исследуем некоторые алгоритмы уже для вариационных неравенств с аналогом относительной сильной выпуклости для операторов (относительной сильной монотонностью).

Хорошо известно, что на классе липшицевых и сильно выпуклых минимизационных задач оптимальная оценка скорости сходимости достигается именно для субградиентного метода \cite{Bach_2012}. В последние годы активно исследуются задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Мы исследуем оценку скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее говоря, рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач. 

Далее, немалую популярность в работах по оптимизации получило упомянутое выше недавно предложенное понятие относительной гладкости функций (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также приведённые в них ссылки), которое позволило существенно расширить класс задач выпуклой оптимизации по сравнению со стандартным предположением о липшицевости градиента с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$ (здесь и далее $N$ --- количество итераций), которая может считаться оптимальной для такого широкого класса задач \cite{Dragomir}. В плане приложений можно отметить подход к построению методов градиентного типа для задач распределенной оптимизации с использованием относительной гладкости и относительной сильной выпуклости \cite{Hendr}. Аналоги относительной гладкости введены в последние пару лет и для более общей постановки задачи решения вариационного неравенства (см. \cite{Inex}, а также имеющиеся там ссылки) с монотонным оператором. Оказывается, что для этого класса задач можно предложить алгоритмы экстраградиентного типа с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$. 

В данной главе в большинстве пунктов рассматривается задача нахождения решения $x_*$ (также называемого слабым решением) вариационного неравенства: 
\begin{equation}\label{eq:1}
    \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
\end{equation}
где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$,
$g: Q \longrightarrow \mathbb{R}^n$. Предположим, что удовлетворяющее \eqref{eq:1} решение $x_*$ существует.
Введём следующий аналог понятия относительной сильной выпуклости функции \cite{Lu_Nesterov_2018} для вариационных неравенств.
\begin{definition}\label{DefRelStrongMonot}
    Назовём оператор $g$ относительно $\mu$-сильно монотонным, где $\mu >0$, если для всяких $x, y \in Q$ верно неравенство
        \begin{equation}\label{eq:3}
             \mu V(y, x) + \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
         \end{equation}
\end{definition}
\begin{definition}\label{DefRelBound}\cite{Main}
    Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $M$-ограниченным, где $M >0$, если для всяких $x, y \in Q$ верно неравенство
    \begin{equation}\label{rel_bound}
         \langle g(x), x - y \rangle \leq M\sqrt{2V(y,x)}.
     \end{equation}
\end{definition}
\iffalse
    Для субградиентного метода вида
    \begin{gather}\label{orig}
        x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
    \end{gather}
    известна следующая оценка скорости сходимости \cite{Bach_2012}:
    \begin{equation}\label{orig_estimation_f}
        f(\widehat{x}) - f(x_*) \leq \frac{2 M^2}{\mu (N+1)}  \; \text{  при   } \; \widehat{x} = \sum\limits_{k=1}^{N} \frac{2 k}{N (N+1)} x_k, 
    \end{equation}
    где $M$ --- константа Липщица целевой функции $f$.
    Поэтому справедлива следующая
    \begin{theorem}\label{ThmBachAdaptive}
        Пусть $f$ --- $\mu$-сильно выпуклая функция. Тогда после $N$ итераций алгоритма:
        $$
            x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
        $$
        будет верно неравенство:
        \begin{equation}\label{adaptive_estimation_f}
            f(\widehat{x}) - f(x_*) \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1},
        \end{equation}
        где
        $$
            \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
        $$
        Если $f$ ещё и $M$-липшицева при $M >0$, то
        $$
             f(\widehat{x}) - f(x) \leq \varepsilon
        $$
        после $N = \mathcal{O}(\frac{M^2}{\mu\varepsilon})$ итераций алгоритма \eqref{orig}.
    \end{theorem}

    Отметим, что если $x_*$ --- точное решение задачи минимизации $f$, то можно получить оценку скорости сходимости по аргументу вида
    \begin{equation} \label{arg_est}
        \|\widehat{x} - x_*\|_2 \leq \frac{4}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1} \leq \frac{4M^2}{\mu(N+1)}.
    \end{equation}

    Переходя к более общей постановке задачи, будем рассматривать задачу нахождения решения $x_*$ (также называемого слабым решением) вариационного неравенства: 
    \begin{equation}\label{eq:1}
    \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
    \end{equation}
    где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$,
    $g: Q \longrightarrow \mathbb{R}^n$. Предположим, что удовлетворяющее \eqref{eq:1} решение $x_*$ существует.

    Всюду далее будем предполагать, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая расстояние, а также соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
    \begin{equation}\label{Brg_form}
    V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
    \end{equation}

    Введём следующий аналог понятия относительной сильной выпуклости функции \cite{Lu_Nesterov_2018} для вариационных неравенств.
    \begin{definition}\label{DefRelStrongMonot}
    Назовём оператор $g$ относительно $\mu$-сильно монотонным, где $\mu >0$, если для всяких $x, y \in Q$ верно неравенство
        \begin{equation}\label{eq:3}
             \mu V(y, x) + \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
         \end{equation}
    \end{definition}
    Как правило, далее в статье мы будем использовать следующее неравенство, естественно вытекающее из \eqref{eq:3}.
    \begin{remark}
    Если оператор $g$ является  относительно $\mu$-сильно монотонным, то для всяких $x, y \in Q$ верно неравенство
    $$
             \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
    $$
    \end{remark}
 \fi

\underline{\textbf{В параграфе 2.2}} мы рассмотрим численные методы решения вариационных неравенств с операторами, удовлетворяющими условиям относительной ограниченности, а также относительной гладкости.

Вслед за \cite{Bach_2012} предложим метод зеркального спуска \eqref{eq:4}, но уже для рассматриваемого в настоящей работе класса  вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами (определения \ref{DefRelStrongMonot} и \ref{DefRelBound}):
\begin{equation} \label{eq:4}
    x_{k+1} := \arg \min_{x \in Q} \left\{ h_k \langle g(x_k), x \rangle + V(x, x_k)\right\},
\end{equation}
где
$$
    h_k = \frac{2}{\mu(k+1)},\quad  \forall k= 0,1, 2, \ldots.
$$

Для метода в данной постановке формулируется и проводится доказательство следующей теоремы:
\begin{theorem}\label{thm_MD_VI}
    Пусть $g$ --- $\mu$-относительно сильно монотонный и $M$-относительно ограниченный оператор. Тогда после $N$ итераций алгоритма: 
    $$ 
        x_{k+1} := \arg \min_{x \in Q} \{ h_k \langle g(x_k), x\rangle + V(x, x_k)\}, \;\;\; h_k = \frac{2}{\mu (k+1)}
    $$
    будет верно неравенство:
    \begin{equation}\label{eq:2}
        \max_{x \in Q} \langle g(x), \widehat{x} - x\rangle \leq \frac{2 M^2}{\mu (N+1)},
    \end{equation}
    где 
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
    $$
\end{theorem}
Как известно, такая оценка сложности оптимальна даже на классе относительно липшицевых и относительно сильно выпуклых задач минимизации \cite{Lu_2018}. Это указывает на её оптимальность и для существенно более широкого класса относительно липшицевых и относительно сильно выпуклых задач минимизации, а значит и для рассматриваемого класса вариационных неравенств. 

\begin{remark} \label{remark4}
    Если прокс-функция $d$ является $1$-сильно выпуклой, то итоговая оценка \eqref{eq:2} приобретает следующий вид:
    \begin{equation}
        \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|g(x_k)\|_*^2}{k+1} \leq \varepsilon.
    \end{equation}
\end{remark}

\underline{\textbf{В параграфе 2.3}} показывается естественная взаимосвязь между вариационными неравенствами с монотонными операторами и выпукло-вогнутыми седловыми задачами. Выпукло-вогнутые седловые задачи играют важную роль для самых разных прикладных проблем. Поэтому в данном пункте статьи мы покажем, как полученные в предыдущих пунктах результаты о методах для вариационных неравенств можно применить к седловым задачам вида
\begin{equation}\label{eqsedlo}
    f^* = \min_{u \in Q_1} \max_{v \in Q_2} f(u, v),
\end{equation}
где $f$ --- относительно сильно выпукла по $u$ и относительно сильно вогнута по $v$.

Как известно, необходимость решения вариационных неравенств мотивируется, в частности, как раз седловыми задачами вида \eqref{eqsedlo}. Для данного типа задач вводится аналог дивергенции Брэгмана \cite{Fedor_relative_adapuniv}. 
$$
    V_{\text{new}}\left((y, \boldsymbol{\lambda}), (x, \boldsymbol{\lambda}^{'})\right) = V(y,x) + \frac{1}{2} \left\|\boldsymbol{\lambda} - \boldsymbol{\lambda}^{'}\right\|_2^2, \quad  \forall y, x \in Q, \boldsymbol{\lambda},  \boldsymbol{\lambda}^{'} \in \mathbb{R}_+^m.
$$
введенная таким образом дивергенция позволяет ослабить требования к ограничениям $\boldsymbol{\lambda}$.

Также вводится вспомогательный оператор 
\begin{equation}\label{operator-sedlo}
    g(x) := \Bigg( 
    \begin{aligned}
        f^{'}_{u}(u,v)\\
        -f^{'}_{v}(u,v)
    \end{aligned}
    \Bigg).
\end{equation}
В данной постановке можно воспользоваться \eqref{eq:2} и доказать для задачи \eqref{eqsedlo} следующую оценку
\begin{equation}
    \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)}.
\end{equation}


\iffalse
    $$
        V_{\text{new}}\left((y, \boldsymbol{\lambda}), (x, \boldsymbol{\lambda}^{'})\right) = V(y,x) + \frac{1}{2} \left\|\boldsymbol{\lambda} - \boldsymbol{\lambda}^{'}\right\|_2^2, \quad  \forall y, x \in Q, \boldsymbol{\lambda},  \boldsymbol{\lambda}^{'} \in \mathbb{R}_+^m.
    $$
    введенная таким образом дивергенция позволяет ослабить требования к ограничениям $\boldsymbol{\lambda}$.

    Перейдём теперь к методике для нахождения приближённого решения задачи \eqref{eqsedlo}. Для всякого $\varepsilon > 0$ под $\varepsilon$-точным решением задачи \eqref{eqsedlo} будем понимать пару $(\widehat{u}, \widehat{v})$ такую, что $$\max_{v \in Q_2} f(\widehat{u}, v) - \min_{u \in Q_1} f(u, \widehat{v}) \leq \varepsilon.$$ Обозначим $x = (u, v), y = (z, t)$, а также введем оператор 
    \begin{equation}\label{operator-sedlo}
        g(x) := \Bigg( 
        \begin{aligned}
            f^{'}_{u}(u,v)\\
            -f^{'}_{v}(u,v)
        \end{aligned}
        \Bigg).
    \end{equation}
    В данной постановке можно воспользоваться \eqref{eq:2} и доказать для задачи \eqref{eqsedlo} следующую оценку
    \begin{equation}
        \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)}.
    \end{equation}
\fi

\underline{\textbf{В параграфе 2.4}} доказывается адаптивный вариант для оценки скорости сходимости субградиентного спуска из работы \cite{Bach_2012}. Также приводятся результаты численных экспериментов, которые позволяют сравнить оригинальную оценку скорости сходимости и адаптивную ее версию. Рассматривается пример, когда оригинальная оценка неприменима. 

Напомним оптимальную оценку, доказанную в \cite{Bach_2012}, для субградиентного спуска с липшицевым и $\mu$-сильно выпуклым функционалом:
\begin{equation}\label{orig_estimation_f}
    f(\widehat{x}) - f(x_*) \leq \frac{2 M^2}{\mu (N+1)}  \; \text{  при   } \; \widehat{x} = \sum\limits_{k=1}^{N} \frac{2 k}{N (N+1)} x_k, 
\end{equation}
где $M$ --- константа Липщица целевой функции $f$.

Данную оценку можно несколько улучшить на классе сильно выпуклых задач, что было описано в \cite{Stonyakin_2021}. Воспользуемся рассуждением, проведенным для вариационных неравенств в замечании \ref{remark4} и приведем соответствующий известный результат:
\begin{theorem}\label{ThmBachAdaptive}
    Пусть $f$ --- $\mu$-сильно выпуклая функция. Тогда после $N$ итераций алгоритма:
    \begin{equation}\label{orig_2}
        x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
    \end{equation}
    будет верно неравенство:
    \begin{equation}\label{adaptive_estimation_f}
        f(\widehat{x}) - f(x_*) \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1},
    \end{equation}
    где
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
    $$
    Если $f$ ещё и $M$-липшицева при $M >0$, то
    $$
         f(\widehat{x}) - f(x) \leq \varepsilon
    $$
    после $N = \mathcal{O}(\frac{M^2}{\mu\varepsilon})$ итераций алгоритма \eqref{orig_2}.
\end{theorem}

Полученный в теореме \ref{ThmBachAdaptive} результат применим и в случаях, когда константа Липщица ($M$) --- бесконечна или её значение сложно оценить. Более того, данный подход может быть распространён на важные прикладные задачи, среди которых задача бинарной классификации методом опорных векторов (SVM) \cite{Bach_2012}. По аналогии с работой \cite{Bach_2012} можно применять стохастический вариант зеркального спуска \eqref{orig_2}. Также отметим, что данные рассуждения и метод аналогичны описанным в предыдущих пунктах обобщениям на класс вариационных неравенств, лагранжевых и седловых задач. 

Приведем результаты численных экспериментов для сравнения оригинальной оценки \eqref{orig_estimation_f} с ее адаптивной версией \eqref{adaptive_estimation_f} и приведем пример, когда оригинальная оценка неприменима. 

Для иллюстрации различия между \eqref{orig_estimation_f} и \eqref{adaptive_estimation_f} была выбрана достаточно простая задача о нахождении наименьшего покрывающего шара:
\begin{gather}\label{sphere_cover_strongly}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2^2, \|x - a_1\|_2^2, ..., \|x - a_m\|_2^2\right\},
\end{gather}

Основным преимуществом данной задачи является ее очевидная сильная выпуклость и тривиальность нахождения константы Липшица функции: $ M = 2 \cdot diam\{Q\} $. $Q$ в данном случае является шаром.
Рассматривалось несколько начальных конфигураций, во всех экспериментах размерность пространства $n \sim (10^3 - 10^4)$. Как правило, диаметр допустимого множества $Q$ был меньше или равен размеру искомого покрывающего шара. Такое построение позволяет работать с очень простой процедурой проектирования точек из $\mathbb{R}^n$ на $Q$.

На всех графиках в данном пункте зеленая линия отвечает за невязку по функции, синяя за уточненную оценку \eqref{adaptive_estimation_f}, а оранжевая за глобальную оценку \eqref{orig_estimation_f}. Хорошо показано на рис. \ref{r_20_q_6}, насколько уточненная оценка ближе к реальному значению невязки, чем ее глобальная версия. 

\begin{figure}[h]
	\centering
	\includegraphics[height=0.28\paperheight]{"compare_radius_20"}
    \caption{Невязка по функции и оценки для задачи минимизации \eqref{sphere_cover_strongly} для $Q$ радиуса 6.}
    \label{r_20_q_6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[height=0.25\paperheight]{"q_unlim"}
    \caption{Работа уточненной оценки \eqref{adaptive_estimation_f} для задачи минимизации \eqref{sphere_cover_strongly} для шара $Q = \mathbb{R}^n$.}
    \label{q_unlim}
\end{figure}

Также появляется интересная возможность, использовать ее в тех случаях, когда константу Липшица ($M$) целевой функции невозможно или сложно оценить. В качестве примера используется случай, когда $Q = \mathbb{R}^n$, в таком случае значение $M$ неограниченно и уходит в бесконечность. Именно этот случай показан на рис. \ref{q_unlim}. 

Именно субградиентная природа методов и необходимость изменять алгоритм проектирования привела к необходимости реализации собственной библиотеки для проведения этих и последующих экспериментов. В большинстве встроенных методов нет возможности задания специальных  множеств и при работе с задачами, имеющими субградиентную природу, такие библиотеки как scipy выдают некорректный результат. Также зачастую подобные библиотеки имеют сложное внутреннее представление для проверки необходимых свойств и вычисления необходимых глобальных констант. Такой подход позволяет им работать с большим количеством возможных постановок поддерживаемых классов задач. Как правило, оптимизации подобных библиотек направлены на работу с популярными постановками, например, задача минимизации квадратичной формы. В более сложных случаях собственноручно реализованный метод, даже без значительных оптимизаций под конкретную задачу, показывает меньшее время работы. 


\underline{\textbf{В третей главе}} проводится экспериментальное сравнение скорости сходимости субградиентных методов при условии острого минимума и при условии сильной выпуклости. Рассматриваются результаты полученные с использованием техники рестартов. Также предложен механизм рестартов для зеркального спуска с использованием условия относительного $\gamma$-роста. 

\underline{\textbf{В параграфе 3.1}} описывается основная мотивация для применения механизма рестартов. А именно пессиместичные теоретические оценки скорости сходимости для негладких задач. 

\underline{\textbf{В параграфе 3.2}} предложен анализ экспериментального сравнения двух субградиентных методов, использующих различные дополнительные условия:
\begin{enumerate}
    \item сильной выпуклости,
    \item обобщенного условия острого минимума.
\end{enumerate}

Напомним определение условия острого минимума для $f$
\begin{gather}\label{sm}
    f(x) - f(x_*) \geq \alpha \min_{x_* \in X_*} \|x- x_*\|_2 \quad \forall x \in Q
\end{gather}
для некоторого фиксированного $\alpha >0$ и $f(x_*) = f^* = \min\limits_{x\in Q} f(x)$ для всякого $x_* \in X_*$, где $Q$ --- выпуклое и замкнутое подмножество $\mathbb{R}^n$, $X_*$ --- компакт и $\|\cdot\|_2$ --- евклидова норма. 

Здесь необходимо упомянуть об оценке, доказанной в \cite{sharp22} моими соавторами: С.С. Аблаев, Ф.С. Стонякин, М.С. Алкуса, И.В. Баран. Отмечу, что эта оценка показывает линейную скорость сходимости, причем наиболее значимый вклад в полученную оценку (достаточно громоздкую, чтобы приводить ее полностью) дает последнее слагаемое $\frac{\Delta^2}{2\|\nabla f(x_k)\|^2_2}$. 

Теперь перейдём к экспериментальному сравнению предложенных подходов для задач с $\Delta$-острым минимумом с адаптивной оценкой скорости сходимости \eqref{adaptive_estimation_f} из теоремы \ref{ThmBachAdaptive}. Выбор класса сильно выпуклых задач для сравнения обусловлен известными и  применимыми на практике теоретическими оценками качества приближённого решения по аргументу. 

Поскольку для вышеупомянутых методов требуются различные свойства, то представлены 2 постановки для одной и той же, с геометрической точки зрения, задачи. Задача о наименьшем покрытии точек шаром для $2$-сильно выпуклой функции (уже использовалась ранее \eqref{sphere_cover_strongly})
\begin{gather}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2^2, \|x - a_1\|_2^2, ..., \|x - a_m\|_2^2\right\},
\end{gather}
а также для не сильно выпуклой (но выпуклой) функции
\begin{gather}\label{sphere_cover}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\right\}.
\end{gather}

Теперь перейдём к выпуклой постановке \eqref{sphere_cover} с целью исследования эффективности предложенного метода, учитывающего $\Delta$-острый минимум. Рассматривается целевая функция вида
\begin{gather}\label{allpha_sphere_cover}
    f(x) := \alpha \max_{x\in Q}\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\}.
\end{gather}

Для сравнения, ниже на рис. \ref{res_sharp_convex} и \ref{res_strong_convex} приведены результаты работы для того же набора входных точек, которые необходимо покрыть в обоих постановках --- (\ref{allpha_sphere_cover}) и (\ref{sphere_cover_strongly}). Начальная точка также одна и та же. Метод, опирающийся на $\Delta$-острый минимум, обеспечивает сходимость буквально за 10 итераций к <<приближённому>> решению с заданной точностью и даже позволяет эту точность повысить. Метод, опирающийся на сильную выпуклость, достигает схожих (с геометрической точки зрения) результатов за значительно большее количество итераций, однако он позволяет повышать точность приближённого решения на дальнейших итерациях.

\begin{figure}[h]
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{sharp_convex_x.png}
    \endminipage\hfill
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{sharp_convex_f.png}
    \endminipage\hfill
    \caption{ Результаты решения задачи минимизации (\ref{allpha_sphere_cover}), где  $n= 1\,000, r = 0.7525, \alpha = 0.6$.}
    \label{res_sharp_convex}
\end{figure}

\begin{figure}[h]
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{strong_convex_small_rad_x.png}
    \endminipage\hfill
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{strong_convex_small_rad_f.png}
    \endminipage\hfill
    \caption{ Результаты решения задачи минимизации (\ref{sphere_cover_strongly}), где  $n= 1\,000, r = 0.7525$.}
    \label{res_strong_convex}
\end{figure}

Данные результаты привели к идее объединения данных подходов для достижения лучших результатов в скорости сходимости без потери возможности дальнейшего повышения точности.


\underline{\textbf{В параграфе 3.3}} вводится аналог условия острого минимума, а именно - условие относительного $\gamma$-роста. На его основе к методу классического зеркального спуска применяется схема рестартов, что позволяет улучшить получаемые оценки.

Метод, который будет использоваться для рестартов, был представлен в работе \cite{Lu_2018}:
\begin{theorem} \label{vanilla_mirror}
    Пусть $f$ --- является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$ c сильно выпуклой прокс-функцией $d(x)$. Тогда можно задать метод следующим образом:
    \begin{equation} \label{mirr_upd}
        x_{k+1} = \arg \min_{x \in Q} {\left[ f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{h_k} V(x, x_k)\right]},
    \end{equation}
    где $\{ h_k \}$ - последовательность размеров шагов.
    Для него справедлива следующая оценка скорости сходимости:
    \begin{equation} \label{general_est}
        \min_{0\leq k \leq N} f(x_k) - f(x) \leq \frac{\frac{1}{2} M^2 \sum_{k=0}^N h_k^2 + V(x, x_0)}{\sum_{k=0}^N h_k}
    \end{equation}
\end{theorem}

\begin{remark}
    Если в \eqref{mirr_upd} в формулировке теоремы \ref{vanilla_mirror} выбрать шаг следующим образом:
    \begin{equation} \label{mirr_step}
        h_{k} = \frac{\sqrt{2 \left[\min\limits_{x_* \in X_*}{V(x_*, x_0)}\right] }}{M\sqrt{N}},
    \end{equation}
    то можно выписать такую оценку скорости сходимости:
    \begin{equation} \label{mirr_est}
        f(\widehat{x_N}) - f(x_*) \leq \frac{M\sqrt{2 \left[\min\limits_{x_* \in X_*}{V(x_*, x_0)}\right]}}{\sqrt{N}}
    \end{equation}
\end{remark}

Если функция обладает дополнительными свойствами, аналогичными острому минимуму,  то становится возможным применение техники рестартов. Используем аналог данного условия и вслед за Шапиро–Немировским (см. \cite{shapiro_2005} и \cite{shapiro_2021} ) впервые введем условие относительного $\gamma$-роста ($\gamma \geq 1$). Отметим, что подобное условие впервые вводится с использованием дивергенции Брэгмана, что позволяет обобщить ряд условий, таких как условия острого минимума, квадратичного доминирования и $\gamma$-роста.
\begin{definition}
   Будем говорить, что $f$ --- удовлетворяет условию относительного $\gamma$-роста, если для всякого $x \in Q$ верно неравенство:
   \begin{equation} \label{gamma-growth}
       f(x) - f(x_*) \geq \mu_{\gamma}\left(\min_{x_* \in X_*}{V(x_*,x)}\right)^{\gamma/2},
   \end{equation}
   где $X_*$ --- множество возможный решений задачи минимизации $f$. 
\end{definition}

Справедлива следующая теорема:
\begin{theorem} \label{simple_restart}
    Пусть $f$ --- удовлетворяет условию относительного $\gamma$-роста \eqref{gamma-growth} и также является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$. В таком случае Алгоритм \ref{alg:rest_gamma} после 
    \begin{equation}
    \begin{aligned}
       N =\mathcal{O}\left(\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}}\right) \text{ при } \gamma = 1, \\
       N = \mathcal{O}\left(\frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2^{\gamma} - 2)} \left[\varepsilon^{(1 - \gamma)} - \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)}\right]\right) \text{ при } \gamma > 1,
    \end{aligned}
    \end{equation}
    обращений к субградиенту $f$ будут справедливы неравенства:
    \begin{equation}
        \min_{x_* \in X_*}{V(x_*, \widehat{x_p})} \leq \varepsilon
    \end{equation}
    и
    \begin{equation}
        f(\widehat{x_p}) - f(x_*) \leq M \sqrt{2 \varepsilon}.  
    \end{equation}
\end{theorem}

\begin{algorithm}[htp]
    \caption{Рестарты зеркального спуска при условии относительного $\gamma$-роста.}
    \label{alg:rest_gamma}
    \KwData{$\varepsilon > 0$}
    \KwResult{$x_p$}
    $p \gets 0$\;
    $\min\limits_{x_* \in X_*}{V(x_*, x_0)} \gets \min\limits_{x_* \in X_*}{V(x_*,x_0^0)}$\;
    \While{$p < \log_2\left(\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}\right).$}{
        $x_{p}$ --- результат работы метода \eqref{mirr_upd} с шагом \eqref{mirr_step} и параметром $N_{p} = \ceil*{\frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{p(1 - \gamma)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}}$\;
        $x_0 = \widehat{x_p}$\;
        $\min\limits_{x_* \in X_*}{V(x_*, x_0)} \gets \frac{1}{2^{p+1}}\min\limits_{x_* \in X_*}{V_{0}(x_*, x_0^0)}$\;
        $p=p+1$\;
    }
\end{algorithm}


\underline{\textbf{В параграфе 3.4}} предлагается развитие полученных в предыдущем параграфе результатов. На практике для использования приведенной выше теоремы \ref{simple_restart} требуется знание о точном решении для оценки $\min\limits_{x_* \in X_*}{V(x_*, x_0)}$, что лишает данный метод практической пользы. 

Введем $\Theta$ как мажорирующую константу для $\min\limits_{x_* \in X_*}{V(x_*, x_0)}$, что формально описывается
$$
    \Theta \geq \min\limits_{x_* \in X_*}{V(x_*, x_0)}.
$$

Вспомогательный результат, доказанный в данном пункте, позволяет при определении шага следующим образом 
\begin{equation} \label{eps_step}
    h_{k} = \frac{\varepsilon}{\norm{\nabla f(x_k)}^2_2},
\end{equation}
и используя критерий остановки метода
\begin{equation} \label{stop_crit}
    \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2_2} \geq \frac{2 \Theta^2}{\varepsilon^2}, \;\;\;\;\text{ где } \Theta \geq \min\limits_{x_* \in X_*}{V(x_*, x_0)},
\end{equation}
получить следующее неравенство для невязки по функции:
\begin{equation} 
\begin{aligned}
    \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \varepsilon.
\end{aligned}
\end{equation}

\iffalse
    Все вспомогательные результаты, полученные ранее, дают возможность объединить их и сформулировать в виде удобного с практической точки зрения алгоритма. Данный алгоритм требует оценки сверху для расстояния от начальной точки до точного решения, которая может быть сколь угодно <<грубой>>. Это позволяет в дальнейшем усложнять алгоритм, подбирая лучшую начальную оценку расстояния. Также соответствующая теорема \ref{restared_criteria} показывает, что даже существенно <<ослабленная>> версия условия <<острого>> минимума позволяет значимо повысить скорость сходимости.
\fi

Воспользуемся предложенным критерием остановки и сформулируем следующий Алгоритм \ref{alg:rest_criteria}. 
 \begin{algorithm}[htp]
    \caption{Рестарты зеркального спуска при условии $\gamma$-роста с критерием остановки.}
    \label{alg:rest_criteria}
    \KwData{$\varepsilon > 0$}
    \KwResult{$x_p$}
    $p \gets 0$\;
    $\Theta_0 \geq \min\limits_{x_* \in X_*}{V(x_*,x_0^0)}$\;
    \While{$p < \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}.$}{
        $x_{p}$ --- результат работы метода \eqref{mirr_upd} с шагом \eqref{eps_step} и критерием остановки $\sum_{k=0}^{N_p} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{ 2^{(p \gamma - p + \gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} } $, где $N_p$ --- количество итераций на данном рестарте метода\;
        $x_0 = x_{min}^p$\;
        $p=p+1$\;
    }
\end{algorithm}
\begin{theorem}
    Пусть $f$ удовлетворяет условию $\gamma$-роста \eqref{gamma-growth} и также является $M$-липшицевой на $Q$ относительно некоторой дивергенции Брэгмана $V(x, y)$. В таком случае Алгоритм \ref{alg:rest_criteria} после 
    \begin{equation}
       N = \mathcal{O} \left( \frac{4 M^2}{\mu_{\gamma}^2} \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}\right) \text{ при } \gamma = 1
    \end{equation}
    или
    \begin{equation}
       N = \mathcal{O}\left( \frac{2 M^2 }{2^{\gamma - 1} - 1}\left[ \frac{2}{\mu_{\gamma}^{\frac{2}{\gamma}}}\varepsilon^{\frac{2}{\gamma} - 2} - \frac{2^{\gamma}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \right] \right) \text{ при } \gamma > 1
    \end{equation}
    обращений к оракулу, где $\Theta_0$ --- это оценка сверху для $\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}$, будет справедливо неравенство
    \begin{equation}
        f(x_{min}^N) - f(x_*) \leq \varepsilon.
    \end{equation}
\end{theorem}

Полученный результат имеет схожие оценки с предыдущей теоремой \ref{simple_restart}, однако на практике критерий остановки может значительно сократить количество итераций, необходимых для достижения заданной точности $\varepsilon$. Также при помощи данного метода легко прогнозируется точность перед каждым рестартом метода, что удобно для контроля за правильностью работы метода. Также подобная информация позволяет переключаться между несколькими модификациями метода, изменяющими, например, параметр $\Theta_0$. Оптимизация данного параметра позволит улучшить оценки без модификации самого метода, что позволит встроить его в более комплексные адаптивные фреймворки в дальнейшем. 

\FloatBarrier
\pdfbookmark{Заключение}{conclusion}                                  % Закладка pdf
В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\pdfbookmark{Литература}{bibliography}                                % Закладка pdf


\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
    \renewcommand{\bibname}{\large \bibtitleauthor}
    \nocite{*}
    \insertbiblioauthor           % Подключаем Bib-базы
    %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
    % Цитирования.
    %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
    %  * Если не соблюдать порядок "как для \printbibliography", нумерация в `\insertbiblioauthor` будет кривой.
    %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
    \nocite{Stonyakin_2021}%
    \nocite{yakovlev2019algorithms}%
    \nocite{sharp22}%
    \nocite{GorbunovKMR20}%
    %

    \ifnumgreater{\value{usefootcite}}{0}{
        \begin{refcontext}[labelprefix={}]
            \ifnum \value{bibgrouped}>0
                \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
            \else
                \insertbiblioauthor      % Вывод всех работ автора
            \fi
        \end{refcontext}
    }{
        \ifnum \totvalue{citeexternal}>0
            \begin{refcontext}[labelprefix=A]
                \ifnum \value{bibgrouped}>0
                    \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
                \else
                    \insertbiblioauthor      % Вывод всех работ автора
                \fi
            \end{refcontext}
        \else
            \ifnum \value{bibgrouped}>0
                \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
            \else
                \insertbiblioauthor      % Вывод всех работ автора
            \fi
        \fi
        %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
        \begin{refcontext}[labelprefix={}]
            \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
        \end{refcontext}
        % Невидимый библиографический список для подсчёта количества внешних публикаций
        % Используется, чтобы убрать приставку "А" у работ автора, если в автореферате нет
        % цитирований внешних источников.
        \printbibliography[heading=nobibheading, section=0, env=countexternal, keyword=biblioexternal, resetnumbers=true]%
    }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
