\pdfbookmark{Общая характеристика работы}{characteristic}             % Закладка pdf
\section*{Общая характеристика работы}

\newcommand{\actuality}{\pdfbookmark[1]{Актуальность}{actuality}\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\pdfbookmark[1]{Разработанность темы}{progress}\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\pdfbookmark[1]{Цели}{aim}\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\pdfbookmark[1]{Задачи}{tasks}\underline{\textbf{\tasksTXT}}}
\newcommand{\aimtasks}{\pdfbookmark[1]{Цели и задачи}{aimtasks}\aimtasksTXT}
\newcommand{\novelty}{\pdfbookmark[1]{Научная новизна}{novelty}\underline{\textbf{\noveltyTXT}}}
\newcommand{\influence}{\pdfbookmark[1]{Практическая значимость}{influence}\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\pdfbookmark[1]{Методология и методы исследования}{methods}\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\pdfbookmark[1]{Положения, выносимые на защиту}{defpositions}\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\pdfbookmark[1]{Достоверность}{reliability}\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\pdfbookmark[1]{Апробация}{probation}\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\pdfbookmark[1]{Личный вклад}{contribution}\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\pdfbookmark[1]{Публикации}{publications}\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.

\pdfbookmark{Содержание работы}{description}                          % Закладка pdf
\section*{Содержание работы}
Во \underline{\textbf{введении}} обосновывается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по~изучаемой проблеме,
формулируется цель, ставятся задачи работы, излагается научная новизна
и практическая значимость представляемой работы. В~последующих главах
сначала описывается общий принцип, позволяющий \dots, а~потом идёт
апробация на частных примерах: \dots  и~\dots.


\underline{\textbf{Первая глава}} посвящена обзору литературы по теме выпуклой оптимизации. В ней затрагиваются методы нулевого порядка и приводится исследование специального функционала, не обладающего специальными свойствами, при помощи данных методов. Также анализируется их эффективность с точки зрения скорости сходимости с методами первого порядка. Описываются стандартные проблемы возникающие при практической работе с подобными методами и ограничениями на реализацию при работе с ними. В частности возможность параллелизации и ее сложность. Описываются недостатки оракульной оценки. Современные вычислительные мощности во многом опираются на возможность автоматической параллелизации и предполагают, что разработчик будет в значительной мере учитывать архитектурные возможности конкретного вычислительного устройства. Использование оракульной сложности удобно для сравнения методов в предположении, что каждое исполняющее устройство работает с алгоритмом последовательно. Однако в современных условиях она не отражает и может противоречить фактическому времени исполнения. 

Приводится пример работы с существенно невыпуклым функционалом и анализируются результаты работы стандартных методов оптимизации на несвойственном для них классе задач. 

\underline{\textbf{В парафе 1.1}} приводится обзор методов первого порядка вводится общая постановка задачи, приводятся оптимальные оценки важных классов задач. Также рассматриваются популярные постановки задач, например задача минимизации квадратичной формы. Проводятся пояснения о стандартных методах ускорения методов и затрагивается проблематика квазиньютоновских методов и их сильные и слабые стороны.


\underline{\textbf{В парафе 1.2}} описывается работа методов в конкретном, весьма нестандартном приложении. Приводится опыт работы с невыпуклым функционалом, основанным на ряде физических свойств объекта. В силу невыпкулости изначальной постановки были опробованы как методы нулевого порядка (покоординатный спуск в различных вариациях), так и методы первого порядка. Поскольку задача является практической одним из приоритетов было качество и скорость работы. В итоге возникло интересное противопоставление - безградиентного метода со значительной параллелизацией и более линейного метода сопряженных градиентов. 


\underline{\textbf{Вторая глава}} посвящена исследованию методов первого порядка для двух классов вариационных неравенств с операторами, удовлетворяющими предлагаемому аналогу условия относительной сильной выпуклости с аналогом ограниченности (относительная ограниченность), а также с аналогом условия Липшица (относительная гладкость).

Исследуется хорошо известная оптимальная оценка скорости сходимости на классе липшицевых и сильно выпуклых минимизационных, которая достигается именно для субградиентного метода \cite{Simon_Julien_Bach_2012}. Доказывается уточненная версия данной оценки при помощи перехода от глобальных констант к локальным аналогам, что позволяет сделать ее более адаптивной и удобной для практического применения. 

Также используется подход, набравший популярность в последние годы, а именно обобщение на случай задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Мы исследуем оценку скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее говоря, в данной главе рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач с соответствующими условиями относительной липшицевости функционалов. 

Далее, немалую популярность в работах по оптимизации получило упомянутое выше недавно предложенное понятие относительной гладкости функций (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также приведённые в них ссылки), которое позволило существенно расширить класс задач выпуклой оптимизации по сравнению со стандартным предположением о липшицевости градиента с гарантией оценки скорости сходимости $O(N^{-1})$ (здесь и далее $N$ --- количество итераций), которая может считаться оптимальной для такого широкого класса задач \cite{Dragomir}. 

Работа состоит из введения, трех основных частей (пунктов) и заключения. Второй пункт статьи посвящён модификации метода зеркального спуска и выводу оценки его скорости сходимости для вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. В частности, полученная оценка указывает на оптимальность такого метода на выделенном классе вариационных неравенств, поскольку она оптимальна (с точностью до умножения на не зависящую от параметров метода и размерности пространства константу) даже на более узком классе задач минимизации относительно липшицевых и относительно сильно выпуклых функций \cite{Lu_2018}. В третьем пункте статьи рассматривается класс относительно сильно монотонных и относительной гладких операторов и анализируется возможность использования рестартованного адаптивного проксимального зеркального метода для такого класса задач с обоснованием гарантии линейной скорости сходимости. В четвертом пункте показывается, как предложенные ранее алгоритмы для вариационных неравенств и полученные теоретические оценки их скорости сходимости могут быть применены для решения относительно сильно выпукло-вогнутых седловых задач с соответствующими предположениями о гладкости функционалов.

\underline{\textbf{В парафе 2.1}} вводятся понятия относительной гладкости и определяется дивергенция Брэгмана. Также происходит переход от классической постановки к слабому решению вариационного неравенства. 

Численные методы градиентного типа достаточно часто используются для самых разнообразных постановок задач выпуклой оптимизации в пространствах больших размерностей. Это объясняется небольшими затратами памяти на итерациях, а также возможностью обоснования приемлемых оценок скорости сходимости, не содержащих (в отличие, например, от методов отсекающей гиперплоскости) параметров размерности пространства. Однако при этом существенны предположения о функциональных свойствах таких задач (гладкость, липшицевость, сильная выпуклость). Так, несколько лет назад был выделен класс относительно гладких задач оптимизации (см., например \cite{Bauschke,Drag,Lu_Nesterov_2018}). Свойство относительной $L$-гладкости ($L > 0$) обобщает ycловие $L$-гладкости ($L$-липшицевости градиента)  $f$ путём замены в известном для $L$-гладкий фyнкций $f$ неравенстве ($Q$ --- область определения $f$)
$$
    f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{L}{2} \|x - y \|_2^2 \quad   \forall x, y \in Q
$$	
выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией (расхождением) Брэгмана (см. \eqref{Brg_form} и \eqref{funct_rel_smooth} ниже), которая порождается некоторой выпуклой прокс-функцией (важно, что она не обязательно сильно выпукла). Отметим, что здесь и всюдy далее $\|\cdot\|_2$ --- евклидова норма в $n$-мерном пространстве $\mathbb{R}^n$.

Для выпyклых относительно гладких задач которых оценки сходимости обычных (неускоренных) методов градиентного типа оптимальны с точностью до умножения на константу, не зависящую от размерности и параметров метода (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также имеющиеся в них ссылки). В работе \cite{Lu_Nesterov_2018} введено понятие относительной сильной выпуклости функции, которое позволило расширить класс выпуклых оптимизационных задач, для которых можно доказать линейную скорость сходимости (сходимость со скоростью геометрической прогрессии) метода градиентного типа, причём соответствующая оценка не содержит параметров размерности задачи. В данной работе мы развиваем этот подход и исследуем некоторые алгоритмы уже для вариационных неравенств с аналогом относительной сильной выпуклости для операторов (относительной сильной монотонностью). Напомним, что понятие относительной сильной выпуклости \cite{Lu_Nesterov_2018} функции $f$ обобщает понятие обычной $\mu$-сильной выпуклости $f$ ($\mu > 0$) путём замены в неравенстве 
\begin{equation}
    f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{\mu}{2} \|x - y \|_2^2 \leq f(y) \quad   \forall x, y \in Q,
    \end{equation}
выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией Брэгмана (см. \eqref{Brg_form} и \eqref{eqrelativestorngconv} ниже), которая порождается некоторой выпуклой прокс-функцией. 

В данной главе рассматриваются методы первого порядка для двух классов вариационных неравенств с операторами, удовлетворяющими предлагаемому аналогу условия относительной сильной выпуклости (см. ниже определение  \ref{DefRelStrongMonot} относительной сильной монотонности оператора): с аналогом ограниченности (относительная ограниченность, см. определение 2 ниже), а также с аналогом условия Липшица (относительная гладкость, см. определение 3 ниже).

Хорошо известно, что на классе липшицевых и сильно выпуклых минимизационных задач оптимальная оценка скорости сходимости достигается именно для субградиентного метода \cite{Simon_Julien_Bach_2012}. В последние годы активно исследуются задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Мы исследуем оценку скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее говоря, в данной главе рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач с соответствующими условиями относительной липшицевости функционалов. 

Далее, немалую популярность в работах по оптимизации получило упомянутое выше недавно предложенное понятие относительной гладкости функций (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также приведённые в них ссылки), которое позволило существенно расширить класс задач выпуклой оптимизации по сравнению со стандартным предположением о липшицевости градиента с гарантией оценки скорости сходимости $O(N^{-1})$ (здесь и далее $N$ --- количество итераций), которая может считаться оптимальной для такого широкого класса задач \cite{Dragomir}. В плане приложений можно отметить подход к построению методов градиентного типа для задач распределенной оптимизации с использованием относительной гладкости и относительной сильной выпуклости \cite{Hendr}. Аналоги относительной гладкости введены в последние пару лет и для более общей постановки задачи решения вариационного неравенства (см. \cite{Inex}, а также имеющиеся там ссылки) с монотонным оператором. Оказывается, что для этого класса задач можно предложить алгоритмы экстраградиентного типа с гарантией оценки скорости сходимости $O(N^{-1})$. Мы же рассматриваем класс относительно сильно монотонных и относительно гладких операторов и приводим соответствующие оценки.

Перед переходом к постановке задачи в терминах вариационных неравенств следует упомянуть адаптивный аналог \cite{Stonyakin_2021} теоретической оценки качества выдаваемого решения для субградиентного метода \cite{Bach_2012}. Напомним, что в работе рассматриваются задачи вида
\begin{gather}\label{min_q}
    f(x)\rightarrow\min_{x\in Q},
\end{gather}
где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^{n}$. Для субградиентного метода вида
\begin{gather}\label{orig}
    x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
\end{gather}
известна следующая оценка скорости сходимости \cite{Bach_2012}:
\begin{equation}\label{orig_estimation_f}
    f(\widehat{x}) - f(x_*) \leq \frac{2 M^2}{\mu (N+1)}  \; \text{  при   } \; \widehat{x} = \sum\limits_{k=1}^{N} \frac{2 k}{N (N+1)} x_k, 
\end{equation}
где $M$ --- константа Липщица целевой функции $f$.
Поэтому справедлива следующая
\begin{theorem}\label{ThmBachAdaptive}
    Пусть $f$ --- $\mu$-сильно выпуклая функция. Тогда после $N$ итераций алгоритма:
    $$
        x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
    $$
    будет верно неравенство:
    \begin{equation}\label{adaptive_estimation_f}
        f(\widehat{x}) - f(x_*) \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1},
    \end{equation}
    где
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
    $$
    Если $f$ ещё и $M$-липшицева при $M >0$, то
    $$
         f(\widehat{x}) - f(x) \leq \varepsilon
    $$
    после $N = \mathcal{O}(\frac{M^2}{\mu\varepsilon})$ итераций алгоритма \eqref{orig}.
\end{theorem}

Отметим, что если $x_*$ --- точное решение задачи минимизации $f$, то можно получить оценку скорости сходимости по аргументу вида
\begin{equation} \label{arg_est}
    \|\widehat{x} - x_*\|_2 \leq \frac{4}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1} \leq \frac{4M^2}{\mu(N+1)}.
\end{equation}

Переходя к более общей постановке задачи, будем рассматривать задачу нахождения решения $x_*$ (также называемого слабым решением) вариационного неравенства: 
\begin{equation}\label{eq:1}
\max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
\end{equation}
где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$,
$g: Q \longrightarrow \mathbb{R}^n$. Предположим, что удовлетворяющее \eqref{eq:1} решение $x_*$ существует.

Всюду далее будем предполагать, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая расстояние, а также соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
\begin{equation}\label{Brg_form}
V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
\end{equation}

Введём следующий аналог понятия относительной сильной выпуклости функции \cite{Lu_Nesterov_2018} для вариационных неравенств.
\begin{definition}\label{DefRelStrongMonot}
Назовём оператор $g$ относительно $\mu$-сильно монотонным, где $\mu >0$, если для всяких $x, y \in Q$ верно неравенство
    \begin{equation}\label{eq:3}
         \mu V(y, x) + \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
     \end{equation}
\end{definition}
Как правило, далее в статье мы будем использовать следующее неравенство, естественно вытекающее из \eqref{eq:3}.
\begin{remark}
Если оператор $g$ является  относительно $\mu$-сильно монотонным, то для всяких $x, y \in Q$ верно неравенство
$$
         \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
$$
\end{remark}

\underline{\textbf{В парафе 2.2}} мы рассмотрим численные методы решения вариационных неравенств с операторами, удовлетворяющими условиям относительной ограниченности, а также относительной гладкости.
\begin{definition}\label{DefRelBound}\cite{Main}
    Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $M$-огранич\-енным, где $M >0$, если для всяких $x, y \in Q$ верно неравенство
    \begin{equation}\label{rel_bound}
         \langle g(x), x - y \rangle \leq M\sqrt{2V(y,x)}.
     \end{equation}
\end{definition}
\begin{definition}\cite{Inex}
    Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $L$-гладким, где $L > 0$, если для всяких $x, y \in Q$ верно неравенство
    \begin{equation}\label{rel_smooth}
        \langle g(y)-g(z),x-z\rangle \leq LV(x,z) + LV(z,y).
    \end{equation}
\end{definition}
Отметим, что если функция $f$ $L$-относительно гладкая \cite{Bauschke}, т.е.
\begin{equation}\label{funct_rel_smooth}
    f(y) \leq f(x) + \langle \nabla f(x), y - x\rangle + LV(y, x) \quad \forall x, y \in Q,
\end{equation}
то оператор $g(x) = \nabla f(x)$ yдовлетворяет \eqref{rel_smooth}. Однако в слyчае непотенциального оператора $g$ yсловие \eqref{rel_smooth} не сводится, вообще говоря, к \eqref{funct_rel_smooth} для какой-нибyдь фyнкции $f$.


Вслед за \cite{Simon_Julien_Bach_2012} предложим метод зеркального спуска \eqref{eq:4}, но уже для рассматриваемого в настоящей работе класса  вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами (определения \ref{DefRelStrongMonot} и \ref{DefRelBound}):
\begin{equation} \label{eq:4}
    x_{k+1} := \arg \min_{x \in Q} \left\{ h_k \langle g(x_k), x \rangle + V(x, x_k)\right\},
\end{equation}
где
$$
    h_k = \frac{2}{\mu(k+1)},\quad  \forall k= 0,1, 2, \ldots.
$$

Для метода в данной постановке формулируется и проводится доказательство следующей теоремы:
\begin{theorem}\label{thm_MD_VI}
    Пусть $g$ --- $\mu$-относительно сильно монотонный и $M$-относитель\-но ограниченный оператор. Тогда после $N$ итераций алгоритма: 
    $$ 
        x_{k+1} := \arg \min_{x \in Q} \{ h_k \langle g(x_k), x\rangle + V(x, x_k)\}, \;\;\; h_k = \frac{2}{\mu (k+1)}
    $$
    будет верно неравенство:
    \begin{equation}\label{eq:2}
        \max_{x \in Q} \langle g(x), \widehat{x} - x\rangle \leq \frac{2 M^2}{\mu (N+1)},
    \end{equation}
    где 
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
    $$
\end{theorem}

\begin{remark}
    Если $x_*$ --- сильное решение рассматриваемого вариационного не\-равенства, то можно выписать оценку скорости сходимости и <<по аргументу>>, поскольку $\langle g(x_*), x_k - x_*\rangle \geq 0$. Тогда следуя логике, использованной в доказательстве теоремы \ref{thm_MD_VI}, можно получить следующие результаты: 
        \begin{equation} \label{eq:12}
        \begin{aligned} 
            \sum_{k=1}^{N} \frac{2k\mu V(x_k, x_*)}{N(N+1)} \leq \frac{2M^2}{\mu(N+1)} \quad  \forall x \in Q,
        \end{aligned}
        \end{equation}
    Если же прокс-функция $1$-сильно выпукла относительно нормы $\|\cdot\|$, то из \eqref{eq:12} вытекает следующая оценка:
        \begin{equation} 
        \begin{aligned} 
            \|x_* - x_k\|^2 \leq \frac{4M^2}{\mu(N+1)}.
        \end{aligned}
        \end{equation}
\end{remark}
\begin{remark}
    Если прокс-функция $d$ является $1$-сильно выпуклой, то оценку \eqref{eq:2} можно уточнить:
    \begin{equation}
        \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|g(x_k)\|_*^2}{k+1} \leq \varepsilon.
    \end{equation}
    Правая часть предыдущего неравенства может оказаться существенно меньшей, чем для оценки \eqref{eq:2}. Этот подход полностью аналогичен подходу, описанному при получении \eqref{arg_est}.
\end{remark}

\underline{\textbf{В парафе 2.3}} показывается естественная взаимосвязь между вариационными неравенствами с монотонными операторами и выпукло-вогнутыми седловыми задачами. Выпукло-вогнутые седловые задачи играют важную роль для самых разных прикладных проблем. Поэтому в данном пункте статьи мы покажем, как полученные в предыдущих пунктах результаты о методах для вариационных неравенств можно применить к седловым задачам вида
\begin{equation}\label{eqsedlo}
    f^* = \min_{u \in Q_1} \max_{v \in Q_2} f(u, v),
\end{equation}
где $f$ --- относительно сильно выпукла по $u$ и относительно сильно вогнута по $v$.

Как известно, необходимость решения вариационных неравенств мотивируется, в частности, как раз задачами вида \eqref{eqsedlo}. В качестве примера можно рассмотреть лагранжеву седловую задачу, порожденную задачей относительно сильно выпуклого программирования.  
\begin{example} Рассмотрим задачу относительно сильно выпуклого программирования (все функционалы $\widehat{f}, g_1, g_2, ...$ относительно сильно выпуклы):
    \begin{equation}\label{problem_with_fun_constraints}
        \left\{\begin{array}{c}
        \min_{x \in Q} \widehat{f}(x), \\
        g_{1}(x), g_{2}(x), \ldots, g_{m}(x) \leq 0.
        \end{array}\right.
    \end{equation}
        
    Рассмотрим соответствующую \eqref{problem_with_fun_constraints} лагранжеву седловую задачу следующего вида
    \begin{equation}\label{lagrange_problem}
        \min_{x \in Q} \max_{ \boldsymbol{\lambda}= (\lambda_1, \ldots, \lambda_m)^T \in \mathbb{R}_+^m} L(x, \boldsymbol{\lambda}) :=  \widehat{f}(x) + \sum_{p=1}^{m} \lambda_p g_p(x) - \varepsilon \sum_{p=1}^m \lambda_{p}^2.
    \end{equation}
\end{example}
Для данного типа задач можно ввести такой аналог дивергенции Брэгмана \cite{Fedor_relative_adapuniv}:
$$
    V_{\text{new}}\left((y, \boldsymbol{\lambda}), (x, \boldsymbol{\lambda}^{'})\right) = V(y,x) + \frac{1}{2} \left\|\boldsymbol{\lambda} - \boldsymbol{\lambda}^{'}\right\|_2^2, \quad  \forall y, x \in Q, \boldsymbol{\lambda},  \boldsymbol{\lambda}^{'} \in \mathbb{R}_+^m.
$$
введенная таким образом дивергенция позволяет ослабить требования к ограничениям $\boldsymbol{\lambda}$.

Перейдём теперь к методике для нахождения приближённого решения задачи \eqref{eqsedlo}. Для всякого $\varepsilon > 0$ под $\varepsilon$-точным решением задачи \eqref{eqsedlo} будем понимать пару $(\widehat{u}, \widehat{v})$ такую, что $$\max_{v \in Q_2} f(\widehat{u}, v) - \min_{u \in Q_1} f(u, \widehat{v}) \leq \varepsilon.$$ Обозначим $x = (u, v), y = (z, t)$, а также введем оператор 
\begin{equation}\label{operator-sedlo}
    g(x) := \Bigg( 
    \begin{aligned}
        f^{'}_{u}(u,v)\\
        -f^{'}_{v}(u,v)
    \end{aligned}
    \Bigg).
\end{equation}
Тогда ввиду выпукло-вогнутости $f$ имеем: 
\begin{equation}
\begin{aligned}
    \langle g(x), x - y \rangle &=
     \Bigg( 
    \begin{aligned}
        f^{'}_{u}(u,v)\\
        -f^{'}_{v}(u,v)
    \end{aligned}
    \Bigg)
     (u - z, v - t)  = \langle f^{'}_{u}(u,v), u - z \rangle - \langle f^{'}_{v}(u,v), v - t \rangle \geq \\&
     \geq f(u, v) - f(z, v) 
    - f(u, v)+ f(u, t)=  f(u,t) - f(z, v).
\end{aligned}
\end{equation}
Будем предполагать относительную ограниченность оператора \eqref{operator-sedlo}. Тогда метод \eqref{eq:4} для задач \eqref{eqsedlo} приводит к оценкам вида:
\begin{equation} \label{eq:21}
    \sum_{k=1}^{N} \frac{2k}{N(N+1)} \langle g(x_k), x_k -x\rangle \leq \frac{2 M^2}{\mu (N+1)}.
\end{equation}
Если $x_k = (u_k, v_k), \;\; x = (u, v)$, то  
\begin{equation}
    \langle g(x_k), x_k -x\rangle \geq f(u_k,v) - f(u, v_k) \quad \forall (u, v).
\end{equation}
Далее, \eqref{eq:21} означает, что 
\begin{equation}
    \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k,v) - f(u, v_k)) \leq \frac{2M^2}{\mu (N+1)}.
\end{equation}
Положим
\begin{equation}
    (\widehat{u}, \widehat{v}) := \frac{1}{N(N+1)} \sum_{k=1}^{N} 2k (u_k,v_k).
\end{equation}
Тогда получаем, что
\begin{equation}
    \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k, v) - f(u, v_k)) \geq f(\widehat{u}, v) - f(u, \widehat{v}), 
\end{equation}
откуда ввиду \eqref{eq:2} получаем для задачи \eqref{eqsedlo} следующую оценку
\begin{equation}
    \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)}.
\end{equation}

\underline{\textbf{В парафе 2.4}} описываются численные эксперименты, сравнивающие оригинальную оценку \eqref{orig_estimation_f} с полученной в параграфе 2.1 оценкой \eqref{adaptive_estimation_f} и приводится пример, когда оригинальная оценка неприменима. 

Для иллюстрации различия между \eqref{orig_estimation_f} и \eqref{adaptive_estimation_f} была выбрана достаточно простая задача о нахождении наименьшего покрывающего шара:
\begin{gather}\label{sphere_cover_strongly}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2^2, \|x - a_1\|_2^2, ..., \|x - a_m\|_2^2\right\},
\end{gather}

Основным преимуществом данной задачи является ее очевидная сильная выпуклость и тривиальность нахождения константы Липшица функции: $ M = 2 \cdot diam\{Q\} $. $Q$ в данном случае является шаром.
Рассматривалось несколько начальных конфигураций, во всех экспериментах размерность пространства $n \sim (10^3 - 10^4)$. Как правило, диаметр допустимого множества $Q$ был меньше или равен размеру искомого покрывающего шара. Такое построение позволяет работать с очень простой процедурой проектирования точек из $\mathbb{R}^n$ на $Q$.

На всех рисунках в данном параграфе зеленая линия отвечает за невязку по функции, синяя за уточненную оценку \eqref{adaptive_estimation_f}, а оранжевая за глобальную оценку \eqref{orig_estimation_f}. Хорошо показано на рис. \ref{r_20_q_6} насколько уточненная оценка ближе к реальному значению невязки, чем ее глобальная версия. 

\begin{figure}[h]
	\centering
	\includegraphics[height=0.28\paperheight]{"compare_radius_20"}
    \caption{Невязка по функции и оценки для задачи минимизации \eqref{sphere_cover_strongly} для $Q$ радиуса 6.}
    \label{r_20_q_6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[height=0.25\paperheight]{"q_unlim"}
    \caption{Работа уточненной оценки \eqref{adaptive_estimation_f} для задачи минимизации \eqref{sphere_cover_strongly} для шара $Q = \mathbb{R}^n$.}
    \label{q_unlim}
\end{figure}

Также появляется интересная возможность, использовать ее в тех случаях, когда константу Липшица ($M$) целевой функции невозможно или сложно оценить. В качестве примера используется случай, когда $Q = \mathbb{R}^n$, в таком случае значение $M$ неограниченно и уходит в бесконечность. Именно этот случай показан на рис. \ref{q_unlim}. 

\begin{figure}[h]
	\centering
	\includegraphics[height=0.25\paperheight]{"q_unlim_raw"}
    \caption{Поведение невязки функции в отсутствии усреднения для задачи \eqref{sphere_cover_strongly} для шара $Q = \mathbb{R}^n$}
    \label{non_avg}
\end{figure}

Для общего понимания показано поведение неусредненной версии невязки на рис. \ref{non_avg}. Такое поведение является следствием субградиентой природы задачи. 

Именно субградиентная природа методов и необходимость изменять алгоритм проектирования привела к необходимости реализации собственной библиотеки для проведения этих и последующих экспериментов. В большинстве встроенных методов нет возможности задания специальных  множеств и при работе с задачами, имеющими субградиентную природу, такие библиотеки как scipy выдают некорректный результат. Также зачастую подобные библиотеки имеют сложное внутреннее представление для проверки необходимых свойств и вычисления необходимых глобальных констант. Такой подход позволяет им работать с большим количеством возможных постановок поддерживаемых классов задач. Как правило оптимизации подобных библиотек направлены на работу с популярными постановками, например задача минимизации квадратичной формы. В более сложных случаях собственноручно реализованный метод, даже без значительных оптимизаций под конкретную задачу, показывает меньшее время работы. 


\underline{\textbf{Третья глава}} посвящена результатам полученным с использованием техники рестартов. Предоставляются результаты сравнения ранее полученных оценок с оценками, полученными при помощи некоторого обобщения острого минимума. Также предложен механизм рестартов для зеркального спуска с использованием условия $\gamma$-роста. 

\underline{\textbf{В парафе 3.1}} вводятся такие понятие как острый минимум и его обобщенная вариация. Также приводятся необходимые для понимания результаты, полученные моими соавторами в работе \cite{sharp22}. 

Говорят, что $f$ удовлетворяет условию острого минимума, если
\begin{gather}\label{sm}
    f(x) - f(x_*) \geq \alpha \min_{x_* \in X_*} \|x- x_*\|_2 \quad \forall x \in Q
\end{gather}
для некоторого фиксированного $\alpha >0$ и $f(x_*) = f^* = \min\limits_{x\in Q} f(x)$ для всякого $x_* \in X_*$, где $Q$ --- выпуклое и замкнутое подмножество $\mathbb{R}^n$, $X_*$ --- компакт и $\|\cdot\|_2$ --- евклидова норма. 

При таком допущении удается предложить субградиентный метод с гарантией  линейной скорости сходимости в случае доступности информации о точном значении $f^*$ \cite{6} без использования с теоретических оценках скорости сходимости  параметра размерности пространства. Условие острого минимума верно, например, для задачи проектирования точки на выпуклый компакт. Однако требование доступности $f^*$ довольно ограничительно. Здесь необходимо ввести несколько понятий, на основе которых будут доказаны оценки \cite{sharp22} моими соавторами: С.С. Аблаев, Ф.С. Стонякин, М.С. Алкуса, И.В. Баран. Таким является некоторое обобщение условия острого минимума
\begin{gather}\label{eq_gen_sharp}
    f(x) - \overline{f} \geq \alpha \min_{x_* \in X_*} \|x - x_* \|_2 - \Delta,
\end{gather}
где $\overline{f}$  --- это некоторое приближение минимального значения функции $f^*$, причём $\overline{f} \geq f^*$. Такое обобщение позволяет несколько расширить класс применимости субградиентных методов для задач с острым минимумом и шагом Б.Т. Поляка. Например, оно может покрыть постановку задачи с неточной информацией о $f^*$. При этом выводятся оценки качества выдаваемого решения субградиентным методом с <<неточным>> аналогом шага Б.Т. Поляка для задач с неизвестной константой Липшица целевой функции. Такой подход, связанный с использованием в теоретических результатах локальных аналогов глобальных характеристик целевой функции (в данном случае константа Липшица $f$) позволяет применять полученные результаты и к более широким классам задач с необязательно липшицевыми целевыми функциями.  

Напомним \cite{7}, что $f$ называется \textit{слабо $\beta$-квазивыпуклой} относительно точки минимума $x_{*}$ задачи \eqref{min_q} на множестве $Q$, если для произвольного $x\in Q$ выполнено неравенство:
\begin{gather}\label{eqquasiconv}
f(x_{*})\geqslant f(x)+\frac{1}{\beta} \langle \nabla f(x), x_{*}-x \rangle,
\end{gather}
где $\nabla f(x)$~--- произвольный субградиент $f$ в точке $x$.

На основе этого моими соавторами была доказана теорема в \cite{sharp22}:
\begin{theorem}\label{theorem1}
Пусть $f$ --- слабо $\beta$-квазивыпуклая функция и для задачи \eqref{min_q} с условием \eqref{eq_gen_sharp} используется метод \eqref{orig} c шагом
$h_k = \dfrac{\beta(f(x_k) - \overline{f})}{\| \nabla f(x_k) \|_2^2}$. Пусть также $\forall i \geq 0$ верно $\alpha^2 \beta^2 \leq 2 \| \nabla f(x_i) \|_2^2$. Тогда верно неравенство:
\begin{gather}\label{adaptive_estimate}
    \begin{aligned}
    \min_{x_* \in X_*} \|x_{k+1} - x_* \|_2^2 \leq &  \prod_{i=0}^k \left ( 1 - \frac{\alpha^2\beta^2}{2 \| \nabla f(x_i) \|_2^2} \right ) \min_{x_* \in X_*} \|x_0 - x_* \|_2^2 + \\& 
    \qquad \qquad \qquad \qquad + \sum_{i=0}^{k-1} \prod_{j=i+1}^k \left ( 1 - \frac{\alpha^2\beta^2}{2 \| \nabla f(x_j) \|_2^2} \right )\Delta_i + \Delta_k,
    \end{aligned}
\end{gather}
где $\Delta_k = \frac{\Delta^2}{2 \| \nabla f(x_k) \|_2^2}$ для всякого $k \geqslant 0$.
\end{theorem}

\underline{\textbf{В парафе 3.2}} проводится исследование, сравнивающее специфику работы методов зеркального спуска в двух различных вариациях: 
\begin{enumerate}
    \item с использованием сильной выпуклости
    \item с использованием обобщенного условия острого минимума \eqref{eq_gen_sharp}
\end{enumerate}
Также описывается проблематика острого минимума и возможность использования этого свойства для ускорения метода при помощи механизма рестартов.

Теперь перейдём к экспериментальному сравнению предложенных подходов для задач с $\Delta$-острым минимумом с работой известного субградиентного метода \cite{Bach_2012} для некоторых сильно выпуклых задач. Выбор класса сильно выпуклых задач и метода \cite{Bach_2012} для сравнения обусловлен известными и  применимыми на практике теоретическими оценками качества приближённого решения по аргументу. Напомним адаптивную оценку для сильно выпуклых функций, полученную ранее в \ref{ThmBachAdaptive}:
\begin{equation}
    f(\widehat{x}) - f(x_*) \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1},
\end{equation} 

Для сравнения скорости сходимости метода \cite{Bach_2012} и полученной в теореме \ref{ThmBachAdaptive} оценки с предложенной вариацией субградиентных методов для задач с $\Delta$-острым минимумом в теореме \ref{theorem1} проведены численные эксперименты для задачи о наименьшем покрытии точек шаром для $2$-сильно выпуклой функции, использовавшаяся ранее \eqref{sphere_cover_strongly}
\begin{gather}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2^2, \|x - a_1\|_2^2, ..., \|x - a_m\|_2^2\right\},
\end{gather}
а также для не сильно выпуклой (но выпуклой) функции
\begin{gather}\label{sphere_cover}
    f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\right\}.
\end{gather}

Начнём с иллюстрации преимуществ адаптивной оценки метода \cite{Bach_2012} из теоремы \ref{ThmBachAdaptive}. Будем рассматривать множество Q, которое равно евклидову шару с центром в 0. Начальная точка выбиралась случайно, но внутри Q. На рис. \ref{res_ex_strong_r5} ниже показано поведение и характер убывания для оригинальной оценки (\ref{orig_estimation_f}) --- сплошная линия, адаптивной оценки (\ref{adaptive_estimation_f}) --- штрих-пунктирная линия и непосредственно невязки по функции и по аргументу соответственно --- штриховая линия. На рис. \ref{res_ex_strong_r5} показано поведение глобальной оценки, адаптивной и невязки по функции и аргументу в случае ограниченного $Q (R = 5)$. Данный график наглядно демонстрирует, насколько более точной может оказаться адаптивная оценка (\ref{adaptive_estimation_f}) для задачи \eqref{sphere_cover_strongly} и позволяет оценить скорость сходимости в логарифмических шкалах.

\begin{figure}[h]
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{x_discr_rad_5_q_4_it_70_000_dim_1000.png}
    \endminipage\hfill
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{f_discr_rad_5_q_4_it_70_000_dim_1000.png}
    \endminipage\hfill
    \caption{Результаты решения задачи минимизации \eqref{sphere_cover_strongly}, где  $n= 1\,000, r = 5$ и  шар $Q$ радиуса 4.}
    \label{res_ex_strong_r5}
\end{figure}

Теперь перейдём к выпуклой постановке \eqref{sphere_cover} с целью исследования эффективности предложенного в теореме \ref{theorem1} субградиентного метода с $\Delta$-острым минимумом. К существующему набору точек, представленных для покрытия, с известным значением центра добавим дополнительную точку, которая находится вне исходного шара достаточно близко к границе (удалена не более, чем на $\Delta > 0$). Данный подход позволяет оценить <<приближённое>> значение минимума $\overline{f}$, что позволит применить описанный ранее субградиентный метод с $\Delta$-острым минимумом. При этом новое значение минимума останется внутри исходной сферы. Поскольку оптимальное значение функции --- это радиус искомого шара, покрывающего все точки, а $x_*$ всегда будет расположена внутри него, то для всякого $x$ верно неравенство $ f(x) \geq \| x - x_*\|_2$. Рассмотрим целевую функцию вида
\begin{gather}\label{allpha_sphere_cover}
    f(x) := \alpha \max_{x\in Q}\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\}.
\end{gather}
Тогда значение $\Delta$ можно оценить  из (\ref{eq_gen_sharp}): 
    $f(x) - \overline{f} \geq \alpha\|x- x_*\|_2 - \Delta, \quad \Delta \geq \overline{f}$.

Отметим, что данная постановка значительно влияет на величину теоретической оценки качества решения (\ref{adaptive_estimate}) для метода \eqref{orig}.
Наиболее значимый вклад в оценку (\ref{adaptive_estimate}) дает последнее слагаемое $\frac{\Delta^2}{2\|\nabla f(x_k)\|^2_2}$, причём 
$     \Delta \sim \overline{f} \sim \alpha \|\overline{x}-a\|_2 $ и 
$     \|\nabla f(x_k)\|_2 = \alpha $. Поэтому последнее слагаемое пропорционально радиусу шара, соответсвующему <<приближённому>> решению. Это и подтверждается экспериментально. Для сравнения, ниже на рис. \ref{res_sharp_convex} и \ref{res_strong_convex} приведены результаты работы для того же набора входных точек, которые необходимо покрыть в обоих постановках --- (\ref{allpha_sphere_cover}) и (\ref{sphere_cover_strongly}). Начальная точка также одна и та же. Сравниваются методы \eqref{adaptive_estimate} и \eqref{orig}. Первый из этих методов обеспечивает сходимость буквально за 10 итераций к <<приближённому>> решению с заданной точностью и даже позволяет эту точность повысить. Второй же метод достигает схожих (с геометрической точки зрения) результатов за значительно большее количество итераций, однако он позволяет повышать точность приближённого решения на дальнейших итерациях.

\begin{figure}[h]
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{sharp_convex_x.png}
    \endminipage\hfill
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{sharp_convex_f.png}
    \endminipage\hfill
    \caption{ Результаты решения задачи минимизации (\ref{allpha_sphere_cover}), где  $n= 1\,000, r = 0.7525, \alpha = 0.6$.}
    \label{res_sharp_convex}
\end{figure}

\begin{figure}[h]
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{strong_convex_small_rad_x.png}
    \endminipage\hfill
    \minipage{0.49\textwidth}
    \includegraphics[width=\linewidth]{strong_convex_small_rad_f.png}
    \endminipage\hfill
    \caption{ Результаты решения задачи минимизации (\ref{sphere_cover_strongly}), где  $n= 1\,000, r = 0.7525$.}
    \label{res_strong_convex}
\end{figure}

Подтверждение данного теоретического наблюдения хорошо иллюстрируется на рис. \ref{res_sharp_convex} и \ref{res_strong_convex}. На рис. \ref{res_sharp_convex} показано поведение субградиентного спуска, использующего $\Delta$-острый минимум (теорема \ref{eqquasiconv}), а именно --- быстрая сходимость к <<приближенному>> решению. Штрих-пунктирная линия соответствует оценке \eqref{eq_gen_sharp}, а штриховая --- невязке по функции и аргументу. На рис. \ref{res_strong_convex} показано поведение метода для той же задачи, но с использованием сильно выпуклого целевого функционала (теорема \ref{ThmBachAdaptive}). Скорость убывания уже не столь высокая, но точность получаемого решения в итоге выше. Сплошная линия --- это глобальная оценка \eqref{orig_estimation_f}, штрих-пунктирная --- адаптивная \eqref{adaptive_estimation_f}, а штриховая --- невязка по функции и аргументу.

Тем не менее, сравнение с известным точным решением $x_*$, а также график динамики значения целевой функции показывает, что за малое число шагов (значительно меньшее, чем для метода \eqref{orig}) реализация метода \eqref{adaptive_estimate} приводит к неплохому качеству приближённого решения. При этом, однако, для метода \eqref{adaptive_estimate} после достижения такого уровня дальнейшее повышение качества выходной точки в отличие от метода \eqref{orig} уже не наблюдается.

\underline{\textbf{В парафе 3.3}} вводится аналог условия острого минимума, а именно - условие гамма роста. На его основе к методу классического зеркального спуска применяется схема рестартов, что позволяет улучшить получаемые оценки.

Воспользуемся формулировкой, предложенной в \fixme{не помню из какой статьи}:
\begin{theorem} \label{vanilla_mirror}
    Пусть $f$ --- является $M$-липщицевой на $Q$ относительно некоторой функции Брегмана $V_d(x, y)$ c 1-сильно выпуклой прокс-функцией $d(x)$. Тогда можно задать метод следующим образом:
    \begin{equation} \label{mirr_upd}
        x_{k+1} = \arg \min_{x \in Q} {\left[ f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{h_k} V_d(x, x_k)\right]},
    \end{equation}
    где $\{ h_k \}$ - последовательность размеров шагов.
    Для него справедлива следующая оценка скорости сходимости:
    \begin{equation} \label{general_est}
        \min_{0\leq k \leq N} f(x_k) - f(x) \leq \frac{\frac{1}{2} M^2 \sum_{k=0}^N h_k^2 + V(x, x_0)}{\sum_{k=0}^N h_k}
    \end{equation}
\end{theorem}


\begin{remark}
    Если в теореме \ref{vanilla_mirror} выбрать шаг следующим образом:
    \begin{equation} \label{mirr_step}
        h_{k} = \frac{\sqrt{2 V(x_*, x_0)}}{M\sqrt{N}},
    \end{equation}
    то скорость сходимости можно оценить так:
    \begin{equation} \label{mirr_est}
        f(\widehat{x_N}) - f(x_*) \leq \frac{M\sqrt{2V(x_*, x_0)}}{\sqrt{N}}
    \end{equation}
\end{remark}
Если функция обладает дополнительными свойствами, аналогичными <<острому минимуму>>,  то становится возможным применение техники рестартов. Используем аналог данного условия и вслед за Шапиро–Немировским (см. \cite{shapiro_2005} и \cite{shapiro_2021} ) введем  условие условие $\gamma$-роста ($\gamma > 1$):
\begin{definition} \label{gamma-growth}
   $f$ --- удовлетворяет условию $\gamma$-роста тогда и только тогда:
   \begin{equation}
       f(x) - f(x_*) \geq \mu_{\gamma}(V(x_*,x))^{\gamma/2}
   \end{equation}
\end{definition}

В данных предположениях была сформулирована и доказана следующая теорема:
\begin{theorem} \label{simple_restart}
    Пусть $f$ --- удовлетворяет условию $\gamma$-роста (\ref{gamma-growth}) и также является $M$-липшицевой на $Q$ относительно некоторой функции Брегмана $V_d(x, y)$. В таком случае Алгоритм \ref{alg:rest_gamma} достигнет точности $\epsilon$ не более чем за:
    \begin{equation}
    \begin{aligned}
       N =\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{V(x_*, x_0^0)}{\varepsilon}} \text{ при } \gamma = 1 \\
       N = \frac{2 M^2}{\mu_{\gamma}^2 \varepsilon^{(\gamma-1)} } \left[1 - \frac{1} {V(x_*, x_0^0)^{(\gamma - 1)}}\right] \text{ при } \gamma > 1
    \end{aligned}
    \end{equation}
    причем будут справедливы неравенства:
    \begin{equation}
       V(x_*, \widehat{x_p}) \leq \varepsilon
    \end{equation}
    и
    \begin{equation}
        f(\widehat{x_p}) - f(x_*) \leq  \langle \nabla f(\widehat{x_p}), \widehat{x_p} - x_* \rangle \leq M \sqrt{ 2 V(x_*, \widehat{x_p})} \leq M \sqrt{2 \varepsilon}  
    \end{equation}
\end{theorem}

\begin{algorithm}[htp]
    \caption{Рестарты зеркального спуска при условии $\gamma$-роста.}
    \label{alg:rest_gamma}
    \KwData{$\varepsilon > 0$}
    \KwResult{$x_p$}
    $p \gets 0$\;
    $V(x_*, x_0) \gets V(x_*,x_0^0)$\;
    \While{$p > \log_2\left(\frac{V(x_*, x_0^0)}{\varepsilon}\right).$}{
        $x_{p}$ --- результат работы метода \ref{mirr_upd} с шагом \ref{mirr_step} и количеством шагов $N_{p} = \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{p(1 - \gamma)}} V(x_*, x_0^0)^{1 - \gamma}$\;
        $x_0 = \widehat{x_p}$\;
        $V(x_*, x_0) \gets \frac{1}{2^{p+1}}V_{0}(x_*, x_0^0)$\;
        $p=p+1$\;
    }
\end{algorithm}

Также путем замены $\delta := M \sqrt{2 \varepsilon}$ в предыдущей теореме, можно сформулировать следующую теорему, уточнение:
\begin{theorem}
    Пусть $f$ --- удовлетворяет условию $\gamma$-роста (\ref{gamma-growth}) и также является $M$-липшицевой на $Q$ относительно некоторой функции Брегмана $V_d(x, y)$. В таком случае Алгоритм \ref{alg:rest_gamma} достигнет точности $\delta$ не более чем за:
    \begin{equation}
        \begin{aligned}
           N =\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{2 M^2 V(x_*, x_0^0)}{\delta^2}} \text{ при } \gamma = 1 \\
           N = \frac{2^\gamma M^{2\gamma}}{\mu_{\gamma}^2 \delta^{2(\gamma-1)} } \left[1 - \frac{1} {V(x_*, x_0^0)^{(\gamma - 1)}}\right] \text{ при } \gamma > 1
        \end{aligned}
    \end{equation}
    причем будут справедливы неравенства:
    \begin{equation}
       f(\widehat{x_p}) - f(x_*)  \leq \delta 
    \end{equation}
    и
    \begin{equation}
       V(x_*, \widehat{x_p}) \leq \frac{\delta^2}{2 M^2}
    \end{equation}
\end{theorem}

\underline{\textbf{В парафе 3.4}} предлагается развитие полученных в предыдущем параграфе результатов, а именно формулируются критерии остановки, которые на практике способны в значительной мере сократить количество необходимых итераций.

\begin{remark} \label{adapt_mirror}
    Для теоремы \ref{vanilla_mirror} можно уточнить полученную оценку: 
    \begin{equation} \label{adapt_est}
        \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N h_k^2 \norm{\nabla f(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{V(x_*, x_0) }{\sum_{k=0}^N h_k}
    \end{equation}
\end{remark}

\begin{remark}
    Если в \eqref{adapt_est} выбрать шаг следующим образом:
    \begin{equation} \label{eps_step}
        h_{k} = \frac{\varepsilon}{\norm{\nabla f(x_k)}^2},
    \end{equation}
    и воспользоваться критерием остановки:
    \begin{equation} \label{stop_crit}
        \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2} \geq \frac{2 V(x_*, x_0)}{\varepsilon^2}    
    \end{equation}
    то скорость сходимости можно оценить так:
    \begin{equation} 
    \begin{aligned} \label{mirr_est}
        \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N \frac{\varepsilon^2}{\norm{\nabla f(x_k)}^4} \norm{\nabla f(x_k)}^2} {2 \sum_{k=0}^N \frac{\varepsilon}{\norm{\nabla f(x_k)}^2}} + \frac{V(x_*, x_0) }{\sum_{k=0}^N \frac{\varepsilon}{\norm{\nabla f(x_k)}^2}} = \\
        = \frac{\varepsilon} {2} \frac{ \sum_{k=0}^N \frac{1}{\norm{\nabla f(x_k)}^2}} {\sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}} + \frac{V(x_*, x_0) }{\varepsilon \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}}  = \frac{\varepsilon}{2} + \frac{V(x_*, x_0) }{\varepsilon \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}} \leq \varepsilon
    \end{aligned}
    \end{equation}
 \end{remark}
 В дальнейших рассуждениях будем обозначать 
 \[
    x_{min}^j  := \min_{0\leq k \leq N} f(x_k) \;\;\; \text{на} \;\; j\text{-м рестарте}.
 \]

 \begin{algorithm}[htp]
     \caption{Рестарты зеркального спуска при условии $\gamma$-роста с критерием остановки.}
     \label{alg:rest_criteria}
     \KwData{$\varepsilon > 0$}
     \KwResult{$x_p$}
     $p \gets 0$\;
     $V(x_*, x_0) \gets V(x_*,x_0^0)$\;
     \While{$p = \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{V(x_*, x_0^0)}{2}\right]}.$}{
         $x_{p}$ --- результат работы метода \ref{mirr_upd} с шагом \ref{eps_step} и критерием остановки $\sum_{k=0}^{N_p} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq  \frac{2 \cdot 2^{\gamma} \cdot 2^{p\gamma} V(x_*, x_0) }{\mu_{\gamma}^2 V(x_*, x_0^0)^{\gamma}}  $\;
         $x_0 = x_{min}^p$\;
         $p=p+1$\;
     }
 \end{algorithm}
 \begin{theorem}
     Пусть $f$ --- удовлетворяет условию $\gamma$-роста (\ref{gamma-growth}) и также является $M$-липшицевой на $Q$ относительно некоторой функции Брегмана $V_d(x, y)$. В таком случае Алгоритм \ref{alg:rest_criteria} достигнет точности $\varepsilon$ не более чем за:
     \begin{equation}
        N =  \frac{4 M^2}{\mu_{\gamma}^2} \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{V(x_*, x_0^0)}{2}\right]} \text{ при } \gamma = 1
    \end{equation}
    или
    \begin{equation}
        N = \frac{8  M^2}{\mu_{\gamma}^2 (2 - 2^{\gamma})} \left[ \left(\frac{2}{V(x_*, x_0^0)}\right)^{(\gamma - 1)}  - \left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2\gamma - 2}{\gamma}} \right] \text{ при } \gamma > 1
    \end{equation}
 \end{theorem}

 Полученный результат имеет схожие оценки с предыдущей теоремой \ref{simple_restart}, однако на практике критерий остановки значительно сокращает количество итераций, необходимых для достижения заданной точности $\varepsilon$. Также при помощи данного метода легко прогнозируется точность перед каждым рестартом метода, что удобно для контроля за правильностью работы метода. Также подобная информация позволяет переключаться между несколькими модификациями метода, чередуя не только "эффективные" и "аналитические" шаги, но и перезапуски.  

\FloatBarrier
\pdfbookmark{Заключение}{conclusion}                                  % Закладка pdf
В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\pdfbookmark{Литература}{bibliography}                                % Закладка pdf


\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
    \renewcommand{\bibname}{\large \bibtitleauthor}
    \nocite{*}
    \insertbiblioauthor           % Подключаем Bib-базы
    %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
    % Цитирования.
    %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
    %  * Если не соблюдать порядок "как для \printbibliography", нумерация в `\insertbiblioauthor` будет кривой.
    %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
    \nocite{Stonyakin_2021}%
    \nocite{yakovlev2019algorithms}%
    \nocite{sharp22}%
    %

    \ifnumgreater{\value{usefootcite}}{0}{
        \begin{refcontext}[labelprefix={}]
            \ifnum \value{bibgrouped}>0
                \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
            \else
                \insertbiblioauthor      % Вывод всех работ автора
            \fi
        \end{refcontext}
    }{
        \ifnum \totvalue{citeexternal}>0
            \begin{refcontext}[labelprefix=A]
                \ifnum \value{bibgrouped}>0
                    \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
                \else
                    \insertbiblioauthor      % Вывод всех работ автора
                \fi
            \end{refcontext}
        \else
            \ifnum \value{bibgrouped}>0
                \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
            \else
                \insertbiblioauthor      % Вывод всех работ автора
            \fi
        \fi
        %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
        \begin{refcontext}[labelprefix={}]
            \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
        \end{refcontext}
        % Невидимый библиографический список для подсчёта количества внешних публикаций
        % Используется, чтобы убрать приставку "А" у работ автора, если в автореферате нет
        % цитирований внешних источников.
        \printbibliography[heading=nobibheading, section=0, env=countexternal, keyword=biblioexternal, resetnumbers=true]%
    }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
