
{\actuality} 
Задачи оптимизации применяются в самых разных областях человеческой деятельности от экономики до машинного обучения. Например, методы оптимизации позволяют решать системы уравнений, моделировать экономические процессы, обучать нейросети и анализировать риски в банках. Такое многообразие приложений связано с тем, что задача оптимизации - это поиск лучшего доступного решения, которое удовлетворяет требованиям исследуемой модели. Методы оптимизации предлагают обширный выбор инструментов, которые распределены по характеристикам моделей и предъявляемых к ней ограничений. Общая постановка задачи минимизации имеет вид
$$
    \min_{x\in Q} {f\left( x \right)}.
$$
Здесь и всюду далее $n$ --- это размерность пространства переменных, а $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$. Учет размерности задачи в современном мире стал необходимостью, поскольку модели усложняются и количество учитываемых параметров растет экспоненциально. Подобный рост размерности привел к росту популярности численных методов оптимизации градиентного типа. Речь о методах, в которых на итерациях может использоваться информация о значении целевой функции ее градиента. К недостаткам данного типа методов можно отнести необходимость прибегать к информации о функциональных свойствах для исследуемых задач, таких как гладкость, липшицевость и сильная выпуклость. Напомним определения используемых далее функционального свойства. Одним из них является условие Липшица вида
$$
    |f(y) - f(x)| \leq M \norm{y-x}_2 \;\;\; \forall x, y \in Q
$$
при некотором фиксированном $M > 0$, а норма $\norm{\cdot}_2$ --- это евклидова норма. Гладкость целевой функции определяется как условие Липшица для градиента:
$$
    \norm{\nabla f(x) - \nabla f(y)}_2 \leq L \norm{x - y}_2 \;\;\; \forall x, y \in Q
$$
при некотором фиксированном $L > 0$.
Данное условие влечет неравенство, более часто применяемое на практике,
\begin{equation}\label{l_grad}
    f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{L}{2} \|x - y \|_2^2 \quad   \forall x, y \in Q.
\end{equation}
Часто полезным будет свойство сильной выпуклости функции \cite{Pol66}:
$$
\begin{aligned}
    f(\lambda x + &(1 - \lambda)y) \le\\ 
    &\lambda f(x) + (1 - \lambda)f(y) - \frac{\mu}{2} \lambda (1 - \lambda)\|x - y\|^2_2 \;\;\; \forall x, y \in Q,
\end{aligned}
$$
где $0 \le \lambda \le 1$, для некоторого $\mu > 0$, называемого константой сильной выпуклости.
Для таких функций верно неравенство
\begin{equation}
    f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2} \norm{x - y}_2^2 \leq f(y) \;\;\; \forall x, y \in Q.
\end{equation}

Одна из причин популярности градиентных методов - это низкая затратность и возможность отказаться в оценках скорости сходимости от размерности задачи, что делает их применение эффективным для многих задач. В широком смысле под эффективностью метода оптимизации понимается время, необходимое для получения достаточно хорошего решения. Однако, подобная величина зависит от огромного количества аспектов, не имеющих прямого отношения к алгоритму поиска достаточно хорошего решения. Такими аспектами могут являться мощность вычислительного устройства, доступная точность представления чисел, необходимое количество памяти и т.д. Чтобы разделить технический и теоретический аспекты, под эффективностью численного метода оптимизации на определенном классе задач часто понимают именно эффективность в смысле Бахвалова-Немировского \cite{Nemirovski1979} --- число обращений по ходу работы метода к \textit{оракулу}, достаточное для достижения приемлемой точности. 
Оракулом называется подпрограмма расчета значений целевой функции (градиента, гессиана или некоторой заменяющей их величины) с необходимой точностью. Как правило, данное обращение является наиболее <<дорогостоящей>> частью шага. В некоторых ситуациях оптимизация работы оракула является необходимостью для приемлемой работы метода, этот аспект будет более подробно обсуждаться на примере в рамках 1й главы в пункте 1.3.5.

Таким образом, аналитическая сложность оптимизационной задачи для метода на заданном классе характеризуется количеством запросов к оракулу для гарантии нахождения приближенного решения с заранее заданной точностью $\varepsilon > 0$. Что можно более формально описать 
$$
    \norm{x_N - x_*}_2 \leq \varepsilon, 
$$
где $x_N$ --- точка выхода алгоритма после $N$ итераций используемого метода, а $x_*$ --- искомое точное решение задачи. Также часто рассматривают точность с точки зрения значения функции, а именно
$$
    |f(x_N) - f(x_*)| \leq \varepsilon.
$$

Оракульный подход очень удобен для анализа сложности и сравнения численных методов. Если ответы оракула являются локальными и не накладывают никаких ограничений, не требуют каких-либо свойств от исследуемой функции, то такой подход также называют \textit{концепцией чёрного ящика}.

Cуществует несколько типов оракульных оценок:
\begin{itemize}
    \item верхняя --- количество обращений к оракулу не более данного значения (хуже не будет),
    \item нижняя --- не менее данного числа обращений (лучше не будет),
    \item оптимальная соответствует ситуации, когда нижняя и верхняя оценки совпадают с точностью до константных множителей, не зависящих от параметров задачи, таких как размерность, константа гладкости и прочее.
\end{itemize}

Ранее многие задачи относились к задачам \textit{небольшой размерности}, но в последнее время размерность задач увеличивается. Задачей \textit{небольшой размерности} обычно называют задачу, когда возможно $N \geq n$, где $N$ --- количество обращений к оракулу, а $n$ --- размерность пространства. На практике к задачам небольшой размерности можно отнести задачи, имеющие размерность в несколько сотен. Неплохую эффективность для таких задач имеют методы секущей гиперплоскости, к которым относятся метод центров тяжести и метод эллипсоидов. Однако их оптимальные оценки $N \sim \mathcal{O}\left(n \log{n}\right)$ существенно зависят от размерности пространства \cite{bubeck_2015}. 
Эта зависимость приводит к высоким требованиям по необходимой памяти и необходимости увеличения количества итераций с ростом размерности пространства для сохранения той же точности. Градиентные методы в противоположность им обладают сравнительно скромными требованиями по необходимой памяти, и соответствующие оценки не зависят от размерности задачи. Отметим также, что лучшие верхние оценки на классе гладких выпуклых функций известны для ускоренных методов \cite{Nesterov1983}.

Как уже было отмечено ранее, недостатком данного типа методов является необходимость ряда функциональных свойств у исследуемой функции. Выпишем известные оптимальные оценки сложности задач выпуклой оптимизации в зависимости от предположений о гладкости задачи.

\begin{table}[h]
    \caption{Оптимальные оценки количества обращений к субградиенту.}
    \label{est_tbl}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
         & \makecell{$|f(y) - f(x)| \leq$ \\ $\leq M \| y - x \|_2$} & \makecell{$\|\nabla f(y) - \nabla f(x)\|_2 \leq $\\ $\leq L \| y - x \|_2$} \\
        \hline
        $f(x)$ -- выпукла & $\mathcal{O} \left( \frac{M^2 R^2}{\varepsilon^2} \right)$ & $\mathcal{O} \left( \sqrt{\frac{L R^2}{\varepsilon}} \right)$ \\
        \hline
        \makecell{$f(x)$ -- $\mu$-сильно \\ выпукла в $\| \cdot \|_2$ - норме} & $\mathcal{O} \left( \frac{M^2}{\mu \varepsilon} \right)$ & $\mathcal{O} \left( \sqrt{\frac{L}{\mu}} \left[\ln{\frac{\mu R^2}{\varepsilon}}\right] \right)$ \\
        \hline
    \end{tabular}
\end{table}
В таблице \ref{est_tbl} $R$ --- это $\|x_0 - x_*\|_2 $, а $x_0$ --- стартовая точка алгоритма. Как видим, оценки в гладком случае существенно лучше, чем в негладком. В случаях, когда функционалы не обладают достаточными свойствами гладкости, что проявляется во многих задачах, актуальны вопросы разработки соответствующих эффективных методов. Для негладких задач нет возможности гарантировать высокую скорость сходимости. В данной работе рассматриваются некоторые типы негладких задач для которых исследуются возможные усовершенствования известных результатов при дополнительных условиях. Важным направлением является исследование обобщений уже известных классов задач, которые позволяют сохранить приемлемые вычислительные гарантии сходимости численных методов.

\iffalse
    Для улучшения оценок для негладких задач существует несколько подходов. Например, выделяют специальные подклассы задач, при помощи, например, условия острого минимума, предложенного в конце 1960-х годов Б.Т. Поляком \cite{Polyak1969}. Стоит подчеркнуть, что подобные условия зачастую выставляют более жесткие требования к задаче. 
\fi

Напомним, что для негладких задач в качестве обобщения понятия градиента рассматривают понятие субградиента. Вектор $g$ называют субградиентом в точке $x_0$, если
$$
    f(x) \geq f(x_0) + \langle g, x - x_0 \rangle \;\;\; \forall x \in Q.
$$

Известно, что оптимальная оценка субградиентного метода сублинейна на классах как выпуклых, так и сильно выпуклых липшицевых задач \cite{Bach_2012}. Это трудно считать достаточным для ряда приложений. В связи с этим для улучшения оценок скорости сходимости субградиентных методов используют такие допущения, как, например, условие острого минимума. Некоторые новые результаты в этом направлении изложены в данной работе.
\iffalse
    Подобные условия позволяют сделать несколько начальных эффективных шагов, что было продемонстрировано в работе \cite{sharp22} и будет затронуто во второй главе данной работы. Также в упомянутой главе было проведено сравнение оценок скорости субградиентных для классов задач с острым минимумом и сильной выпуклостью. Острый минимум может приводить к лучшей оценке скорости сходимости, однако данное свойство требует информации о точном решении, что является существенным ограничением.

    Оценки, получаемые при помощи условия острого минимума, обладают лучшими свойствами сходимости, однако в отсутствии знаний о точном решении нет возможности повысить точность выше предварительно заданной точности, известной для приближенного решения. Оценки скорости сходимости, использующие сильную выпуклость, не обладают столь впечатляющими оценками скорости сходимости, однако позволяют добиться гораздо большей точности. 
\fi

Отметим важное и развиваемое в данной работе обобщение оптимальных результатов для субградиентного метода на случай задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость). Такая прокс-функция, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Напомним важное понятие дивергенции (расхождения) Брэгмана. Предполагается, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая некоторое расстояние, и соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
\[
    V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
\]

При помощи дивергенции Брэгмана вводятся такие необходимые понятия, как \textit{относительная липишицевость, относительная сильная выпуклость, относительная гладкость и условие относительного $\gamma$-роста}. Например, свойство относительной $L$-гладкости функции $f$ обобщает условие $L$-гладкости $f$ (см. \eqref{l_grad}), где квадрат евклидовой нормы в определении заменяется дивергенцией Брэгмана
$$
    f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + L V(y,x) \quad   \forall x, y \in Q,
$$
где $Q$ --- область определения $f$.

Наиболее заметными современными приложениями, обладающими свойствами относительной гладкости и относительной сильной выпуклости, являются задачи распределенной оптимизации в предположении схожести слагаемых \cite{Hendr}. 

Во второй главе данной работы исследуется оценка скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее, рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач с соответствующими условиями относительной липшицевости функционалов. Вариационные неравенства являются важной вехой для работы, например, с лагранжевыми седловыми задачами. Известно, что задача решения вариационного неравенства имеет вид
$$
    \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
$$
где $x_*$ называют слабым решением вариационного неравенства, $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$, $g: Q \longrightarrow \mathbb{R}^n$. Предполагается, что решение $x_*$ существует. Такая постановка приводит к существенно более широкому классу задач, чем минимизационные задачи. Класс вариационных неравенств имеет достаточно широкую область применения в различных областях математики, таких как теория игр, моделирование потоков и математическая экономика. 

В третьей главе вводится понятие относительного $\gamma$-роста целевой функции, которое позволяет улучшить сублинейные оценки скорости сходимости субградиентного метода (зеркального спуска в случае неевклидовой прокс-структуры) для сильно выпуклых функций при помощи рестартов оригинального метода. Предлагается удобный с практической точки зрения алгоритм, использующий адаптивный критерий остановки на каждом из рестартов. 

% {\progress}
% Этот раздел должен быть отдельным структурным элементом по
% ГОСТ, но он, как правило, включается в описание актуальности
% темы. Нужен он отдельным структурынм элемементом или нет ---
% смотрите другие диссертации вашего совета, скорее всего не нужен.

{\aim} данной работы является развитие теории методов оптимизации для задач, не обладающих стандартными условиями гладкости, с упором на анализ субградиентных методов на классе задач с современными аналогами условий Липшица, сильной выпуклости и $\gamma$-роста ($\gamma > 1$).

{\underline{\textbf{Задачи,}}} решаемые в данной диссертации:
\begin{enumerate}
    \item Экспериментальная проверка теоретических результатов для задач распределенной оптимизации, не обладающая стандартными условиями гладкости, недавно полученных соавторами в работе \cite{GorbunovKMR20}.
    \item Экспериментальный анализ методов безградиентного, градиентного и квазинютоновского типов для работы с невыпуклым, вообще говоря, функционалом специальной структуры, а именно практический анализ возможностей эффективной оптимизации для задачи минимизации белка, имеющей различные физические основания,  OPLS force field. 
    \item Вывод оценки скорости сходимости зеркального спуска с использованием адаптивно подбираемых параметров на классе сильно выпуклых задач. Доказательство оценки скорости сходимости зеркального спуска для относительно ограниченных и относительно сильно монотонных операторов на классе вариационных неравенств.
    \item Исследование специальной методики рестартов зеркального спуска для относительно липшицевых задач минимизации с условием относительного $\gamma$-роста. 
\end{enumerate}
В данной работе мы для метода зеркального спуска рассматриваем аналоги условия Липшица, которые позволяют сохранить свойственные липшицевым задачам оптимальные оценки скорости сходимости. Также производится переход к аналогичной постановке задачи в терминах вариационных неравенств, что позволяет дополнительно расширить доступный класс задач. Получены улучшенные оценки скорости сходимости при помощи механизма рестартов зеркального спуска в случае наличия дополнительных условий острого минимума или относительного $\gamma$-роста. Сделан существенный акцент на экспериментальной составляющей.

{\novelty}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Впервые получены оценки скорости сходимости метода зеркального спуска на классе вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами.
  \item Впервые получен адаптивный аналог оптимальной оценки скорости сходимости зеркального спуска для задач минимизации сильно выпуклых функций с использованием локальных аналогов константы Липшица.
  \item Впервые получены оценки скорости сходимости рестартованного метода зеркального спуска для относительно липшицевых задач оптимизации с относительным $\gamma$-ростом при $\gamma > 1$. 
\end{enumerate}

{\defpositions}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Предложен вариант метода зеркального спуска для вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. Доказана инвариантная по размерности пространства оценка скорости сходимости этого метода, оптимальная на указанном классе задач с точностью до умножения на постоянный множитель.
  \item Получена адаптивная оценка скорости сходимости зеркального спуска для задач минимизации сильно выпуклых функций с использованием локальных аналогов константы Липшица. При этом сохраняется оптимальность этой оценки на классе сильно выпуклых липшицевых задач с точностью до умножения на константу. В частности, это позволяет работать и с задачами, не удовлетворяющими условию Липшица.
  \item Введён аналог острого минимума ($\gamma$-роста) с использованием дивергенции Брэгмана. Получена оценка скорости сходимости рестартованного метода зеркального спуска для относительно липшицевых задач оптимизации с относительным $\gamma$-ростом. В ситуации сильно выпуклой прокс-функции предложены адаптивные правила остановки для рестартов исследуемого метода зеркального спуска и получен результат о его скорости сходимости.
\end{enumerate}

{\probation}
Основные результаты работы докладывались~на:
\begin{itemize}
    \item проектной смене <<Современные методы теории информации, оптимизации и управления>> в центре <<Сириус>>, 2021,
    \item QIPA (Quasilinear Equations, Inverse Problems and Their Applications), 2021,
    \item 64-й всероссийской научной конференции МФТИ, 2021,
    \item QIPA (Quasilinear Equations, Inverse Problems and Their Applications), 2018,
    \item 61-й всероссийской научной конференции МФТИ, 2018.
\end{itemize}

{\contribution} Ключевые результаты получены и доказаны автором лично. Также разработана библиотека для анализа и проверки методов оптимизации, обеспечивающая необходимую гибкость настройки. 

\ifnumequal{\value{bibliosel}}{0}
{%%% Встроенная реализация с загрузкой файла через движок bibtex8. (При желании, внутри можно использовать обычные ссылки, наподобие `\cite{vakbib1,vakbib2}`).
    {\publications} Основные результаты по теме диссертации изложены
    в~XX~публикациях,
    X из которых изданы в журналах, рекомендованных ВАК,
    X "--- в тезисах докладов.
}%
{%%% Реализация пакетом biblatex через движок biber
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=1.
        % Процитированные здесь работы:
        %  * подсчитываются, для автоматического составления фразы "Основные результаты ..."
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthor` или `\insertbiblioauthorgrouped`
        %  * нумеруются там в зависимости от порядка команд `\printbibliography` в этом разделе.
        %  * при использовании `\insertbiblioauthorgrouped`, порядок команд `\printbibliography` в нём должен быть тем же (см. biblio/biblatex.tex)
        %
        % Невидимый библиографический список для подсчёта количества публикаций:
        \printbibliography[heading=nobibheading, section=1, env=countauthorvak,          keyword=biblioauthorvak]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorwos,          keyword=biblioauthorwos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopus,       keyword=biblioauthorscopus]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorconf,         keyword=biblioauthorconf]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorother,        keyword=biblioauthorother]%
        \printbibliography[heading=nobibheading, section=1, env=countregistered,         keyword=biblioregistered]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorpatent,       keyword=biblioauthorpatent]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorprogram,      keyword=biblioauthorprogram]%
        \printbibliography[heading=nobibheading, section=1, env=countauthor,             keyword=biblioauthor]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorvakscopuswos, filter=vakscopuswos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopuswos,    filter=scopuswos]%
        %
        \nocite{*}%
        %
        {\publications} Основные результаты по теме диссертации изложены в~\arabic{citeauthor}~печатных изданиях
        % \arabic{citeauthorvak} из которых изданы в журналах, рекомендованных ВАК\sloppy% 
        \ifnum \value{citeauthorscopuswos}>0%
            , \arabic{citeauthorscopuswos} "--- в~периодических научных журналах, индексируемых Web of~Science или Scopus\sloppy%
        \fi%
        \ifnum \value{citeauthorconf}>0%
            , \arabic{citeauthorconf} "--- в~тезисах докладов.
        \else%
            .
        \fi%
        \ifnum \value{citeregistered}=1%
            \ifnum \value{citeauthorpatent}=1%
                Зарегистрирован \arabic{citeauthorpatent} патент.
            \fi%
            \ifnum \value{citeauthorprogram}=1%
                Зарегистрирована \arabic{citeauthorprogram} программа для ЭВМ.
            \fi%
        \fi%
        \ifnum \value{citeregistered}>1%
            Зарегистрированы\ %
            \ifnum \value{citeauthorpatent}>0%
            \formbytotal{citeauthorpatent}{патент}{}{а}{}\sloppy%
            \ifnum \value{citeauthorprogram}=0 . \else \ и~\fi%
            \fi%
            \ifnum \value{citeauthorprogram}>0%
            \formbytotal{citeauthorprogram}{программ}{а}{ы}{} для ЭВМ.
            \fi%
        \fi%
        % К публикациям, в которых излагаются основные научные результаты диссертации на соискание учёной
        % степени, в рецензируемых изданиях приравниваются патенты на изобретения, патенты (свидетельства) на
        % полезную модель, патенты на промышленный образец, патенты на селекционные достижения, свидетельства
        % на программу для электронных вычислительных машин, базу данных, топологию интегральных микросхем,
        % зарегистрированные в установленном порядке.(в ред. Постановления Правительства РФ от 21.04.2016 N 335)
    \end{refsection}%
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=2.
        % Процитированные здесь работы:
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthorimportant`.
        %  * ни на что не влияют в противном случае

        % \nocite{vakbib2}%vak
        % \nocite{patbib1}%patent
        % \nocite{progbib1}%program
        % \nocite{bib1}%other
        % \nocite{confbib1}%conf
    \end{refsection}%
        %
        % Всё, что вне этих двух refsection, это refsection=0,
        %  * для диссертации - это нормальные ссылки, попадающие в обычную библиографию
        %  * для автореферата:
        %     * при usefootcite==0, ссылка корректно сработает только для источника из `external.bib`. Для своих работ --- напечатает "[0]" (и даже Warning не вылезет).
        %     * при usefootcite==1, ссылка сработает нормально. В авторской библиографии будут только процитированные в refsection=0 работы.
}

% \ifsynopsis
% \else
%   \insertbiblioauthor      % Вывод всех работ автора
% \fi
