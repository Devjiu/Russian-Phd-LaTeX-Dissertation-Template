
{\actuality} 
Задачи оптимизации применяются в самых разных областях человеческой деятельности от экономики до машинного обучения. Методы оптимизации позволяют решать системы уравнений и анализировать риски в банках. Такое многообразие приложений связано с тем, что задача оптимизации - это поиск лучшего доступного решения, которое удовлетворяет требованиям исследуемой модели. Методы оптимизации предлагают обширный выбор инструментов, которые распределены по характеристикам моделей и предъявляемых к ней ограничений. Общая постановка задачи минимизации имеет вид
$$
    \min_{x\in \mathbb{R}^n} {f\left( x \right)}.
$$
Здесь и далее $n$ --- это размерность пространства переменных. Учет размерности задачи в современном мире стал необходимостью, поскольку модели усложняются и количество учитываемых параметров растет экспоненциально. Подобный рост размерности привел к росту популярности так называемых численных методов оптимизации градиентного типа, речь о методах в которых на итерациях может использоваться информация о значении целевой функции ее градиента. К недостаткам данного типа методов можно отнести необходимость прибегать к информации о функциональных свойствах для исследуемых задач, таких как гладкость, липшицевость и сильная выпуклость. Напомним используемые далее функциональные свойства, одним из которых является липшицевость. Одно из ее определений:
$$
    |f(y) - f(x)| \leq M \norm{y-x}_2 \;\;\; \forall x, y \in Q,
$$
где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$, а норма $\norm{\cdot}_2$ --- это евклидова норма. Данное свойство ограничивает возможности роста функции. Гладкость определяется как липшицевость градиента. 
$$
    f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{L}{2} \|x - y \|_2^2 \quad   \forall x, y \in Q
$$	
В дальнейшем будет использоваться $L$ как константа $L$-гладкости. Сильная выпуклость определяется так:
$$
    f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2} \norm{x - y}_2^2 \leq f(y) \;\;\; \forall x, y \in Q.
$$
$\mu$ --- это константа сильной выпуклости, иногда это свойство обозначают как $\mu$-сильная выпуклость.

Одна из причин популярности градиентных методов - это высокая скорость сходимости. Определим понятие \textit{оценки скорости сходимости} метода. В широком смысле под эффективностью понимается время, необходимое для получения достаточно хорошего решения. Однако, подобный параметр зависит от огромного количества аспектов, не имеющих прямого отношения к алгоритму поиска достаточно хорошего решения. Такими аспектами могут являться мощность вычислительного устройства, доступная точность представления чисел, необходимое количество памяти и т.д. Чтобы разделить технический и теоретический аспекты, под эффективностью численного метода оптимизации на определенном классе задач часто понимают именно эффективность в смысле Бахвалова-Немировского \cite{Nemirovski1979} --- число обращений по ходу работы метода к \textit{оракулу}. 

Оракулом называется подпрограмма расчета значений целевой функции (градиента, гессиана или некоторой заменяющей их величины) с необходимой точностью. Как правило, данное обращение является наиболее <<дорогостоящей>> частью шага. В некоторых ситуациях оптимизация работы оракула является необходимостью для приемлемой работы метода, этот аспект будет более подробно обсуждаться на примере в рамках 1й главы в пункте 1.3.5.

Таким образом, аналитическая сложность оптимизационной задачи для метода на заданном классе характеризуется количеством запросов к оракулу для гарантии нахождения приближенного решения с заранее заданной точностью $\varepsilon > 0$. Что можно более формально описать 
$$
    \norm{x_N - x_*}_2 \leq \varepsilon, 
$$
где $x_N$ --- точка выхода алгоритма после $N$ итераций используемого метода, а $x_*$ --- искомое точное решение задачи. Также часто рассматривают точность с точки зрения значения функции, а именно
$$
    |f(x_N) - f(x_*)| \leq \varepsilon, 
$$

Этот подход очень удобен для анализа сложности и сравнения численных методов. Тут стоит обратить внимание, что если ответы оракула являются локальными и не накладывают никаких ограничений, не требуют каких-либо свойств от исследуемой функции, то такой подход также называют \textit{концепцией чёрного ящика}.

Cуществует несколько типов оракульных оценок:
\begin{itemize}
    \item верхняя --- количество обращений к оракулу не более данного значения (хуже не будет),
    \item нижняя --- не менее данного числа обращений (лучше не будет),
    \item оптимальная соответствует ситуации, когда нижняя и верхняя оценки совпадают с точностью до константных множителей, не зависящих от параметров задачи, таких как размерность, константа гладкости и прочее.
\end{itemize}

Как уже упоминалось, ранее многие задачи относились к задачам \textit{небольшой размерности}. Задачей \textit{небольшой размерности} обычно называют задачу, когда возможно $N \geq n$, где $N$ --- количество обращений к оракулу, а $n$ --- размерность пространства. На практике к задачам умеренной размерности можно отнести задачи, имеющие размерность в несколько сотен. Наибольшую эффективность для таких задач имели методы секущей гиперплоскости, к которым относятся метод центров тяжести и метод эллипсоидов. Однако их оптимальные оценки $N \sim \mathcal{O}\left(n \log{n}\right)$ имеют существенную зависимость от размерности пространства \cite{bubeck_2015}. 

Эта зависимость приводит к высоким требованиям по необходимой памяти и необходимости увеличения количества итераций с ростом размерности для сохранения той же точности. Градиентные методы в противоположность им обладают сравнительно скромными требованиями по необходимой памяти и соответствующие оценки не зависят от размерности задачи. Отметим также, что лучшие верхние оценки в гладком выпуклом случае известны для ускоренных методов \cite{Nesterov1983}.

Как уже было отмечено ранее недостатком данного типа методов является необходимость ряда функциональных свойств у исследуемой функции. Выпишем известные оптимальные оценки сложности задач выпуклой оптимизации в зависимости от предположений о гладкости задачи.

\begin{table}[h]
    \caption{Оптимальные оценки количества обращений к субградиенту.}
    \label{est_tbl}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
         & \makecell{$|f(y) - f(x)| \leq$ \\ $\leq M \| y - x \|_2$} & \makecell{$\|\nabla f(y) - \nabla f(x)\|_2 \leq $\\ $\leq L \| y - x \|_2$} \\
        \hline
        $f(x)$ -- выпукла & $\mathcal{O} \left( \frac{M^2 R^2}{\varepsilon^2} \right)$ & $\mathcal{O} \left( \sqrt{\frac{L R^2}{\varepsilon}} \right)$ \\
        \hline
        \makecell{$f(x)$ -- $\mu$-сильно \\ выпукла в $\| \cdot \|_2$ - норме} & $\mathcal{O} \left( \frac{M^2}{\mu \varepsilon} \right)$ & $\mathcal{O} \left( \sqrt{\frac{L}{\mu}} \left[\ln{\frac{\mu R^2}{\varepsilon}}\right] \right)$ \\
        \hline
    \end{tabular}
\end{table}
В таблице \ref{est_tbl} $R$ --- это $\|x_0 - x_*\|_2 $, а $x_0$ соответствует стартовой точке алгоритма. Как видим оценки в гладком случае существенно лучше, чем в негладком.


В случаях, когда функционалы не обладают достаточными свойствами гладкости, что проявляется во многих задачах, актуальны вопросы разработки соответствующих эффективных методов. Для негладких методов верхние оценки не настолько оптимистичны. В данной работе объединяются различные исследования для негладких постановок задач и предпринимаются шаги для расширения доступного класса их применимости, и улучшения оценок скорости сходимости при дополнительных условиях. 

Для улучшения оценок для негладких задач существует несколько подходов, например, выделение специальных подклассов, таких как условие острого минимума, предложенное в конце 1960-х годов Б.Т. Поляком. Стоит подчеркнуть, что подобные условия зачастую выставляют более жесткие требования к задаче, например, острый минимум подразумевает доступность минимального значения функции. 

Популярность набирают различные обобщения условий в оптимальных методах, обладающих приемлемой скоростью сходимости. Возникает естественное желание воспользоваться рядом значимых результатов, таких как оптимальная оценка скорости сходимости на классе липшицевых и сильно выпуклых минимизационных задач, которая достигается именно для субградиентного метода \cite{Bach_2012}. Субградиент является естественным обобщением на негладкий случай понятия градиента. Вектор $g$ называют субградиентом в точке $x_0$, если
$$
    f(x) \geq f(x_0) + \langle g, x - x_0 \rangle \;\;\; \forall x.
$$
По сути на функции появляются <<изломы>>.

Важным аспектом данной оптимальной оценки субградиентного метода является ее сублинейность, что является недостаточным для ряда приложений. В связи с этим для ускорения субградиентных методов используют такие условия, как условие острого минимума. Подобные условия позволяют сделать несколько начальных эффективных шагов, что было продемонстрировано в работе \cite{sharp22} и будет затронуто во второй главе данной работы. Также в упомянутой главе было проведено сравнение оценок принадлежащих различным популярным классам задач. Оценки, получаемые при помощи условия острого минимума, обладают лучшими свойствами сходимости, однако в отсутствии знаний о точном решении нет возможности повысить точность выше предварительно заданной точности, известной для приближенного решения. Оценки скорости сходимости, использующие сильную выпуклость, не обладают столь впечатляющими оценками скорости сходимости, однако позволяют добиться гораздо большей точности. 

Отметим важное и развиваемое в данной работе обобщение на случай задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Данная работа во многом опирается на определения связанные с понятием функции Брэгмана. Предполагается, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая некоторое расстояние, и соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
\[
    V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
\]

При помощи функции Брэгмана вводятся такие необходимые понятия, как \textit{относительная липшицевость, относительная сильная выпуклость, относительная сильная монотонность, относительная гладкость и условие относительного $\gamma$-роста}. Например, свойство относительной $L$-гладкости развивает yсловие $L$-гладкости $f$, где квадрат евклидовой нормы в определении заменяется дивергенцией Брэгмана
$$
    f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + L V(y,x) \quad   \forall x, y \in Q,
$$
где $Q$ --- область определения $f$.

\fixme{[FIXME] что дает <<относительное>> обобщение?} Наиболее заметными современными приложениями, обладающие свойствами относительной гладкости и относительной сильной выпуклости, являются задачи распределенной оптимизации \cite{Hendr}. 

В данной работе исследуется оценка скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее, рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач с соответствующими условиями относительной липшицевости функционалов. Вариационные неравенства являются важной вехой для работы с, например, лагранжевыми седловыми задачами. Постановка задачи в терминах вариационных неравенств представляет собой: 
$$
    \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
$$
где $x_*$ --- называют слабым решением вариационного неравенства, $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$, $g: Q \longrightarrow \mathbb{R}^n$. Предполагается, что решение $x_*$ существует. Уже из данной постановки задачи видно, что возможные варианты классов для оператора $g(x)$ существенно больше.

Класс вариационных неравенств в свою очередь имеет достаточно широкую область применения в различных областях математики, таких как теория игр, моделирование потоков и математическая экономика. 

В третьей главе вводится понятие относительного $\gamma$-роста, которое позволяет улучшить сублинейные оценки скорости сходимости субградиентного зеркального спуска для сильно выпуклых функций при помощи механизма рестартов. Предлагается удобный с практической точки зрения алгоритм, использующий критерий остановки, получаемый при помощи оценки суммы локальных значений нормы градиента. 

Таким образом, данная работа обогащает ряд важных практических задач набором методов с оптимальными на их классе оценками.

% {\progress}
% Этот раздел должен быть отдельным структурным элементом по
% ГОСТ, но он, как правило, включается в описание актуальности
% темы. Нужен он отдельным структурынм элемементом или нет ---
% смотрите другие диссертации вашего совета, скорее всего не нужен.

{\aim} \fixme{[FIXME] исправить, учесть острый мин и показать пользу относительный обобщений} данной работы является исследование и совершенствование методов оптимизации для задач, не обладающих достаточной гладкостью для применения классических методов. Подобные задачи возникают в различных областях, таких как белковый фолдинг и машинное обучение. Существенно, что исследуемые задачи имеют большую размерность. 


В данной работе мы для метода зеркального спуска расширяем доступный класс задач и рассматриваем аналоги условия Липшица, которые позволяют сохранить свойственные липшицевым задачам оптимальные оценки скорости сходимости. Также производится переход к постановке задачи в терминах вариационных неравенств и аналогичных относительных условий, что позволяет дополнительно расширить доступный класс задач. Получены ускоренные оценки скорости сходимости при помощи механизма рестартов зеркального спуска в случае наличия дополнительных условий, а именно условии острого минимума, которое обобщается на более широкий класс. Сделан существенный акцент на экспериментальной составляющей.

{\novelty}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Впервые получены оценки скорости сходимости метода зеркального спуска на классе вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. \fixme{[FIXME]такая формулировка лучше?}
  \item Впервые получены оценки скорости сходимости рестартованного метода зеркального спуска для относительно липшицевых задач оптимизации с относительным $\gamma$-ростом при $\gamma > 1$. \fixme{[FIXME]такая формулировка лучше?}
  \item Впервые получен адаптивный аналог для оптимальных оценок скорости сходимости зеркального спуска для задач минимизации сильно выпуклых функций с использованием локальных аналогов константы Липшица. \fixme{[FIXME]такая формулировка лучше?}
\end{enumerate}

{\defpositions}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Предложен вариант метода зеркального спуска для вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. Доказана инвариантная по размерности пространства оценка скорости сходимости этого метода, оптимальная на указанном классе задач с точностью до умножения на постоянный множитель.
  \item Получена адаптивная оценка скорости сходимости зеркального спуска для задач минимизации сильно выпуклых функций с использованием локальных аналогов константы Липшица. При этом сохраняется оптимальность этой оценки на классе сильно выпуклых липшицевых задач с точностью до умножения на константу. В частности, это позволяет работать и с задачами, не удовлетворяющими условию относительной липшицевости.
  \item Введён аналог ослабленного минимума ($\gamma$-роста) с использованием дивергенции Брэгмана. Получена оценка скорости сходимости рестартованного метода зеркального спуска для относительно липшицевых задач оптимизации с относительным $\gamma$-ростом. В ситуации сильно выпуклой прокс-функции предложены адаптивные правила остановки для рестартов исследуемого метода зеркального спуска и получен результат о его скорости сходимости.
\end{enumerate}

{\probation}
Основные результаты работы докладывались~на:
\begin{itemize}
    \item проектной смене <<Современные методы теории информации, оптимизации и управления>> в центре <<Сириус>>, 2021,
    \item QIPA (Quasilinear Equations, Inverse Problems and Their Applications), 2021,
    \item 64-й всероссийской научной конференции МФТИ, 2021,
    \item QIPA (Quasilinear Equations, Inverse Problems and Their Applications), 2018,
    \item 61-й всероссийской научной конференции МФТИ, 2018.
\end{itemize}

{\contribution} Ключевые результаты получены и доказаны автором лично. Также разработана библиотека для анализа и проверки методов оптимизации, обеспечивающая необходимую гибкость настройки. 

\ifnumequal{\value{bibliosel}}{0}
{%%% Встроенная реализация с загрузкой файла через движок bibtex8. (При желании, внутри можно использовать обычные ссылки, наподобие `\cite{vakbib1,vakbib2}`).
    {\publications} Основные результаты по теме диссертации изложены
    в~XX~печатных изданиях,
    X из которых изданы в журналах, рекомендованных ВАК,
    X "--- в тезисах докладов.
}%
{%%% Реализация пакетом biblatex через движок biber
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=1.
        % Процитированные здесь работы:
        %  * подсчитываются, для автоматического составления фразы "Основные результаты ..."
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthor` или `\insertbiblioauthorgrouped`
        %  * нумеруются там в зависимости от порядка команд `\printbibliography` в этом разделе.
        %  * при использовании `\insertbiblioauthorgrouped`, порядок команд `\printbibliography` в нём должен быть тем же (см. biblio/biblatex.tex)
        %
        % Невидимый библиографический список для подсчёта количества публикаций:
        \printbibliography[heading=nobibheading, section=1, env=countauthorvak,          keyword=biblioauthorvak]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorwos,          keyword=biblioauthorwos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopus,       keyword=biblioauthorscopus]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorconf,         keyword=biblioauthorconf]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorother,        keyword=biblioauthorother]%
        \printbibliography[heading=nobibheading, section=1, env=countregistered,         keyword=biblioregistered]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorpatent,       keyword=biblioauthorpatent]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorprogram,      keyword=biblioauthorprogram]%
        \printbibliography[heading=nobibheading, section=1, env=countauthor,             keyword=biblioauthor]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorvakscopuswos, filter=vakscopuswos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopuswos,    filter=scopuswos]%
        %
        \nocite{*}%
        %
        {\publications} Основные результаты по теме диссертации изложены в~\arabic{citeauthor}~печатных изданиях,
        \arabic{citeauthorvak} из которых изданы в журналах, рекомендованных ВАК\sloppy%
        \ifnum \value{citeauthorscopuswos}>0%
            , \arabic{citeauthorscopuswos} "--- в~периодических научных журналах, индексируемых Web of~Science или Scopus\sloppy%
        \fi%
        \ifnum \value{citeauthorconf}>0%
            , \arabic{citeauthorconf} "--- в~тезисах докладов.
        \else%
            .
        \fi%
        \ifnum \value{citeregistered}=1%
            \ifnum \value{citeauthorpatent}=1%
                Зарегистрирован \arabic{citeauthorpatent} патент.
            \fi%
            \ifnum \value{citeauthorprogram}=1%
                Зарегистрирована \arabic{citeauthorprogram} программа для ЭВМ.
            \fi%
        \fi%
        \ifnum \value{citeregistered}>1%
            Зарегистрированы\ %
            \ifnum \value{citeauthorpatent}>0%
            \formbytotal{citeauthorpatent}{патент}{}{а}{}\sloppy%
            \ifnum \value{citeauthorprogram}=0 . \else \ и~\fi%
            \fi%
            \ifnum \value{citeauthorprogram}>0%
            \formbytotal{citeauthorprogram}{программ}{а}{ы}{} для ЭВМ.
            \fi%
        \fi%
        % К публикациям, в которых излагаются основные научные результаты диссертации на соискание учёной
        % степени, в рецензируемых изданиях приравниваются патенты на изобретения, патенты (свидетельства) на
        % полезную модель, патенты на промышленный образец, патенты на селекционные достижения, свидетельства
        % на программу для электронных вычислительных машин, базу данных, топологию интегральных микросхем,
        % зарегистрированные в установленном порядке.(в ред. Постановления Правительства РФ от 21.04.2016 N 335)
    \end{refsection}%
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=2.
        % Процитированные здесь работы:
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthorimportant`.
        %  * ни на что не влияют в противном случае

        % \nocite{vakbib2}%vak
        % \nocite{patbib1}%patent
        % \nocite{progbib1}%program
        % \nocite{bib1}%other
        % \nocite{confbib1}%conf
    \end{refsection}%
        %
        % Всё, что вне этих двух refsection, это refsection=0,
        %  * для диссертации - это нормальные ссылки, попадающие в обычную библиографию
        %  * для автореферата:
        %     * при usefootcite==0, ссылка корректно сработает только для источника из `external.bib`. Для своих работ --- напечатает "[0]" (и даже Warning не вылезет).
        %     * при usefootcite==1, ссылка сработает нормально. В авторской библиографии будут только процитированные в refsection=0 работы.
}

\ifsynopsis
\else
  \insertbiblioauthor      % Вывод всех работ автора
\fi
