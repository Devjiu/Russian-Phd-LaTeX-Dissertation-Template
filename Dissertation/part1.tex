\chapter{Современные численные методы оптимизации: обзор литератyры.}\label{ch:ch1}

\section{Введение}\label{sec:ch1/sec1}

  Многие прикладные задачи обладают сложной внутренней структурой, которая не позволяет эффективно применять к ним теоретические методы без значительной их модификации. С ростом популярности нейронных сетей, повышением сложности экономических моделей, развитием компиляторных оптимизаций все чаще происходит столкновение теоретической постановки с задачей распределения ресурсов, представленной в различных формах. Часто на практике присутствует существенный набор специальных ограничений, дополнительно усложняют работу плохо определенные минимизируемые функционалы (различные loss-функции, к примеру). Многие из важных прикладных задач теряют свойство выпуклости, и тем более гладкость. Частично об этой проблеме говорит рост популярности адаптивных методов, которые подстраиваются к локальным значениям гладкости (см. например \cite{o2015adaptive, AdaMirr_2021, UMP}). В случае такой общей постановки задачи список методов, которые можно эффективно применять, сильно ограничен. В подобных ситуациях не всегда есть гарантии сходимости, не говоря уже о гарантиях хороших верхних оценок. Значительную роль в подборе метода играет проблема роста размерности, например, при использовании <<покомпонентного спуска>> каждая итерация пропорциональна размерности пространства, что вносит более значимый вклад в скорость сходимости по сравнению с градиентными аналогами. В связи с чем возникает естественное желание расширить допустимый класс задач для которых возможно эффективно применять хорошо исследованные методы первого порядка. 

  Также получила широкое распространение методика аппроксимации задачи к некоторому гладкому аналогу целевой выпуклой функции. Эту идею используют известные универсальные методы в таких работах как \cite{gasnikov2017universal, guminov2019universal}. Универсальные градиентные методы основаны на квадратичной интерполяции целевой функции для выпуклой оптимизации с гельдеровым градиентом, они используют искусственную погрешность для подстройки константы Гельдера. Универсальность в данном случае подразумевает подстройку метода под гладкость функции. К подобным подходам, когда пытаются выделить некоторые локальные свойства функции и подстроится к ее локальным параметрам, можно отнести квазиньютоновские методы, однако они оперируют параметрами более высокого порядка. 

  Анализ известных теорем об оптимальных оценках сформулированных для различных методов первого порядка, показывает, что ряд свойств, которые требуются для достижения соответствующей оценки, допускают существенные обобщения, свойственные для достаточно интересных классов задач. Это и привело в частности к появлению ряда работ \cite{Dragomir, Lu_2018, Lu_Nesterov_2018, Fedor_relative_adapuniv}, которые переходят к <<относительной липшицевости>> и постановке задачи в терминах вариационных неравенств. Подобный подход будет развит в последующих главах, здесь мы не будем подробно останавливаться на данном направлении.

  Отметим особенности представления оценки при помощи количества обращений к оракулу (подпрограмме), что удобно для сравнения методов, однако если анализировать вопрос с точки зрения имплементации, то возникают несоответствия между фактическим временем работы и оценкой. Алгоритмическая сложность не учитывает ряда особенностей самого метода, как и возможностей вычислительного устройства, на котором будут происходить измерения. Например, в данной оценке никак не учитывается зависимость по данным конкретного алгоритма, то есть ситуация, когда следующая по порядку исполнения команда, зависит от вычисления предыдущей команды. А это напрямую определяет возможности его параллелизации. Подобный тип оценки был более удобен ранее, когда исполняющее устройство позволяло только последовательное исполнение, а основным параметром, влияющим на финальное время работы метода, была тактовая частота, определяющая количество обрабатываемых команд в единицу времени. Современные исполняющие устройства в значительной мере используют возможности параллелизации, потому время работы алгоритма определяется в том числе и профилем его доступа к данным. Причем тип и параметры параллелизации будут зависеть как от алгоритма, так и от организации памяти и иных функциональных ограничений устройства. Данная проблематика более подробно затрагиваются в пункте \ref{subsec:sec1/su3} за счет особенностей целевой функции, которые не позволяют гарантировать хорошие оценок скорости сходимости.  Метод покоординатного спуска обладает крайне пессимистичными оценками скорости сходимости, однако он позволяет параллелизовать шаги, производимые в различных измерениях. Попытка воспользоваться данным несоответствием и легла в основу данной главы. Также приводится опыт работы с методами первого порядка на данном функционале в отсутствии гарантий оценок. 

  Схожие проблемы поднимаются в работах, где целью является адаптация методов оптимизации для распределенной систем. Данное направление работы было выделено в отдельный класс задач, называемый распределенной оптимизацией. Во многом это отделение обусловлено существенными техническими ограничениями, возникающими при попытке применения классических методов оптимизации первого порядка. В данном случае важным параметром выступает значительное время необходимое для передачи информации о решаемой задаче другим вычисляющим устройствам, объединенным в определенную топологию. В частности, большой популярностью в данных задачах пользуются стохастические варианты методов, что легко объясняется их структурой. Классической постановкой задачи для стохастического градиентного спуска является сепарабельная:
  $$
  \min_{x \in \mathbb{R}^p} f(x) = \min_{x \in \mathbb{R}^p}\frac{1}{n} \sum_{i=1}^n f_i(x).
  $$
  Произвольный выбор одного или набора элементов из градиентов для следующего шага позволяет выполнять шаги на различных устройствах, контролируя при помощи потери необходимость обмена данными между устройствами. Более подробно рассмотрим постановку задачи в терминах распределенной оптимизации в пункте \ref{raspr}. 
  
  Рассуждая об имплементации методов оптимизации, стоит также принимать в расчет ограничения, возникающие из-за машинного представления чисел (см. например \cite{low_prec_grad, low_prec_train}). Во многих случаях недостаточная точность может приводить к так называемым градиентным взрывам или исчезновению значения, что соответствует слишком большому для представления числу и слишком малому для отделения его от 0. Эти эффекты широко обсуждаются при реализации библиотек, осуществляющих автоматическое дифференцирование. Данные аспекты находят отражение в концепции неточного оракула, что в конечном итоге отражается на теоретических оценках. Также существуют подходы, направленные на обнаружение ошибки и ее исправление при следующих итерациях, аналогичные ранее продемонстрированному подходу. Тут важно отметить, что машинная точность необходимая для работы метода является существенным ограничением для его использования во многих приложениях. Чем выше требуемая точность, тем больше необходимо памяти для хранения результатов работы, тем более сложное представление чисел необходимо поддерживать. Практическим примером подобной проблематики является процесс переноса обучения нейронной сети с серверных мощностей на устройство со значительным ограничением памяти, такое как мобильный телефон. \cite{progeressive_low_precision, high_acc_low}


  Для описания интересующих нас методов рассмотрим общую задачу безусловной выпуклой оптимизации \cite{gasnikov2017universal}, которая является <<классической>>

  \begin{equation}
  \label{ProbForm}
  \min_{x\in \mathbb{R}^n} {f\left( x \right)}.
  \end{equation}

  Через $x^0$ обозначим точку старта, через $x_\ast$ -- искомое точное решение задачи (\ref{ProbForm}). Если решение задачи (\ref{ProbForm}) не единственно, то под $x_*$ будем понимать такое решение задачи (\ref{ProbForm}), которое наиболее близко к $x_0$ в смысле Евклидового расстояния. Определим $R = \left\| {x^0-x_\ast } \right\|_2 $. Будем также считать, что $\lambda_{\max} \left( \nabla^2 f\left( x \right) \right) \le L$, $\lambda_{\min} \left( \nabla^2 f\left( x \right) \right) \ge \mu \ge 0$. Введем также число обусловленности $\chi = L/\mu$. Если $n$ достаточно большое, то самым простым численным методом решения задачи (\ref{ProbForm}) будет \textit{градиентный спуск} \cite{nesterov2013introductory}, \cite{Polyak1983}:

  \begin{equation} \label{Grad}
  x^{k+1} = x^k - \frac{1}{L}\nabla f\left( x^k \right).
  \end{equation}
  Известна следующая теоретическая оценка для метода (\ref{Grad})

  \begin{equation}
  \label{GradConverg}
  f\left( {x^N} \right)-f\left( {x_\ast } \right)\le \frac{LR^2}{2} \min \left\{ 
  \frac{1}{N}, \exp\left({-\frac{N}{\chi}} \right) \right\}.
  \end{equation}

  На практике значение параметра $L$, которое нужно методу (\ref{Grad}) для работы, как правило, не известно. К тому же, на самом деле, методу (\ref{Grad}) и не и требуется знание глобального значения этого параметра. Достаточно знать значение этого параметра на траектории метода. Возможным решением проблемы незнания $L$ является переход к \textit{методу наискорейшего спуска} \cite{Polyak1983}:

  \begin{equation} \label{GradLine}
  \begin{aligned} 
    h_k \in \argmin_{h\ge 0} f\left( x^k - h\nabla f\left( x^k \right) \right),\\
    x^{k+1} = x^k - h_k\nabla f\left( x^k \right).
  \end{aligned}
  \end{equation}
  Переход к наискорейшему спуску принципиально не меняет оценку скорости сходимости (\ref{GradConverg}), но может обладать иным поведением на практике.

  В связи с написанным выше возникает вопрос, является ли градиентный спуск (наискорейший спуск) оптимальным методом (с точностью до числовых множителей) для выпуклых задач в классе методов вида

  \begin{equation}
  \label{GeneralForm}
  x^{k+1}\in  x^0+\;\mbox{Lin}\left\{ 
  {\nabla f\left( {x^0} \right),...,\nabla f\left( {x^k} \right)} \right\} ?
  \iffalse
  x^{k+1}\in \argmin_{x\in x^0+\;\mbox{Lin}\left\{ 
  {\nabla f\left( {x^0} \right),...,\nabla f\left( {x^k} \right)} \right\}} 
  f\left( x \right)?
  \fi
  \end{equation}

  Ответ на этот вопрос отрицательный (см. например \cite{Nemirovski1979}). У градиентных (наискорейших) спусков существуют различные ускоренные варианты. Поясним это на примере задачи квадратичной оптимизации.

  Одной из характерных задач 
  выпуклой оптимизации является задача минимизации положительно определенной 
  квадратичной формы (эквивалент решения линейной системы уравнений, см. например \cite{bubeck_2015}): 

  \[
    \min_{x\in \mathbb{R}^n} {f\left( x \right)} = \min_{x\in \mathbb{R}^n} {\frac{1}{2}\left\langle {Ax,x} \right\rangle -\left\langle {b,x} \right\rangle }.
  \]

  Изучив данный класс задач, можно пытаться понять, как сходятся различные методы хотя бы локально (в окрестности минимума) в задачах выпуклой оптимизации. Кроме того, такие задачи возникают как вспомогательные подзадачи при использовании методов второго порядка, например, метода Ньютона. Известно, что для задачи выпуклой квадратичной оптимизации в оценке (\ref{GeneralForm}) достаточно использовать историю только с предыдущей итерации. Опишем оптимальный метод в этом классе, который также  называют \textit{метод сопряженных градиентов }(первая итерация делается согласно (\ref{GradLine}))

  \begin{equation}
  \label{ConGrad}
  x^{k+1} = x^k-\alpha _k \nabla f\left( {x^k} \right)+\beta _k \cdot 
  \left( {x^k-x^{k-1}} \right),
  \end{equation}
  где

  \[
  \left( {\alpha _k ,\beta _k } \right) = \argmin_{\alpha ,\beta } f\left( {x^k-\alpha \nabla f\left( {x^k} 
  \right)+\beta \cdot \left( {x^k-x^{k-1}} \right)} \right).
  \]

  Метод (\ref{ConGrad}) обладает следующей оптимальным оценкой

  \begin{equation} \label{CGConv}
    \begin{aligned} 
    f\left( {x^N} \right) & - f\left( {x_\ast } \right) \le \\ 
    \le & \min \left\{ {\frac{LR^2}{2\left( {2N+1} \right)^2}, \; 2LR^2\left( {\frac{\sqrt \chi -1}{\sqrt \chi +1}} \right)^{2N}, \; \left( {\frac{\lambda _{n-N+1} -\lambda _1 }{\lambda _{n-N+1} +\lambda _1 }} \right)^2R^2} \right\},\\
    \end{aligned}
  \end{equation}
  где $N\le n$, $0\le \mu = \lambda_{\min}\left(A\right) = \lambda_{1} \le \lambda_{2} \le ... \le \lambda_{n} = \lambda_{\max}\left(A\right) = L$. Второй аргумент в минимуме (\ref{CGConv}) при больших значениях $N$ оценивается сверху $2LR^2\exp\left(-2N / \sqrt{\chi}\right)$. При $N=n$ метод гарантированно находит точное решение, что следует из последней оценки в минимуме (\ref{CGConv}). Сформулированный результат является 
  фундаментальным фактом выпуклой оптимизации и вычислительной линейной алгебры одновременно, и базируется на наличии рекуррентных формул для \textit{многочленов Чебышёва} \cite{nocedal2006sequential}, \cite{Nemirovski1979}. 

  На практике более распространены варианты методов сопряженных градиентов с одномерной вспомогательной оптимизацией, а не с двумерной постановкой. Наиболее популярными вариантами метода сопряженных градиентов являются следующие два метода \cite{nocedal2006sequential}:

  \[
    h_k \in \argmin_{h\in \mathbb{R}} f\left( {x^k+hp^k} \right),
  \]
  \[
    x^{k+1}=x^k+h_k p^k,
  \]
  \[
    p^{k+1}=-\nabla f\left( {x^{k+1}} \right) + \beta_k p^k,
  \]
  \[
    p^0=-\nabla f\left( {x^0} \right),
  \]
  \[
    \beta _k =-\frac{\left\| {\nabla f\left( {x^{k+1}} \right)} \right\|_2^2 }{\left\| {\nabla f\left( {x^k} \right)} \right\|_2^2 },\quad \mbox{(формула Флетчера--Ривса)}
  \]
  \[
    \beta _k =-\frac{\left\langle {\nabla f\left( {x^{k+1}} \right),\nabla f\left( {x^{k+1}} \right)-\nabla f\left( {x^k} \right)} \right\rangle }{\left\| {\nabla f\left( {x^k} \right)} \right\|_2^2 }. \quad \mbox{(формула Полака--Райбьера--Поляка)}
  \]

  Для задач квадратичной оптимизации оба метода эквивалентны между собой, как и методу (\ref{ConGrad}). В случае общей постановки задачи выпуклой оптимизации, насколько нам известно, для этих методов пока не были получены оптимальные порядки скорости сходимости (установлен только факт глобальной сходимости для задач гладкой выпуклой оптимизации). Тем не менее, именно эти два варианта метода сопряженных градиентов наиболее часто используются при решении практических задач \cite{nocedal2006sequential} (в том числе не обязательно выпуклых). При этом с некоторой периодичностью (обычно период выбирают пропорционально размерности пространства, в котором происходит оптимизация) требуется перезапускать метод, обнуляя историю: вместо $p^{k+1}=-\nabla f\left( {x^{k+1}} \right)+\beta _k p^k$ в момент рестарта полагают $p^{k+1}=-\nabla f\left( {x^{k+1}} \right)$. 


  Для общих (не квадратичных) задач безусловной гладкой выпуклой минимизации достаточно популярным сейчас является следующий (двухшаговый) вариант быстрого (ускоренного) градиентного метода (Нестерова) 

  \[
    x^{k+1}=y^k-\frac{1}{L}\nabla f\left( {y^k} \right),
  \]
  \[
    y^k=x^k+\frac{k-1}{k+2}\left( {x^k-x^{k-1}} \right),
  \]
  который также можно понимать как \textit{моментный метод} \cite{su2014differential}:

  \begin{equation}
  \label{moment}
    x^{k+1}=x^k-\frac{1}{L}\nabla f\left( {x^k+\frac{k-1}{k+2}\left( {x^k-x^{k-1}} \right)} \right)+\frac{k-1}{k+2}\left( {x^k-x^{k-1}} \right).
  \end{equation}

  Приведем также вариант моментного метода для $\mu $-сильно выпуклой функции в евклидовой норме \cite{nesterov2013introductory}:

  \begin{equation}
  \label{momentS}
    x^{k+1}=x^k-\frac{1}{L}\nabla f\left( {x^k+\frac{\sqrt L -\sqrt \mu }{\sqrt L +\sqrt \mu }\left( {x^k-x^{k-1}} \right)} \right)+\frac{\sqrt L -\sqrt \mu }{\sqrt L +\sqrt \mu }\left( {x^k-x^{k-1}} \right).
  \end{equation}

  Оба метода (\ref{moment}), (\ref{momentS}) с точностью до числовых множителей сходятся согласно первому и второму аргументу в минимуме оценки (\ref{CGConv}). Более того, можно даже объединить оба метода (\ref{moment}), (\ref{momentS}) в один, сходящийся по оценке, наилучшей из двух отмеченных оценок \cite{nesterov2013introductory}. Однако все равно требуется априорно знать параметр $\mu$. В общем случае нам не известны ускоренные градиентые методы для сильно выпуклых задач, которые обходились бы без данного параметра. Отметим при этом, что незнание параметра $L$ может быть устранено вспомогательной маломерной минимизацией.

  \iffalse
    Из сопоставления (\ref{Grad}) и (\ref{moment}), (\ref{momentS}) легко заметить, что сложность (трудоемкость) итераций у градиентного спуска и у быстрого градиентного метода практически одинаковы, в то время как скорости сходимости отличаются очень существенно. Именно это обстоятельство и обусловило огромную популярность быстрых градиентных методов. 

    Среди последних достижений в развитии ускоренных (быстрых) градиентных методов отметим, \textit{оптимизированный} вариант быстрого градиентного метода для задач безусловной оптимизации \cite{drori2018efficient}

    \begin{equation}
    \label{optim}
    \begin{array}{c}
     y^{k+1}=\left( {1-\frac{1}{t_{k+1} }} \right)x^k+\frac{1}{t_{k+1} }x^0, \\ 
     d^{k+1}=\left( {1-\frac{1}{t_{k+1} }} \right)\nabla f\left( {x^k} 
    \right)+\frac{2}{t_{k+1} }\sum_{j=0}^k {t_j \nabla f\left( {x^j} 
    \right)} , \\ 
     x^{k+1}=y^{k+1}-\frac{1}{L}d^{k+1}, \\ 
     \end{array}
    \end{equation}
    где

    \[
    t_{k+1} =\frac{1+\sqrt {4t_k^2 +1} }{2},
    \quad
    k=0,...,N-2,
    \]
    \[
    t_N =\theta _N ,
    \quad
    \theta _{k+1} =\frac{1+\sqrt {8\theta _k^2 +1} }{2},
    \quad
    k=0,...,N-1.
    \]

    На классе гладких выпуклых задач метод (\ref{optim}) сходится согласно оценке 

    \begin{equation}
    \label{optimE}
    f\left( {x^N} \right)-f\left( {x_\ast } \right)\le \frac{2LR^2}{\theta _N^2 
    }\left( {\le \frac{LR^2}{N^2}} \right),
    \end{equation}
    которая достигается, например, на функции

    \begin{equation}
    \label{Example}
    f\left( x \right)=\left\{ {\begin{array}{l}
     \frac{LR}{\theta _N^2 }\left\| x \right\|_2 -\frac{LR^2}{\theta _N^4 
    },\;\left\| x \right\|_2 \ge \frac{R}{\theta _N^2 }, \\ 
     \frac{L}{2}\left\| x \right\|_2^2 ,\;\left\| x \right\|_2 <\frac{R}{\theta 
    _N^2 }. \\ 
     \end{array}} \right.
    \end{equation}
    Оценка (\ref{optimE}) является точной оценкой скорости сходимости оптимальных методов вида (\ref{GeneralForm}) на классе гладких выпуклых задач. Другими словами, не существует такого метода вида (\ref{GeneralForm}) с фиксированными шагами (т.е. без вспомогательной маломерной оптимизации), который бы на всем классе задач гладкой выпуклой оптимизации сходился по оценке, лучшей чем (\ref{optimE}). Подчеркнем, что нельзя улучшить даже числовой множитель. Из метода (\ref{optim}) можно сделать метод, не требующий знания параметра $L$. Для этого в процедуре (\ref{optim}) следует заменить

    \[
    x^{k+1}=y^{k+1}-\frac{1}{L}d^{k+1}
    \]
    на (см. также (\ref{GradLine}))

    \[
    x^{k+1}=y^{k+1}-h_{k+1} d^{k+1},
    \]
    где

    \[
    h_{k+1} \in \argmin_{h\in \mathbb{R}} f\left( 
    {y^{k+1}-hd^{k+1}} \right).
    \]
    Такой метод также будет сходиться согласно оценке (\ref{optimE}). 

    \iffalse
      Для класса гладких сильно выпуклых задач среди методов вида (\ref{GeneralForm}) пока не был найден оптимальный метод (с неулучшаемым числовым множителем в оценке скорости сходимости). Подобно (\ref{optim}), предложенный метод также оказался методом с конечной памятью и без вспомогательной маломерной оптимизации. Однако этот метод требует знания \textit{числа обусловленности} задачи $\chi =L \mathord{\left/ {\vphantom {L \mu }} \right. \kern-\nulldelimiterspace} \mu $, и не известны его адаптивные варианты по этому параметру.
    \fi

    В общем случае на каждой итерации методов типа сопряженных градиентов (в частности, (\ref{optim})) вспомогательную задачу можно решить только приближенно. В \cite{Nemirovski1979} было установлено, что достаточно решать вспомогательную задачу с относительной точностью (по функции) $\delta =\mathcal{O}\left( {\varepsilon \mathord{\left/ {\vphantom {\varepsilon {N\left( \varepsilon \right)}}} \right. \kern-\nulldelimiterspace} {N\left( \varepsilon \right)}} \right)$, где $\varepsilon $ -- желаемая относительная точность (по функции) решения исходной задачи, а $N\left( \varepsilon \right)$ -- число итераций, которые делает (внешний) метод. Следовательно, вспомогательная одномерная задача может быть решена за \cite{nocedal2006sequential}

    \[
    \mathcal{O}\left( {\ln 
    \left( {\delta ^{-1}} \right)} \right) =
    \mathcal{O}\left( {\ln \left( {{N\left( \varepsilon \right)} \mathord{\left/ 
    {\vphantom {{N\left( \varepsilon \right)} \varepsilon }} \right. 
    \kern-\nulldelimiterspace} \varepsilon } \right)} \right)=\mathcal{O}\left( {\ln 
    \left( {\varepsilon ^{-1}} \right)} \right)
    \]
    обращений к оракулу (подпрограмме) за значением оптимизируемой функции.  С учетом того, что по теоретическим оценкам использование таких методов не позволяет в общем случае улучшать даже числовые множители (см. текст выше относительно оптимального метода (\ref{optim})), то, кажется, что стоит оставить методы со вспомогательной одномерной оптимизацией на каждой итерации в стороне, и ограничиться рассмотрением методов с фиксированными шагами и конечной памятью. Однако на практике типично, что различные варианты метода сопряженных градиентов (а их насчитывается уже, как минимум, несколько десятков \cite{floudas2008encyclopedia}) работают существенно быстрее ускоренных градиентных методов с постоянными шагами \cite{nesterov2018primal}. Причина такого различия связана с наличием последнего аргумента минимума в оценке (\ref{CGConv}) для методов типа сопряженных градиентов и следующим наблюдением: методы типа сопряженных градиентов сходятся также как ускоренные методы только на вырожденных (мало интересных / слишком простых для практики) примерах, см. (\ref{Example}). 
  \fi

  Хорошо известна геометрическая интерпретация градиентного спуска (см., например, \cite{Polyak1983}), в основу которой положена замена исходной функции параболоидом вращения, касающимся её графика в текущей точке. Точка, доставляющая минимум параболоиду, принимается за новое положение метода. Идея метода наискорейшего спуска заключается в подборе кривизны параболоида с помощью решения вспомогательной задачи одномерной минимизации. В \textit{методе Ньютона} вместо параболоида вращения строится квадратичная аппроксимация оптимизируемой функции (на основе доступного гессиана функции, вычисленного в текущей точке), однако это приводит к необходимости решения на каждом шаге более сложной задачи -- минимизации квадратичной формы (обращения матрицы, или решения системы линейных уравнений). 

  Естественно, возникает идея построения какого-то <<промежуточного>> метода, с одной стороны не требующего вычисления (и, тем более, обращения) гессиана, а с другой стороны все-таки пытающегося как-то аппроксимировать гессиан, исходя из накопленной информации первого порядка (градиентов). В основу \textit{квазиньютоновских методов} положен следующий общий принцип построения квадратичной аппроксимации: квадратичная аппроксимация должна касаться графика оптимизируемой функции в текущей точке и иметь с ней одинаковые градиенты в точке с предыдущего шага (secant equation). Существует много различных способов удовлетворить этим условиям \cite{nocedal2006sequential}. У этих способов есть различные интерпретации, среди которых отметим понимание квазиньютоновских методов, как \textit{методов переменной метрики} (варианта метод сопряженных градиентов \cite{Polyak1983}). Наиболее интересными в практическом плане являются способы, которые требуют не более чем квадратичной (по размерности пространства) трудоемкости и памяти. Среди таких способов наиболее удачно себя зарекомендовал способ, приводящий в итоге к методу \textit{BFGS} \cite{nocedal2006sequential}:

  \[
  h_k = \argmin_{h\in \mathbb{R}} f\left( {x^k-hH_k \nabla 
  f\left( {x^k} \right)} \right),
  \]
  \[
  x^{k+1}=x^k-h_k H_k \nabla f\left( {x^k} \right),
  \]
  \[
  H_{k+1} =H_k +\frac{H_k \gamma _k \delta _k^T +\delta _k \gamma _k^T H_k 
  }{\left\langle {H_k \gamma _k ,\gamma _k } \right\rangle }-\beta _k 
  \frac{H_k \gamma _k \gamma _k^T H_k }{\left\langle {H_k \gamma _k ,\gamma _k 
  } \right\rangle },
  \]
  где

  \[
  \beta _k =1+\frac{\left\langle {\gamma _k ,\delta _k } \right\rangle 
  }{\left\langle {H_k \gamma _k ,\gamma _k } \right\rangle },
  \quad
  \gamma _k =\nabla f\left( {x^{k+1}} \right)-\nabla f\left( {x^k} \right),
  \quad
  \delta _k =x^{k+1}-x^k,
  \quad
  H_0 =I.
  \]

  В отличие от метода сопряженных градиентов (\ref{ConGrad}) в BFGS не обязательно точно осуществлять вспомогательную одномерную оптимизацию. В целом, BFGS оказался наиболее устойчивым (к вычислительным погрешностям) вариантом квазиньютоновских методов. Геометрия квазиньютоновских методов близка (но не идентична!) геометрии субградиентных методов с процедурой растяжения пространства (метод эллипсоидов, методы Шора \cite{Polyak1983}) и связана с типичным <<пилообразным>> поведением градиентных спусков в окрестности минимума для плохо обусловленных задач. Направления $\gamma _k $ и $\delta _k $ <<подсказывают>> направления растяжение/сжатие, и позволяют адаптивно улучшать обусловленность задачи, правильно аккумулируя собранную информацию в $H_k $. 

  С теоретической точки зрения квазиньютоновские методы на данный момент исследованы не полностью \cite{nesterov2013introductory}, \cite{nocedal2006sequential}: глобальная скорость сходимости для гладких задач выпуклой оптимизации в общем случае не выше, чем у обычных (неускоренных) градиентных методов (во всяком случае, только это пока удалось установить), а локальная скорость сходимости в случае невырожденного минимума -- \textit{сверхлинейная}, т.е. быстрее, чем линейная. Многие из упомянутых методов будут применяться при решении задачи минимизации энергии белка. Для них будут рассмотрены соответствующие оценки в пункте \ref{subsec:sec1/su3}. Также в пункте, описывающем численные эксперименты, \ref{pract} показан ряд проблем которые возникают при работе с квазиньютоновскими методами. Иначе говоря, квазиньютоносвкие методы могут быть эффективным инструментом, однако за счет большого числа параметров требуют значительных шагов для адаптации к нестандартной постановке задачи.
\section{Стохастический спуск для распределенных задач при наличии компрессии} \label{raspr}
  Во введении уже обсуждалась связь эффективности применения метода и используемого исполнительного устройства или топологии устройств. В связи с ростом популярности машинного обучения и значительного времени, которое требуется для обучения современных моделей (дни, недели), задачи параллелизации и распределения исполнения по различным устройствам для эффективного их использования выходят на первый план. Остановимся на задаче распределенной оптимизации немного подробнее. Часто постановку задачи распределенной, или федеративной, оптимизации формулируют так:
  \begin{equation} \label{raspr_task}
    \min_{x \in \mathbb{R}^n}\left\{f(x)=\frac{1}{d} \sum_{i=1}^d f_i(x)\right\},
  \end{equation}
  где $d$ соответствует количеству исполнителей или узлов или вычислителей. Предполагается, что информация, касающаяся функции $f_i$, хранится только на узле $i$. Также зачастую каждую из $f_i$ рассматривают как
  $$
    f_i(x) = \frac{1}{m} \sum_{j=1}^m f_{ij}(x).
  $$ 
  Значимым параметром, влияющим на скорость сходимости, является дороговизна пересылки информации между узлами. Данный аспект является так называемым бутылочным горлышком для подобного типа задач. Потому методы, нацеленные на распределенную постановку, как правило, стараются заменить глобальный шаг (обновление всех компонент вектора $x$) на локальные аналоги. Это основная идея, которая лежит в основе таких методов как Local-SGD \cite{Stich2019LocalSC}. Также популярным подходом является уменьшение передаваемой информации между узлами путем выбора наиболее значимых компонент или более сложных механизмов компрессии. Подобные подходы используются в работах \cite{qlsgd, qsgd, err_fdbk}. Отметим, что многие методы предполагают полносвязную топологию доступных узлов или же star-топологию, когда есть один вычисляющий центр и зависимые дополнительные мощности. 
  В работе \cite{GorbunovKMR20}, например, предлагается следующий метод
  $$
  \begin{aligned} 
    x^{k+1} &=x^k-\frac{1}{d} \sum_{i=1}^d v_i^k, \\ 
    e_i^{k+1} &=e_i^k + \gamma g_i^k - v_i^k, \quad i=1,2, \ldots, d . 
  \end{aligned}
  $$
  Здесь $x^k$ отвечает за значение аргумента на итерации $k$, $v_i^k$ --- это результат вычисленный на узле $i$ на итерации $k$, $g_i^k$ --- это оценка $\nabla f_i(x^k)$ полученная узлом $i$, $\gamma$ --- это фиксированный размер шага и $e_i^k$ --- это накопленная к итерации $k$ на узле $i$ ошибка.  Подобная постановка задачи рассматривалась также и в \cite{err_fdbk}, где был проведен схожий анализ, но со своими особенностями, учитывающий отложенное вычисление градиента, компрессию и размеры обновляемой части градиента.
  В упомянутой работе функция компрессии $C()$ представлена так:
  $$
    v_i^k = C(e_i^k + \gamma g_i^k).
  $$
  Данная постановка, как и было показано в \cite{GorbunovKMR20} моими соавторами, обобщает и развивает ряд методов, таких как Error Compensated Stochastic Gradiend Descent (EC-SGD), EC-SGD-DIANA и EC-LSVRG. Теоретические результаты для данного ряда методов также были получены моими соавторами. Однако многие практические результаты были получены автором. В упомянутой статье рассматривается задача линейной регрессии для различных наборов данных
  $$
    \min_{x \in \mathbb{R}^n}\left\{ f(x) = \frac{1}{N} \sum_{i=1}^N \log \left(1+\exp \left(- y_i \cdot(A x)_i\right)\right) + \frac{\mu}{2}\|x\|^2 \right\},
  $$
  где $N$ --- это количество параметров модели, $x$ --- это веса модели, $A$ --- матрица параметров, а $y \in {\{-1,1\}}^N$ --- набор ожидаемых решений. Подобный тип задач имеет широкое распространение в машинном обучении. Важным аспектом являются используемые функции компрессии, а именно 
  \begin{enumerate}
    \item функция $TopK()$ --- $k$ компонентов, отсортированных по убыванию абсолютных значений в векторе ошибок $e_i$,
    \item $RandK()$ --- $k$ случайных компонентов,
    \item $l2-quant()$ --- выделение некоторого подпространства в $\mathbb{R}^n$ на основе вероятности, полученной при помощи нормализации входного вектора. 
  \end{enumerate}

  Очевидно, что подобные преобразования понижают допустимую точность для методов и также появляется шум, связанный с компрессией. Поскольку здесь присутствует достаточно много параметров, которые варьируются для многих методов и различных функций компрессии, то мы приведем один из тестов в качестве иллюстрации влияния компрессии на различные методы --- см. рис. \ref{compr}. Здесь mushrooms --- это наименование использованного набора данных, по горизонтальной оси отложено количество итераций. Видно, что различные методы реагируют на внедрение функции компрессии по-разному. Некоторое методы более устойчивы к компрессионному шуму и позволяют эффективно повышать точность с ростом количества итераций, а именно EC-L-SVRG-DIANA.
  \begin{figure}
    \begin{center}
      \includegraphics[width=12cm]{mushrooms_20.png}
    \end{center}
    \caption{Компрессионный шум для различных методов.}
    \label{compr}
  \end{figure}

  К примеру, если в исходной постановке задачи распределенной оптимизации функции являются относительно $\mu$-сильно выпуклыми и относительно гладкими функциями, то при условии схожести слагаемых можно улучшить оценки скорости сходимости. \cite{distrib_relative} Условие схожести слагаемых предполагает, что при распределении функции по узлам, функция на центральном узле достаточно <<близка>> к полной функции. Формально это описывается $\norm{\nabla^2 \widetilde{f_0}(x) - \nabla^2f(x)}_2 \leq \gamma,$ где $\widetilde{f_0}(x)$ соответствует нормированной суммме на центральном узле. Для данной постановки крайне важна топология узлов, также такую конфигурацию называют star-топологией. 
  Для данного метода можно показать, что после $N$ итераций справедливо
  $$
    f(\hat{x})-f_* \leqslant V\left(x_*, x^0\right)\left(1-\frac{\mu}{\mu+2 \gamma}\right)^N.
  $$
  Данный результат более полно освещен в \cite{distrib_relative} и показывает важность расширения применимости методов на класс относительно липшицевых и относительно сильно выпуклых задач для распределенной оптимизации.


\section{Задача о локальной минимизации силового поля для трехмерного представления макромолекул} \label{sec:ch1/sec2}

\subsection{Проблематика задачи} \label{subsec:sec1/su1}
  Развитие экспериментальных методов измерений в молекулярных и клеточных системах в последние десятилетия привело к значительному росту понимания биологических процессов, протекающих при различных патологических явлениях.
  В свою очередь это позволяет выявлять новые мишени для проведения эффективной лекарственной терапии даже для самых тяжелых заболеваний из области онкологии и аутоиммунных нарушений.
  Терапевтическими агентами в этом случае должны служить искусственно созданные биологические молекулы, обеспечивающие заданные функции.
  Обеспечение возможности рационального дизайна таких молекул, а также предсказание их свойств без проведения реального биологического эксперимента, является одной из важнейших целей современной вычислительной биологии.

  К сожалению, как недостаточные вычислительные мощности, так и фундаментальные ограничения используемых методов математического моделирования биологических систем не позволяют решать эту задачу полностью в общем случае.
  Для этого активно развиваются методы предсказания структуры синтетических белков, имеющие, по большей части, нефизичные основания.
  Другой не менее перспективной областью является модификация природных белков с целью улучшения их характеристик или придания альтернативных свойств.
  Оба направления подразумевают оптимизацию геометрии исследуемых молекул с целью поиска конформации (взаимного положения всех атомов), наиболее точно соответствующей реальному состоянию биологического объекта в природе.
  То, что исследуемые объекты существуют при температурах значительно превышающих ноль по шкале Кельвина, определяет постоянное движение всех компонентов молекул, вызываемых внешними возмущениями среды и, как следствие, отсутствия равновесия в действующих силах.
  Тем не менее практически полезно рассматривать некоторую статистически наиболее часто встречающуюся конформацию, как ту, в которой биологическая молекула проводит большую часть времени, а значит, скорее всего именно в ней способна выполнять свою основную функцию.
  Эта конформация логичным образом будет достигаться в минимуме потенциальной энергии молекулы, определяющейся множеством попарных взаимодействий между атомами.
  Природа этих взаимодействий, по большей части, лежит в области электростатики и выражается соответствующими законами.

  Важной особенностью рассмотренного процесса является то, что белок не стремится к глобальному минимуму потенциальной энергии.
  Действительно, если бы простейший белок из $100$ аминокислот начал бы сворачиваться из линейной конформации в соответствии с поиском глобального минимума, то ему пришлось бы перебирать примерно $10^{100}$ возможных конформаций, что привело бы ко времени сворачивания порядка $10^{80}$ лет даже при скорости перебора в $10^{13}$ конформаций в секунду.
  Это привело бы к тому, что за время существования нашей вселенной не свернулся бы ни один белок, что, очевидно, расходится с нашим эмпирическим опытом.
  Подобные соображения были высказаны впервые в \cite{levinthal1969} и получили название "парадокса Левинталя".
  Разгадкой представленного парадокса является то, что белок образуется не сразу весь а последовательно от первой аминокислоты к последней, а значит и сворачивается не весь целиком одномоментно.
  Это же приводит нас к соображениям, что локально схожие последовательности аминокислот будут давать локально схожие структуры.
  На основе этой идеи разработаны различные методы, позволяющие получать неплохие приближения пространственных конформаций белков с использованием баз уже известных структур.
  Эти методы упомянуты для следующего соображения: в проблемах вычислительной биологии важной задачей становится локальная минимизация энергетического потенциала, не вызывающая значительного возмущения геометрии рассматриваемой биологической молекулы.

  В данной главе рассматривается задача минимизации потенциальной энергии пространственной структуры белка с точки зрения современных численных методов оптимизации в предположении, что имеется достаточно хорошие начальные приближения к нужному решению (локальному минимуму).

  \subsection{Постановка задачи} \label{subsec:sec1/su2}

  Минимизируемая геометрия белка будет представляться координатами его атомов.
  Для каждой из двадцати видов аминокислот, используемых при построении белков в живой природе, можно определить конкретное количество атомов, в нее входящих.
  В среднем это число будет составлять около двух десятков.
  Каждый атом, в свою очередь, имеет три пространственные координаты $\{x_k ,y_k ,z_k\}$. Таким образом конфигурация белка описывается номером атома и его положением в пространстве.
  Далее мы будем работать в этом координатном пространстве.
  Обозначим через $n_{ }\left(n\sim 10^4\right)$ размерность пространства возможных конфигураций белка.
  Отметим, что в литературе часто работают и в другом пространстве: в пространстве углов и расстояний между соседними атомами, см., например, \cite{canutescu2003cyclic}, \cite{coutsias2004kinematic}. 

  Центральное место в задаче минимизации потенциальной энергии молекулярного комплекса, разумеется, занимает приближение межатомных взаимодействий с помощью классических потенциальных сил.
  Различные функциональные формы, а также используемые в данных формулах коэффициенты принято называть \textit{силовыми полями} \cite{Ponder2003}.
  Получение таких полей зависит от большого числа параметров и, как правило, требует проведения физических экспериментов и квантово-механических вычислений.
  В связи с этим коэффициенты силовых полей могут разниться и с разным качеством описывать различные виды молекулярных систем.
  Тем не менее для белков, ввиду малого количества видов аминокислот (по сравнению с общим пространством допустимых органических соединений), применимы многие из этих полей: AMBER \cite{Cornell1995}, CHARMM36 \cite{Huang2017}, OPLS \cite{Jorgensen1996} и другие.

  Поле OPLS, особенно в последних редакциях, отлично зарекомендовало себя в точных оценках потенциальной энергии \cite{Sweere2017}, в связи с чем именно оно было
  выбрано в качестве основной рабочей модели.
  Разберем его устройство.

  В состав поля OPLS входят два вида основных взаимодействий: энергия связей и энергия несвязанных атомов.
  Первый вид энергетической функции рассматривает каждую связь между парой атомов как некоторый протяженный трехмерный объект, способный растягиваться (stretch), гнуться (bend) и скручиваться (torsion).
  Каждый из видов деформации приводит к изменению потенциальной энергии связей в соответствии с законами упругой деформации.

  Так, энергию растяжения можно получить, просуммировав потенциальную энергию по всем парам связанных атомов:
  $$
  E_{stretch} = \sum_{bonds}K(r - r_0)^2
  $$
  Текущая длина связи представлена через $r$ и может быть легко вычислена путем вычитания координат и взятия нормы вектора.
  Помимо этого параметра в данной формуле присутствуют два коэффициента: $K$ (коэффициент растяжения связи) и $r_0$ (равновесная длина связи).
  Коэффициенты являются табличными константами, определенными для каждой пары атомов, в соответствии с их типом.
  Например: $(N, C_{\alpha}), (C, N), (C_{\alpha}, C_{\beta}^{(Ala)})$ и т.д.

  Аналогично, используя коэффициент изгиба $K_{\theta}$ и равновесный угол $\theta_0$ можно ввести энергию изгиба, пользуясь всеми тройками последовательно связанных атомов:
  $$
  E_{bend} = \sum_{angles}K_{\theta}(\theta - \theta_0)^2
  $$

  Потенциальная энергия кручения определяется через двугранный угол $\phi$ между плоскостями, заданных осью, проходящей через крутящуюся связь и двум атомам, прилегающим с концам связи с разных сторон.
  Так как кручение связи имеет периодический характер, то и энергия также будет описываться некоторым периодическим законом.
  В поле OPLS все четверки последовательно связанных атомов образуют последний компонент связанной энергии следующим образом:
  $$
  \begin{aligned}
    E_{torsion} = \sum_{dihedrals}\frac{1}{2}(V_1(1 + cos(\phi)) &+ V_2(1 - cos(2\phi))\\
    &+ V_3(1 + cos(3\phi)) + V_4(1 - cos(4\phi)))
  \end{aligned}
  $$
  Для вычисления потребуется 4 табличных параметра: $V_1$, $V_2$, $V_3$, $V_4$.

  Три рассмотренных вида энергии дают нам полное представление обо всем потенциале связанных атомов:
  $$
  E_{bonded} = E_{stretch} + E_{bend} + E_{torsion}
  $$
  Помимо этой энергии все пары атомов взаимодействуют благодаря электростатическим силам, действующих вне зависимости от наличия связи.
  Такие силы раскладываются на две основных составляющих: кулоновские и силы Ван-дер-Ваальса, включающие в себя три вида слабых электромагнитных взаимодействий.
  Закон Кулона в хорошо известной форме полностью ложится в силовое поле OPLS:
  $$
  \begin{aligned}
  E_{cul} &= \sum_{i < j} \frac{q_i q_j e}{4 \pi \epsilon_0 r_{ij}} \\
         &= C \cdot \sum_{i < j} \frac{q_i q_j}{r_{ij}},\; \text{где } C = \frac{e}{4 \pi \epsilon_0} = 1389.38757  
  \end{aligned}
  $$
  Здесь заряды атомов $q_i$ являются табличными константами, а расстояние, как обычно, параметром.

  Ван-дер-Ваальсовы силы возникают вследствие поляризации атомов в присутствии друг друга и описываются известным потенциалом Леннарда-Джонса, который задает экспоненциальное отталкивание на расстояниях меньше радиуса атома и притяжение на расстояниях больше.
  Коэффициентами функции являются Ван-дер-Ваальсовый радиус атома $\sigma$ и глубина потенциальной ямы $\varepsilon$, параметром опять же расстояние между атомами:
  $$
  E_{vdw} = 4 \cdot \sum_{i < j} \sqrt{\varepsilon_i \varepsilon_j} \left( \left( \frac{\sqrt{\sigma_i \sigma_j}}{r_{ij}} \right)^{12} - \left( \frac{\sqrt{\sigma_i \sigma_j}}{r_{ij}} \right)^{6} \right).
  $$

  Итого, задача минимизация потенциала полной модели и силового поля OPLS будет выглядеть следующим образом:
  $$
  \min_{\{r\},\{\theta\},\{\phi\}} {E(\{r\},\{\theta\},\{\phi\}) = E_{stretch} + E_{bend} + E_{torsion} + E_{cul} + E_{vdw}}.
  % E_{non-bonded}
  $$
  Заметим, что набор переменных, по которым происходит оптимизация $\{r\}$, $\{\theta\}$, $\{\phi\}$ однозачно определяется положениями атомов $\{x_k ,y_k ,z_k\}$. Как уже отмечалось, именно в пространстве $\{x_k ,y_k ,z_k\}$ далее будет решаться выписанная задача. Отметим также, что значения коээфициентов (т.е. всех параметров, не являющихся переменными, по которым осуществляется оптимизация: $\{r\}$, $\{\theta\}$, $\{\phi\}$) определяются структурой рассматриваемой макромолекулы, и на данный момент не известны какие-то красивые (компактные) способы их задания, кроме как табличным образом \cite{Jorgensen1996}.

  % Итого, полная модель OPLS будет выглядеть следующим образом:
  % $$
  % E = E_{stretch} + E_{bend} + E_{torsion} + E_{non-bonded}
  % $$

  Для всех описанных выше функций $E_{bonds}$, $E_{bend}$, $E_{dihedrals}$ можно посчитать значение функции и градиента за $\mathcal{O}\left( n \right)$ арифметических операций, а для $E_{cul}$, $E_{vdw}$ -- за $\mathcal{O}\left( n^2 \right)$.
  При этом результат о сложности вычислении градиента можно получить с помощью автоматического дифференцирования \cite{nocedal2006sequential}. Подход, использующий автоматическое дифференцирование, требует очень аккуратного учета потребления (оперативной) памяти, что влечет необходимость явной имплементации функции для градиента.

  Важно отметить, что рассматривая белки в прикладных задачах, мы всегда подразумеваем существование некоторой внешней среды.
  Обычно эта среда представляет из себя раствор электролитов, то есть молекулы воды с ионами тех или иных солей.
  Представлять подобный раствор можно множеством молекул, взаимодействующих с белком по тем же законам.
  Однако такое представление оказывается вычислительно сложным и излишним в задаче минимизации энергии белка, так что вместо явного представления растворителя можно обратиться к приближению непрерывной среды.
  Данное приближение дает возможность пользоваться некоторым потенциалом силы $E_{solvation}$, действующей между растворителем и атомами рассматриваемых макромолекул.
  Можно выписать расчетные формулы для $E_{solvation}$ \cite{still1990semianalytical}, однако они достаточно громоздки, и могут быть информативными только для специалистов, поэтому было решено их здесь не приводить.
  Далее будет важно только то, что функция $E_{solvation}$ локально достаточно гладкая (можно посчитать градиент), а сложность вычисления функции и ее градиента -- $\mathcal{O}\left( n^2 \right)$, что соответствует (по порядку) сложности вычисления всех остальных слагаемых.
  В ряде экспериментов вычисления проводились без этого слагаемого.

  Задача, которую необходимо решать, может быть сформулирована следующим образом: найти в введенном координатном пространстве такую конфигурацию, которая бы минимизировала функцию $E$, если известно достаточно хорошее начальное приближение. Подчеркнем, что не требуется искать глобальный минимум, как было сказано ранее, это будет неверно с физической точки зрения.
  Нашей же задачей станет поиск именно локального минимума, в который можно попасть из рассматриваемой точки старта.

  Отметим при этом, что используя различные обобщения конструкции работы \cite{wales1997global} (например, Monotonic Basin Hopping -- MBH \cite{posypkin2010}), можно пытаться строить на базе описываемых далее локально сходящихся методов, новые методы, которые находят более глубокий минимум.
  Однако стоит отметить, что в подавляющем числе проведенных экспериментов не удалось сколько-нибудь существенно (больше, чем на 10\% от абсолютного значению функционала в найденном локальном минимуме) улучшать найденный локальный минимум.
  При этом процедура MBH запускалась на GPU на полный день, в то время как локальная минимизация работала десятки секунд.

  \subsection{Возможные подходы к решению задачи. Особенности применения градиентного метода}\label{subsec:sec1/su3}

  Прежде всего заметим, что рассматриваемая задача не является выпуклой. Особенно в окрестности начальной конфигурации. При выставлении начальной конфигурации (позы), некоторые атомы оказываются неправдоподобно близко к друг другу, что приводит к огромным значениям начальной энергии ($\sim 10^{8}$ кДж/моль вместо $\sim -10^4$ кДж/моль в минимуме). Приведем один характерный пример. Если зафиксировать единичный вектор, направленный вдоль антиградиента функции $E$, вычисленного в начальный момент, и двигаться вдоль этого направления, то можно наблюдать следующее поведение энергии $E$, см. рис. \ref{fig1D}.

  \begin{figure}
  \begin{center}
  \includegraphics[width=10cm]{1DSearch.jpg}
  \end{center}
   \caption{Отсутствие выпуклости для задачи минимизации OPLS force field}
    \label{fig1D}
   \end{figure}
   
  Более того, из структуры оптимизируемого функционала и рис. \ref{fig1D} понятно также, что оптимизируемый функционал может рассматриваться как гладкий только локально. Константа Липшица не является равномерно ограниченной в достаточно большой, но ограниченной окрестности точки старта. Но в таком случае для минимизации такого функционала нельзя использовать градиентные методы -- нет гарантий даже локальной сходимости \cite{ghadimi2015generalized}, \cite{nesterov2017random}. 
   
  Для решения таких задач глобальной оптимизации теория рекомендует использовать безградиентные методы типа simulated annealing или марковского поиска \cite{zhigljavsky2007stochastic}, \cite{zhigljavsky2012theory}. Ниже описывается подход, который был выбран изначально для решения задачи в отсутствии возможности использовать распараллеливание на GPU, но в условиях возможности использовать распараллеливание на CPU. Также предполагалось отсутствие составляющей  $E_{solvation}$ в $E$. В качестве языка программирования для реализации описываемого далее безградиентного метода использовался интерпретируемый язык Python 3.
   
  В основе предлагаемого подхода -- <<шевеление>> на каждой итерации только 
  одного, случайно выбранного, атома при <<замороженных>> остальных. Заметим, 
  что при изменении положения одного атома пересчет $E_{stretch} $, $E_{bend} 
  $, $E_{torsion} $ будет стоить $\mathcal{O}\left( 1 \right)$ с точки зрения сложности вычисления, поскольку 
  затрагивает только <<соседние>> по химическим связям атомы. Здесь под <<соседними>> 
  имеются в виду не только непосредственные соседи, но и соседи через две и даже три химические связи. Обычно число таких 
  <<соседей>> не больше 15. Для приближенного пересчета $E_{vdw} $ можно использовать 
  специальные структуры данных типа Kd Tree \cite{de2000computational}, которые имеются, например, в 
  библиотеке SciPy Python 3. Однако оптимально использовать в данном случае 
  деревья диапазонов (Range Tree), описанные, например, в главе 5 \cite{de2000computational}. С помощью такой структуры данных первый пересчет $E_{vdw} $ будет занимать 
  $\mathcal{O}\left( {\ln ^2n} \right)$ -- это время уходит на поиск ближайших 
  пространственных соседей рассматриваемого атома. Последующие пересчеты 
  $E_{vdw} $ при изменении положения того же атома будут занимать $\mathcal{
  O}\left( 1 \right)$. К сожалению, для небольших белков описанный способ пересчета $E_{vdw}$ не дает на практике ускорения по сравнению с честным пересчетом. Связано это с тем, что в число пространственных соседей, которых надо учитывать, для рассматриваемого атома попадают обычно десятки (а иногда и сотни) других атомов, см. рис. \ref{belok}. В протиивном случае, точность аппроксимации будет недостаточной. Еще сложнее дело обстоит с $E_{cul} $. 

  \begin{figure}
  \begin{center}
  \includegraphics[width=.5\linewidth]{Protein_cropped.png}
  \end{center}
  \caption{Структура типичного белка}
  \label{belok}
  \end{figure}
   
  Численные эксперименты убедительно свидетельствовали о том, что для пересчета значений $E_{cul} $ необходимо учитывать все слагаемые в сумме, которые связаны с рассматриваемым атомом. Действительно, относительный вклад в потенциал неучтенных частей $E_{vdw}$ для шара радиуса 7 A (радиус атома водорода 1 А -- один ангстрем) с центром в данном атоме составляет $\sim 10^{-5}$. Другое дело для электростатической составляющей $E_{cul}$ -- для этого же шара относительный вклад будет $\sim 10^{-3}$, что уже сопоставимо с возможной энергетической выгодой, которая получается при шевелении атома. Иначе говоря, нельзя не учитывать в электростатическом потенциале слагаемые вне шара! Поскольку всего таких слагаемых 
  $\mathcal{O}\left( n \right)$, то получается, что сложность пересчета $E_{cul} $ будет $\mathcal{O}\left( n \right)$, что на два порядка больше, чем по остальным 
  компонентам энергии. Далее излагается  оригинальная конструкция, позволяющая получить амортизационную сложность пересчета 
  $E_{cul} $ порядка $\mathcal{O}\left( 1 \right)$. 

  Прежде всего, заметим, что на одной итерации необходимо вычислить значение полной энергии при изменении положения одного атома не один раз, а несколько. Это необходимо для определения наилучшего положения этого атома при фиксированных положениях всех остальных атомов. Выберем число таких пересчетов порядка $n$. Покажем, как можно осуществить все эти $\sim n$ пересчетов за время $\mathcal{O}\left( n \right)$. Пусть на рассматриваемой итерации был выбран атом с номером $i$. Обозначим приращения положения этого   атома через $\{\delta _i^x ,\delta _i^y ,\delta _i^z \}$. Тогда за время $\mathcal{O}\left( {\ln ^2n} \right)$ (см. выше) сумму $E_{cul}$ можно разбить на две 
  неравнозначные составляющие. Первая составляющая ($\mathcal{O}\left( 1 \right)$ 
  слагаемых) будет содержать сумму по ближайшим пространственным соседям атома 
  $i$, а вторая сумма будет содержать все, что не вошло в первую ($\mathcal{
  O}\left( n \right)$ слагаемых). Вторую сумму с хорошей точностью можно 
  заменить ее рядом Тейлора по $\{\delta _i^x ,\delta _i^y ,\delta _i^z \}$ с 
  членами до первого порядка включительно. 
  Несложно понять, что вторая сумма есть просто линейная форма 
  $C_x \delta _i^x + C_y \delta _i^y +C_z \delta _i^z $, коэффициенты которой 
  $\left\{ {C_x ,C_y ,C_z } \right\}$ достаточно посчитать один раз (это можно 
  сделать за $\mathcal{O}\left( n \right))$. Таким образом, чтобы посчитать 
  значение $E_{cul}^i \left( {\delta _i^x ,\delta _i^y ,\delta _i^z } 
  \right)$ (измнение энергии $E_{cul}$, возникшее вследствии шевеления атома $i$ на ${\delta _i^x ,\delta _i^y ,\delta _i^z }$) один раз нужно время $\mathcal{O}\left( n \right)$, но чтобы посчитать 
  $E_{cul}^i \left( {\delta _i^x ,\delta _i^y ,\delta _i^z } \right)$ порядка 
  $n$ раз также нужно время $\mathcal{O}\left( n \right)$. Получается, что среднее 
  время на один расчет будет $\mathcal{O}\left( 1 \right)$. Таким образом, 
  пересчитать полную энергию порядка $n$ раз при изменении положения одного 
  случайно выбранного атома можно за время $\mathcal{O}\left( 
  n \right)$. Это и будет стоимостью итерации, на которой приближенно ищется 
  оптимальное положение случайно выбранного атома. Ясно, что число таких 
  <<дешевых>> итераций можно сделать очень большим. Более того, если 
  в рассматриваемой эпохе случайно <<нарезать>> исходную молекулу на 
  приблизительно одинаковые части и выбирать независимо в каждой части атомы 
  для шевеления (вдали от границ), то можно организовать параллельный запуск 
  описанного выше метода. В конце каждой эпохи все части снова собираются 
  вместе, и происходит новое нарезание на части, и процесс повторяется. Отметим, что для осуществления такого подхода весьма существенно, что атомы для шевеления выбираются случайно. 


  Впрочем, немного более громоздкие рассуждения, возможно, позволят ускорить 
  сходимость метода за счет выбора для шевеления не случайного атома, а атома, 
  от шевеления которого может быть наибольший эффект. Схожий подход был предложен в работе \cite{nutini2015coordinate}, где выбор осуществлялся при помощи правила 
  Гаусса--Саузвелла. К сожалению, чувствительность к шевелению можно определить лишь 
  локально, по пересчету частных производных. Тем не менее, подобно 
  описанному выше пересчету значений функции можно показать, что с аналогичной 
  сложностью можно пересчитывать и частные производные, поскольку пересчитывать их 
  нужно будет только у <<соседних>> атомов. Использование специальных структур и ограниченный пересчет необходимых составляющих функционала позволит поддерживать необходимый для алгоритма быстрый доступ к максимальной компоненте градиента.

  Описанный выше подход был реализован. Для белков средних размеров (около 100 аминокислот) при приближении к локальному минимуму было обнаружено, что подход перестает работать. Заметно не доходя до минимума, метод перестает улучшать честно посчитанное значение энергии. Дальнейший анализ показал, что неточность аппроксимации рядом Тейлора имеет порядок относительной точности $10^{-4}$ -- $10^{-3}$. Что сопоставимо с возможным энергетическим выигрышем, получаемым при шевелении выбранного атома. Данную погрешность можно уменьшить при помощи квадратичной аппроксимации. Однако значительное увеличение количества необходимых вычислений не позволит получить выигрыш от <<дешевизны>> итерации, которая лежит в основе метода. 

  Последующие эксперименты показали, что нет необходимости пересчитывать значение энергии в $\mathcal{O}\left( n \right)$ точках. Рассматривались следующие альтернативные стратегии: 
  \begin{enumerate}
    \item Выбрать случайно одну из координатных осей и посчитать значение энергии еще в двух точках на этой оси, по трем точкам построить параболу и сместить атом вдоль выбранной оси в точку минимума параболы (метод парабол); 
    \item Аналогичный метод, но в 3D, иначе говоря, использовать предыдущую стратегию, однако строить параболоид вращения по пяти аналогично вычисленным точкам; 
    \item По значениям энергии в текущей точке и близкой к ней соседней точке можно оценить частную производную энергии по данной координате и далее использовать различные варианты рандомизированных покомпонентных спусков \cite{conn2009introduction}, \cite{ghadimi2013stochastic}, \cite{wright2015coordinate}.
  \end{enumerate}
  Эксперименты показали, что такое огрубление (неточный поиск положения атома, который был выбран для шевеления) принципиально не меняет скорости сходимости. На рис. \ref{GF} показана работа непараллельного варианта обычного покомпонентного метода с постоянным, предварительно подобранным, шагом \cite{ghadimi2013stochastic}, запущенный на современном ноутбуке (процессор 2.2 GHz; оперативная память 16 ГБ). Сделанное огрубление заметно уменьшает стоимость итерации, не сильно проигрывая в числе итераций. Итоговый общий выигрыш во времени работы обусловлен тем, что выписанные ранее оценки не учитывали числовые константы, которые в подходе с аппроксимацией $E_{cul}$ оказались достаточно большими. 

  \begin{figure}
  \begin{center}
  \includegraphics[width=10cm]{GradNonParallel.png}
  \end{center}
  \caption{Сходимость безградиентного метода (шевеление случайного атома)}
   \label{GF}
  \end{figure}

  Наблюдаемая в экспериментах хорошая работа покомпонентного типа методов наводит на мысль, что и полноградиентные методы также могут хорошо работать несмотря на несоответствие функционала. Обычный градиентный спуск в среднем за 400 итераций сходится к такому же по качеству, с точки зрения значения энергии, решению см. рис. \ref{GF}, --- затрачивая на это приблизительно такое же время (5--10 часов). 


  \begin{figure}
  \begin{center}
  \includegraphics[width=12cm]{comparison_dashed.jpg}
  \end{center}
  \caption{Сравнение скорости сходимости безградиентного метода (шевеление случайного атома) с градиентным спуском}
   \label{Compare}
  \end{figure}

  На рис. \ref{Compare} приведено сравнение времени работы варианта покомпонентного спуска с градиентным спуском. Была выбрана конфигурация с изначально хорошим начальным условием (аналогичные результаты наблюдались и в общем случае). Представленный эксперимент проведен на стандартном ноутбуке(Intel Core i7-3630QM 2.40 GHz, 8GB) в стандартном режиме работы. Дополнительное отключение стандартных функций с целью повышения быстродействия и тюнинг операционной системы не проводились. Использовалась однопоточная реализация обоих методов на Python.

  Метод, обозначенный на графике как 'grad', представляет собой реализацию градиентного спуска с адаптивным шагом. 
  \iffalse
  При движении в направлении возрастания значения составляющей градиента мы уменьшаем величину шага, оставляя 0.8 от предыдущего значения. При уменьшении составляющей градиента -- возвращаемся к начальному значению.


  \begin{algorithm}
  \caption{Градиетный спуск}
  \SetKwFunction{FMain}{Вычисление шага}
  \SetKwProg{Fn}{Function}{:}{}
    \Fn{\FMain{}}{
    $ x^{k+1} = x^k - h * p^k;$ \tcp{$ p^k = \nabla f\left( x^k \right) $}   

    {\eIf{$ p^k \leq p^{k-1} $ \tcp{ Сравнение по каждой координате}}
    	{$ h = h - h * 0.2;$}
    	{$ h = startStep;$ \tcp{В эксперименте startStep = 0.015(const)}}
    }
  }
  \end{algorithm}

  \fi

  В безградиентном методе, обозначенном как 'parab', случайным образом выбирается атом в молекуле (для шевеления). Затем вычисляются значения энергии при 7 различных положениях этого атома: начальное положение, и по две дополнительные позиции, симметрично расположенные относительно начального положения вдоль каждой из трех координатных осей. По этим 7 точкам строится параболическая аппроксимация целевой функции (энергии) и вычисляется положение ее минимума. В итоге из 8 полученных точек выбирается та, которая доставляет минимальное значение целевой функции. Эта точка и принимается за новое положение системы атомов. 
  \iffalse
  По каждой из осей проводится экстраполяция параболой в зависимости от минимума которой и определяется возможная длина шага вдоль данной оси. После из полученных минимумов выбирается наименьший по значению энергии, который и используется для смещения выбранного атома. Иллюстрация одной итерации подобного подхода изображена на рис. \ref{figParab}. 
  \fi
  \iffalse
  При выборе направления и величины шага во внимание принимается также и точность предсказания параболой -- получаемая величина шага не должна в абсолютном значении значительно превосходить начальные значения шага, она должна уточнить его. Корректное поведение продемонстрировано на 
  рис. \ref{figParab}. 
  В случаях, когда все используемые вдоль одной из осей точки, например, расположены на одной ветви параболы -- для шага используется значение, соответствующее минимуму энергии из начальных точек на этой оси.


  \begin{figure}
  	\begin{center}
  		\includegraphics[width=10cm]{parabola_3d_3_crop.png}
  	\end{center}
  	\caption{Визуализация поиска оптимального значения смещения атома, $h$ -- величина шага, $h_{opt}$ -- оптимальное смещение}
  	\label{figParab}
  \end{figure} 
  \fi

  \iffalse
  Стоит заострить внимание на вопросе точности подобных изменений. Целью данного сравнения не является анализ абсолютных значений работы рассматриваемых алгоритмов. Данные при такого рода измерениях неизбежно имеют погрешность, поскольку работающий алгоритм не является единственным процессом в системе, и результат сильно зависит от количества свободной памяти за все время работы алгоритма. Сравнение проводится исключительно чтобы показать относительную близость скоростей сходимости двух методов.

  Данное сравнение позволяет определить является ли реализованный метод перспективным или стоит от него отказать в пользу другого. Схожая скорость сходимости позволяет, что безградиентный метод способен составить конкуренцию градиентному спуску.

  Также приведём сравнительную таблицу методов -- среднее время одной итерации и <<выедаемое>> среднее значение энергии.
  \fi

  \begin{table}[h]
  	\caption{Сравнение характеристик методов}
  	\label{tabular:timesandtenses}
  	\centering
  	\resizebox{\textwidth}{!}{
  		\begin{tabular}{|c|c|c|}
  			\hline
  			\fontsize{12pt}{12pt}\selectfont {\bfseries Показатели} & {\bfseries <<Шевеление>> атома} & {\bfseries Адаптивный градиентный спуск} \\
  			\hline
  			\fontsize{12pt}{12pt}\selectfont Время 1 итерации, секунды    & 1.2075 & 122.9814 \\
              \hline
  			\fontsize{12pt}{12pt}\selectfont Энергия 1 итерации, кДж/моль & 0.0225 & 2.7081 \\
  			\hline
  			\fontsize{12pt}{12pt}\selectfont $\sim\Delta$Энергии к 300 минуте, кДж/моль & $- 347$ & $- 413$ \\
  			\hline
  		\end{tabular}
  	}
  \end{table}

  Безградиентные методы, связанные с шевелением случайно выбранного атома (и их различные модификации), могут быть распараллелены на CPU, но не более чем на нескольких десятках процессоров (для белков стандартных размеров), потому что нельзя параллельно шевелить атомы, близкие к выбранному. Более того, процедура распараллеливания совсем нетривиальная. В итоге тут так и не удалось написать быстро работающую параллельную версию программы. При этом использовать в таком подходе всю мощь имеющихся GPU для ускорения расчетов по шевелению одного атома не представляет большого смысла, поскольку сложность каждого шевеления всего $\mathcal{O}\left( n \right)$. Совсем другое дело распараллеливание вычислений $E_{vdw}$, $E_{cul}$ (и $E_{solvation}$, если учитывается) и их градиентов на GPU. Структура этих функционалов (вида двойной суммы) с диапазоном суммирования в каждой из сумм по порядку равным числу ядер на видеокарте (использовалась видеокарта Tesla V100 c 5120 процессорами/ядрами (1.3 GHz) и общей оперативной памятью 32 ГБ) подсказывает естественный способ распараллеливания вычислений этих двойных сумм -- по внешней сумме. А именно, внешняя сумма представляет собой сумму $\mathcal{O}\left( n \right)$ слагаемых, каждое слагаемое, в свою очередь представляет собой сумму $\mathcal{O}\left( n \right)$ слагаемых. Каждую внутреннюю сумму вычисляет свой процессор (ядро) видеокарты. 

  Поскольку рассматривается задача, не являющаяся выпуклой задачей оптимизации, то можно лишь надеяться на локальную сходимость алгоритмов. Как известно, для достаточно гладких задач обычный градиентный спуск будет сходиться к локальному экстремуму таким образом, что норма градиента после $N$ итераций в общем случае будет убывать как $\sim N^{-1/2}$. Причем в классе методов первого порядка эта оценка не улучшаема  \cite{carmon2017lower2}. Кроме того, даже если использовать методы более высокого порядка, то рассчитывать на сходимость более быструю, чем $\sim N^{-1}$ все равно не приходится \cite{carmon2017lower1}. Заметим, что обычный градиентный спуск может <<застрять>> в седловой точке \cite{nesterov2013introductory}, т.е. не в локальном минимуме. Тем не менее, недавно было показано, что <<обычно>> градиентный спуск сходится именно к локальному минимуму \cite{lee2017first}, \cite{lee2016gradient}. Все эти результаты, однако, мало помогают в поиске наилучшего метода среди методов градиентного типа. Как показали численные эксперименты приведенные выше оценки локальной сходимости менее информативны (полезны), чем оценки глобальной сходимости методов в выпуклом случае. Дело в том, что в некоторой окрестности (локального) минимума можно рассчитывать на то, что поведение оптимизируемой функции с хорошей точностью соответствует поведению выпуклой функции. И хотя для рассматриваемой в  данной статье задаче априорно это совсем не очевидно, тем не менее, в экспериментах локально выпуклое поведение вполне подтверждалось.  

  Опыт использования градиентных методов для решения существенно невыпуклых задач обучения глубоких нейронных сетей \cite{Goodfellow2017} подсказывает, что если в экспериментах градиентные спуски неплохо работают, то можно попробовать использовать и ускоренные (быстрые/моментные) варианты градиентных спусков (см. следующий пункт). 
  Полезен может быть класс универсальных ускоренных методов, под универсальностью понимается возможность настройки на гладкость задачи и на ее выпуклость. Более того, для универсальных вариантов ускоренных методов было доказано, что для невыпуклых задач они сходятся не хуже обычных градиентных методов, а в случае наличия локальной выпуклой структуры могут ускоряться, как это имеет место в глобально выпуклом случае \cite{ghadimi2015generalized}, \cite{guminov2019accelerated}, \cite{guminov2019universal}. Стоит также отметить, что ранее варианты ускоренных методов (методы типа сопряженных градиентов) уже применялись для минимизации похожих потенциалов \cite{wales1997global}. В частности, в пакете инструментов для молекулярного моделирования Schrödinger \cite{sastry2013protein} используется один из таких методов, как метод Полака--Райбьера--Поляка. Сравнение ускоренного и градиентного методов представлено на рис. \ref{AG}.  На нем представлены два графика: график сходимости наилучшей версии (с адаптивным подбором шага по методу парабол) градиентного спуска и стандартная версия ускоренного градиентного спуска (с постоянным шагом). Заметно, что ускоренный сходится немного быстрее. В связи со всеми обнаруженными особенностями исследуемого функционала были предприняты попытки обнаружения наилучшего ускоренного градиентного спуска, не учитывая отсутствие выпуклости задачи. 

  \begin{figure}
  \begin{center}
  \includegraphics[width=12cm]{AccGrad.png}
  \end{center}
  \caption{Убывание энергии на итерациях градиентного спуска с правилом выбора шага по методу парабол и ускоренного градиентного спуска}
   \label{AG}
  \end{figure}


  Основным ограничением по использованию квазиньютоновских методов является необходимость в хранении и обновлении плотной квадратной матрицы $H_k $, что требует (в отличие от того, что имеет место для методов типа сопряженных градиентов) квадратичной памяти и квадратичного времени независимо от разреженности задачи. Это обстоятельство существенно ограничивает возможности по использованию таких методов для задач оптимизации с десятками тысяч переменных и более. Однако на практике используют в основном варианты таких методов \textit{с ограниченной памятью}, см., например, метод \textit{LBFGS} \cite{nocedal2006sequential}. В этом случае в памяти хранится не матрица $H_k $, а вектора, ее порождающие. Проблема, однако, тут в том, что с ростом $k$ размер этой памяти линейно растет. Поэтому обычно последовательности векторов $\left\{ {\gamma _t } \right\}$ и $\left\{ {\delta _t } \right\}$ хранят только с $m$ последних итераций ($m$ -- глубина памяти), и при этом полагают $H_{k-m} =I$. На практике $m$ часто выбирают совсем небольшим $m\simeq 3-5$.

  В заключение этого пункта приведем сопоставительный анализ методов первого порядка (градиентных методов) и методов более высокого порядка, которые могут использоваться для решения задач выпуклой оптимизации умеренных размеров ($n\le 10^4)$ в условиях отсутствия шума на классе достаточно гладких выпуклых задач
  \[
    \left\| {\nabla ^rf\left( y \right)-\nabla ^rf\left( x \right)} \right\|_2 
    \le M_r \left\| {y-x} \right\|_2 ,
    \quad
    x,y\in \mathbb{R}^n,
    \quad
    M_r \le \infty ,
    \quad
    r=0,1,2,... \;.
  \]
  Здесь $\nabla ^rf\left( y \right)$ --- это тензор ранга $r$. Поэтому следует пояснить, что понимается под 2-нормой в левой части данного неравенства. Ограничимся случаем $r=2$, тогда

  \[
    \nabla ^2f\left( x \right)=\left\| {{\partial ^2f\left( x \right)} 
    \mathord{\left/ {\vphantom {{\partial ^2f\left( x \right)} {\partial x_i 
    \partial x_j }}} \right. \kern-\nulldelimiterspace} {\partial x_i \partial 
    x_j }} \right\|_{i,j=1}^n ,
  \]
  \[
     \left\| {\nabla ^2f\left( y \right)-\nabla ^2f\left( x \right)} \right\|_2 
    = \sup_{\left\| {x_1 } \right\|_2 \le 1} \; \sup_{\left\| {x_2 } \right\|_2 \le 1} \left\langle {\left( {\nabla 
    ^2f\left( y \right)-\nabla ^2f\left( x \right)} \right)\left[ {x_1 } 
    \right],x_2 } \right\rangle = 
  \]
  \[
      \sup_{\left\| {x_1 } \right\|_2 \le 1} \; \sup_{\left\| {x_2 } \right\|_2 \le 1} \left\langle {\left( {\nabla 
    ^2f\left( y \right)-\nabla ^2f\left( x \right)} \right)x_1 ,x_2 } 
    \right\rangle
  \]
  В общем случае см. \cite{baes2009estimate}, \cite{nesterov2018implementable}. Отметим также, что при $r=0$ --- $\nabla ^0 f\left( x \right)=f\left( x \right)$. 

  Для класса методов, у которых на каждой итерации разрешается не более чем $\mathcal{O}\left( 1 \right)$ раз обращаться к оракулу (подпрограмме) за $\nabla ^rf\left( x \right)$, $r\le 1$, оценка числа итераций, необходимых для достижения точности $\varepsilon $ (по функции), будет иметь вид
  \[
    \mathcal{O}\left( {\min \left\{ {\left( {\frac{M_1 
    R^2}{\varepsilon }} \right)^{1/2};\;\;\left( {\frac{M_1 }{\mu }} \right)^{1/2}\ln \left( {\frac{\mu R^2}{\varepsilon }} 
    \right)} \right\}} \right).
  \]

  где, как и раньше, $R=\left\| {x^0-x_\ast } \right\|_2 $, $\Delta f=f\left( {x^0} \right)-f\left( {x_\ast } \right)$. Данная оценка в общем случае не может быть улучшена, даже известна дополнительная информация о $M_r$. \cite{Nemirovski1979} При этом данная оценка достигается \cite{nesterov2013introductory}, \cite{Nemirovski1979}.  Можно заметить, что быстрые градиентные методы и методы сопряженных градиентов улучшают последний аргумент минимума в этой оценке. Заметим, что если вместо $r=1$ имеет место $r=0$, то в приведенной оценке все аргументы минимума следует домножить на размерность пространства $n$ \cite{bayandina2017gradient}, \cite{dvurechensky2018accelerated}, \cite{Protasov1996}.

  Отметим также, что у известных сейчас методов, отвечающих (с точностью до логарифмического множителя) первому аргументу минимума, достаточно дорогой является составляющая итерации, не связанная с вычислением градиента: $\gg n^2$ \cite{lee2015faster}.

  Можно показать, что оценки для методов более высокого порядка (см. например \cite{nesterov2018implementable}, \cite{arjevani2017oracle}) ускоряют сходимость, однако при этом возникают сложные вспомогательные задачи, которые необходимо решать на каждой итерации \cite{gasnikov2018global}, \cite{nesterov2018implementable}, \cite{monteiro2013accelerated}. Поскольку в данном случае в виду отсутствия выпуклости и равномерной гладкости нельзя рассчитывать на дополнительное ускорение, связанное с использованием методов высокого порядка (не выполняются необходимые для этого условия), а при этом сложность итерации гарантированно существенно увеличивается, то было решено ограничиться только методами первого порядка, к которым были отнесены, в частности, метод Полака--Райбьера--Поляка и метод LBFGS.


\subsection{Практические аспекты реализации градиентных методов}\label{pract}

  Для тестирования были отобраны следующие методы: метод сопряженных градиентов Полака--Райбьера--Поляка (CG\_PRP), быстрый градиентный метод (FGM) и LBFGS (с параметром/памятью 3).

  Стоит отметить, что во всех алгоритмах выполняется нормирование текущего направления спуска. Эта процедура, не имеющая особого смысла с математической точки зрения, повсеместно применяется на практике т.к. существенно облегчает работу внутренних алгоритмов одномерного поиска (см. пункт \ref{LS_METHODS}), которые в изложенных ниже алгоритмах представлены как
  \[
     h_k = \argmin_{h} f( x^k + h r^k ) .
  \]
  Подобную запись следует интерпретировать именно как процедуру неточного одномерного поиска, а не <<математически честную>> минимизацию.  Тем не менее, каждая такая процедура либо находит релаксирующую точку, т.е. точку, в которой $f(x^k + h_k r^k) < f(x^k)$, либо выдают ошибку одномерного поиска, сигнализирующую, что наш градиентный метод <<зашёл в тупик>> и процедуру минимизации следует либо останавливать, либо использовать другое направление спуска.

\subsubsection{Метод FGM}
\label{FGM_DESC}

  Метод FGM (Fast Gradient Method) является реализацией быстрого (ускоренного) градиентного метода Ю.Е. Нестерова (см. пункт \ref{sec:ch1/sec1} и работу \cite{su2014differential}). Основным отличием от оригинальной версии является применение процедуры одномерного поиска на каждой итерации метода, т.к. значение параметра $L$ (константа Липшица) в нашем случае априори неизвестно. Особенностью метода является его немонотонность, т.е. в процессе спуска могут быть итерации, когда  $f(x^{k+1}) > f(x^k)$, поэтому необходимо дополнительно запоминать <<наилучшую точку>> $x^{\tilde{k}}$ которая и будет результатом работы метода:
  \[
    K = [0, N],\quad \tilde{k} \in K,\quad \forall k \in K: f(x^{\tilde{k}}) \le f(x^k) ,
  \]
  где $N$ --- число выполненных итераций.

  \iffalse
  Описание реализованного метода FGM представлено в алгоритме~\ref{FGM_ALGORITHM}.

  \begin{algorithm}[h]
    \caption{Метод FGM}
    \label{FGM_ALGORITHM}
    \KwData{$x^0 \in \mathbb{R}^n$}
    \KwResult{$x_*$}
    $x_* \leftarrow x^0$\;
    $x^{-1} \leftarrow x^0$\;
    $\theta^{-1} \leftarrow 1$\;
    $k \leftarrow 0$\;
    \Repeat {\textbf{convergence}} {
      $\theta_k \leftarrow 0.5\ \theta_{k-1} \left( \sqrt{ \theta_{k-1}^2 + 4} - \theta_{k-1} \right)$\;
      $\beta_k \leftarrow \theta_{k-1} (1 - \theta_{k-1}) / (\theta_{k-1}^2 + \theta_k)$\;
      $w^k \leftarrow x^k + \beta_k (x^{k} - x^{k-1})$\;
      $r^k \leftarrow -\nabla f(w^k) / \| \nabla f(w^k) \|_2$\;
      $h_k \leftarrow \argmin_{h \ge 0} f( w^k + h r^k )$\;
      $x^{k+1} \leftarrow w^k + h_k r^k$\;
      \If{$f(x^{k+1}) < f(x_*)$} {
        $x_* \leftarrow x^{k+1}$\;
      }
      Удаляем вектор $x^{k-1}$ из памяти\;
      $k = k + 1$\;
    }
  \end{algorithm}
  \fi

\subsubsection{Метод CG} \label{CG_DESC}

  Метод CG (Conjugate Gradient method) является реализацией метода сопряжённых градиентов (см. пункт \ref{sec:ch1/sec1} и соответсвующую работу \cite{nocedal2006sequential}). По умолчанию используется вариант Полак--Рибьер--Поляк (как показавший наилучшие результаты при решении задач рассматриваемого класса), но текущая реализация позволяет применять и другие известные варианты метода, в частности, Hestenes--Stiefel, Fletcher--Reeves, Polak--Ribière--Polyak(plus), Con\-ju\-gate Descent--Flet\-cher, Liu--Storey, Dai--Yuan (детали изложены в \cite{andrei_40_CG_2008}).

  Достоинством метода, определившего его высокую популярность на практике, является высокая скорость работы и универсальность применения --- при правильной настройке он показывает конкурентоспособные результаты на широком классе задач. К недостаткам стоит отнести необходимость рестартов, т.е. периодическому сбросу истории. К сожалению, на сегодняшний день не существует надёжных критериев рестарта, поэтому на практике применяются различные эвристические подходы, самый простой и популярный из которых --- рестарт каждые $K$ итераций. От того, насколько адекватно настроен критерий рестарта напрямую зависит эффективность работы алгоритма: как при слишком маленьком значении $K$ (тогда метод приближается к методу наискорейшего спуска), так и при слишком большом его значении скорость сходимости падает. Поэтому для получения максимальной производительности на конкретном классе задач следует потратить некоторое время на настройку параметров метода.

  Описание реализованного метода CG представлено в алгоритме~\ref{CG_ALGORITHM}.

  \begin{algorithm}[!h]
    \caption{Метод CG (сопряжённых градиентов)}
    \label{CG_ALGORITHM}
    \KwData{$x^0 \in \mathbb{R}^n$}
    \KwResult{$x^{k}$}
    $r^0 \leftarrow -\nabla f(x^0)$\;
    $k \leftarrow 0$\;
    \Repeat{\textbf{convergence}.} {
      $\tilde{r}^k \leftarrow r^k / \| r^k \|_2$\;
      $h_k \leftarrow \argmin_{h \ge 0} f( x^k + h \tilde{r}^k )$\;
      $x^{k+1} \leftarrow x^k + h_k \tilde{r}^k$\;
      $s^{k} \leftarrow x^{k+1} - x^k$\;
      $y^{k} \leftarrow \nabla f( x^{k+1} ) - \nabla f( x^k )$\;
      \eIf{\textbf{restart}} {
        \tcp{Выполяем рестарт метода (см. описание)}
        $r^{k+1} \leftarrow -\nabla f(x^{k+1})$\;
      } {
        \tcc{Вычисляем значение $\beta_k$ согласно используемому варианту метода (см. пункт \ref{sec:ch1/sec1})}
        $r^{k+1} \leftarrow -\nabla f( x^{k+1} ) + \beta_k r^k$\;
      }
      $k = k + 1$\;
    }
  \end{algorithm}

\subsubsection{LBFGS} \label{LBFGS_DESC}

  Метод LBFGS (Limited-memory BFGS method) является реализацией квазиньютоновского метода BFGS (см. пункт \ref{sec:ch1/sec1} и работу \cite{nocedal2006sequential}). Этот метод также является одним из самых популярных на практике в силу своей универсальности и высокой эффективности. Вычислительная сложность его итерации существенно меньше, чем у <<полных>> квазиньютоновских методов и сопоставима с методами FGM и CG, т.к. используются только операции линейной алгебры над векторами, а операции матричного типа отсутствуют. Преимуществом  перед методом CG является отсутствие процедур рестарта, поскольку размер используемой истории явно задаётся параметром $m$. Вычислительный опыт показывает, что в большинстве случаев значения $m$ порядка 3--5 вполне достаточно  и дальнейшее его увеличение лишь повышает вычислительную сложность метода без улучшения качества его работы.

  Описание реализованного метода LBFGS представлено в алгоритмах~\ref{LBFGS_DIR_ALGORITHM}~и~\ref{LBFGS_ALGORITHM}.

  \begin{algorithm}[!h]
    \caption{Вычисление направления спуска (метод LBFGS)}
    \label{LBFGS_DIR_ALGORITHM}
    \KwData{$k \ge 0$}
    $r \leftarrow -\nabla{f(x^k)}$\;
    \If{$k = 0$} {
      \Return $r \leftarrow r / \| r \|_2$\;
    }
    \For{$i = (k-1), (k-2), ..., (k-m)$} {
      $\alpha_i \leftarrow \rho_i \langle s^i, r \rangle$\;
      $r \leftarrow r - \alpha_i y^i$\;
    }
    $r \leftarrow \langle s^k, y^k \rangle / \langle y^k, y^k \rangle r$\;
    \For{$i = (k-m), (k-m+1), ..., (k-1)$} {
      $\beta \leftarrow \rho_i \langle y^i, r \rangle$\;
      $r \leftarrow r + (\alpha_i - \beta) s^i$\;
    }
    \Return $r$\;
  \end{algorithm}

  \begin{algorithm}[!h]
    \caption{Метод LBFGS}
    \label{LBFGS_ALGORITHM}
    \KwData{$x^0 \in \mathbb{R}^n$, $m \in \mathbb{N}$}
    $k \leftarrow 0$\;
    \Repeat{\textbf{convergence}.} {
      Вычисляем текущее направление спуска $r^k$ используя алгоритм \ref{LBFGS_DIR_ALGORITHM}\;
      $h_k \leftarrow \argmin_{h \ge 0} f\left( x^k + h r^k \right)$\;
      $x^{k+1} \leftarrow x^k + h_k r^k$\;
      $s^k \leftarrow x^{k+1} - x^k$\;
      $y^k \leftarrow \nabla f( x^{k+1} ) - \nabla f( x^k )$\;
      $\rho_k \leftarrow 1 / \langle s^k, y^k \rangle$\;
      Удаляем вектора $s^{k-m}$, $y^{k-m}$ из памяти\;
      $k \leftarrow k + 1$\;
    }
    \Return $x_k$\;
  \end{algorithm}

\subsection{Методы одномерного поиска}
\label{LS_METHODS}

  Все представленные ранее методы для своей работы используют процедуру одномерного поиска. Рассматриваемый в работе класс оптимизируемых функций является вычислительно затратным, т.е. каждое обращение (запрос) к оракулу (вычисление функции и, особенно, её градиента), требует выполнения достаточно большого объёмы вычислений. Поскольку процесс одномерного поиска неизбежно приводит к таким обращениям и, соответственно, вычислительным затратам, первостепенным вопросом с точки зрения повышения эффективности работы алгоритмов становится необходимость уменьшения количества таких запросов. Использование плохо оптимизированных процедур одномерного поиска, которые многократно вычисляют значение оптимизируемой функции в процессе своей работы, может радикально замедлить общее быстродействие и <<свести на нет>> все усилия, затраченные на вышестоящие алгоритмы. Можно утверждать, что в нашем случае именно процедуры одномерного поиска являются <<бутылочным горлышком>>, от которого, в итоге, и зависит скорость поиска локального экстремума.

  В рамках проведённых экспериментов использовались два метода одномерного поиска. Они основаны на хорошо зарекомендовавших себя на практике подходах (см., например \cite{nocedal_wright_2006} \cite{numerical_recipes_2007}), но также реализуют ряд идей, направленных как на учёт специфики исследуемого класса задач, так и на повышение вычислительной эффективности. В частности, используется история работы алгоритма, т.е. шаг одномерного поиска, найденный на предыдущей итерации градиентного метода, становится стартовым значением при следующем запуске одномерной минимизации. Даже если он является удачным, методы одномерного поиска пытаются найти другой шаг, обеспечивающий лучшую релаксацию оптимизируемой функции. Такое поведение позволяет обработать ситуации, когда, например, был (неудачно) выбран небольшой стартовый шаг ($10^{-3}$, например), постоянно применяя который, градиентный метод может спускаться, но скорость такого спуска будет абсолютно неудовлетворительной. В целях повышения быстродействия число обращений к оракулу сокращено до минимально возможного --- в большинстве случаев оказывается достаточно 2-3 вызовов для нахождения релаксирующей точки. Краеугольной идеей используемых алгоритмов одномерного поиска является переход от максимального качества работы (т.е. <<честный>> $\argmin$) к максимальному быстродействию. Проведённые вычислительные эксперименты показали, что за фиксированное время более выгодным оказывается выполнение б\'{о}льшего числа грубых итераций градиентного метода чем меньшего числа высокоточных. 
  
  Подробное описание методов одномерного поиска носит больший интерес с технической точки зрения, чем с теоретической. Многие детали относятся к специфике разработки кода для конкретного устройства, а именно --- кеширование результатов, частичная развертка цикла и т.п. По данным причинам мы не будем углубляться в их описание. 

  \iffalse
  \begin{algorithm}[!h]
    \caption{Метод одномерного поиска LS\_H}
    \label{LS_H_ALGORITHM}
    \KwData{ $x^0 \in \mathbb{R}^n$, $r \in \mathbb{R}^n$,}
    $h_0 > 0$, $0 < \varepsilon_h < 1$, \;
    $k_{(+)} > 1$, $0 < k_{(-)} < 1$\;
    $f_* \leftarrow f(x^0)$ \tcp*{Выполняем, только если значение не вычислено в вызывающем методе}
    $\tilde{f}_0 \leftarrow f(x^0 + h_0 r)$\;
    \If{$\tilde{f}_0 < f_*$} {
      $h_1 \leftarrow k_{(+)} h_0$\;
      $\tilde{f}_1 \leftarrow f(x^0 + h_1 r)$\;
      \If{$\tilde{f}_1 < \tilde{f}_0$} {
        \Return ($h_1, \tilde{f}_1$)\;
      }
      \Return ($h_0, \tilde{f}_0$)\;
    }
    $h_2 \leftarrow k_{(-)} h_0$\;
    $\tilde{f}_2 \leftarrow f(x^0 + h_2 r)$\;
    \While{$\tilde{f}_2 > f_*$} {
      $h_2 \leftarrow k_{(-)} h_2$\;
      \If{$h_2 \le \varepsilon_h$} {
        \Return ($0, f_*$) \tcp*{Невозможно найти релаксирующий шаг}
      }
      $\tilde{f}_2 \leftarrow f(x^0 + h_2 r)$\;
    }
    \Return ($h_2, \tilde{f}_2$)\;
  \end{algorithm}

  \begin{algorithm}[!h]
    \caption{Метод одномерного поиска LS\_PAR}
    \label{LS_PAR_ALGORITHM}
    \KwData{ $x^0 \in \mathbb{R}^n$, $r \in \mathbb{R}^n$,}
    $h_0 > 0$, $K \ge 2$, $G_0 \in \{ true, false\}$\;
    $\tilde{h}_0 \leftarrow 0$\;
    $\tilde{f}_0 \leftarrow f_* \leftarrow f(x^0)$ \tcp*{Значение уже вычислено в вызывающем методе}
    \eIf{$G_0 = true$} {
      $g_* \leftarrow \nabla f(x^0)$ \tcp*{Значение уже вычислено в вызывающем методе}
      $\tilde{h}_1 \leftarrow h_0$\;
      $\tilde{f}_1 \leftarrow f(x^0 + \tilde{h}_1 r)$\;
      \tcc{По 3-м парам $(0, f_*)$, $(0, \langle g_*, r \rangle )$ и $(\tilde{h}_1, \tilde{f}_1)$ строится аппроксимация параболы, точка минимума которой заносится в $\tilde{h}_2$}
      $\tilde{f}_2 \leftarrow f(x^0 + \tilde{h}_2 r)$\;
    }{
      $\tilde{h}_1 \leftarrow -1/2 h_0$\;
      $\tilde{f}_1 \leftarrow f(x^0 + \tilde{h}_1 r)$\;
      $\tilde{h}_2 \leftarrow  1/2 h_0$\;
      $\tilde{f}_2 \leftarrow f(x^0 + \tilde{h}_2 r)$\;
    }
    \For{$k = 2, ..., K$} {
      \tcc{По 3-м парам $(\tilde{h}_0, \tilde{f}_0)$, $(\tilde{h}_1, \tilde{f}_1)$ и $(\tilde{h}_2, \tilde{f}_2)$ строится аппроксимация параболы, точка минимума которой заносится в $\tilde{h}_3$}
      $\tilde{f}_3 \leftarrow f(x^0 + \tilde{h}_3 r)$\;
      \tcc{Вектора $\tilde{h}$ и $\tilde{f}$ согласовано сортируются по возрастанию элементов $\tilde{f}$}
    }
    \If{$K = 2$} {
      \tcc{Вектора $\tilde{h}$ и $\tilde{f}$ согласовано сортируются по возрастанию элементов $\tilde{f}$}
    }
    \If{$\tilde{f}_0 < f_*$} {
      \Return ($\tilde{h}_0, \tilde{f}_0$)\;
    }
    \Return ($0, f_*$) \tcp*{Релаксирующий шаг не найден}
  \end{algorithm}
  \fi

\subsection{Результаты экспериментов}

  Проверка описанных выше методов оптимизации производилась в контексте алгоритмов для решения задачи предсказания белок-белковых комплексов (белок-белковый докинг). Простыми словами данная задача может быть описана следующим образом: для имеющейся пары молекул нужно определить наиболее вероятное взаимное расположение этих молекул при образовании молекулярного комплекса белок-белок. Задача считается успешно решенной, если на выходе имеется структура комплекса, которая соответствует встречаемому в природе комплексу или очень близка к этому положению. Задача белок-белкового докинга имеет большую значимость для решения прикладных задач, связанных с разработкой лекарственных средств, основанных на белковых молекулах, так как терапевтический эффект обусловлен связыванием терапевтического белка с мишенью, а это и есть белок-белковый комплекс. Также стоит отметить, что в рамках типичного проекта по разработке лекарственного препарата, основанного на антителах,
  %в компании БИОКАД 
  эту задачу требуется решать сотни раз.

  На промежуточных этапах решения задачи белок-белкового докинга могут быть найдены, и очень часто находятся, физически-невозможные структуры комплексов, в частности, структуры со столкнувшимися атомами. Такие ситуации возникают в следствии использования грубых метрик на начальных этапах с целью ускорения процесса сканирования большого пространства решений. Следствием этой проблемы является не возможность применения наиболее физически обоснованной метрики для финального ранжирования решений -- потенциальной энергии белкового комплекса. Эта метрика является наиболее обоснованной, так как меньшему значению потенциальной энергии соответствует более энергетически выгодная молекулярная структура, отсюда следует что минимуму (либо одному из возможных минимумов) потенциальной энергии соответствует структура, встречаемая в природе. Учитывая вышесказанное, минимизация энергии является одним из важнейших этапов решения задачи докинга, так как позволяет привести структуру комплекса в физически-оправданную конформацию, а значит, и позволяет использовать энергетические метрики в качестве метрик оценки решений, что, в свою очередь существенно повышает точность предсказаний. 

  Для оценки методов минимизации был использован тестовый набор для оценки точности белок-белкового докинга. Этот набор состоит из белок-белковых комплексов с достоверно известной трехмерной структурой, встречаемой в природе (нативная структура), другими словами правильный ответ уже известен. Часть данных были получены из прошедших раундов соревнований по белок-белковому докингу CAPRI (Critical Assessment of PRedicted Interactions) \cite{janin2002welcome} и из курируемого набора данных для белок-белкового докинга Protein-Protein Docking Becnchmark 5.0 \cite{vreven2015updates}. Для каждого комплекса были сгенерированы 499 начальных приближений, отличающихся друг от друга расположением и поворотом в трехмерном пространстве одной из молекул. То есть одна из молекул комплекса оставалась неподвижной, изменялось же расположение только второй молекулы. Кроме того, было искусственно добавлено заведомо верное расположение молекул, соответствующее нативным структурам. Так как нативной структуре комплекса соответствует минимум потенциальной энергии (E), в данном эксперименте производилась минимизация всех начальных приближений, сортировка по значению E и оценка порядкового номера нативной (или очень близкой к таковой) структуры. Если близкая к нативной структура присутствует в первых 30 результатах, задача считалась успешно решенной.

  В качестве основного режима работы реализованных алгоритмов был выбран float (одинарная точность), что обусловлено как более высоким быстродействием современных GPU при работе с вещественными числами одинарной точности, так и тем, что для требуемых расчетов float-арифметика обеспечивает достаточную точность результатов.

  Для определения близости к нативной структуре была использована метрика RMSD (Root-mean-square deviation), которая является стандартной для измерения расстояния между подобными молекулярными структурами 
  \[
  RMSD = \sqrt{\frac{1}{n}\sum_{i=1}^n ||v_i - w_i||^2},
  \]
  где $n$ -- утроенное число атомов (каждый атом описывается тремя координатами), а $v_i$, $w_i$ -- соответствующие координаты соответствующих атомов, сравниваемых структур.
  Структура считалась близкой к нативной при значении RMSD < 10Å. Этот порог является максимальным при оценке решений во всемирных соревнованиях по белок-белковому докингу CAPRI \cite{janin2002welcome}, поэтому он и был взят. Методы оценивались с точки зрения RMSD и значения потенциальной энергии.

  На основании средних значений наименьших индексов структур, близких к нативным, было получено, (более подробно технические детали изложены в \cite{yakovlev2019algorithms}) что методы FGM и CG\_PRP показывают результаты лучше, нежели LBFGS. Тем не менее, тестовый набор данных недостаточно велик для окончательного выбора одного метода. Существует ряд тестовых наборов, в которых метод LBFGS показывает лучшие результаты с точки зрения абсолютного значения потенциальной энергии, тем не менее, с точки зрения решения практической задачи, решаемой в этом эксперименте, метод LBFGS ощутимо проигрывает.

  \iffalse
  \begin{table}[h]
    \scriptsize
    \begin{center}
    \begin{tabular}{ l | r | r | r | c | c | c }
      идентификатор 
      & \multicolumn{3}{p{3.5cm}|}{\centering индекс структуры, \\ близкой к нативной} 
      & \multicolumn{3}{p{3.5cm}}{\centering RMSD \\ до нативной позы, Å} \\ 
      \hline
       & LBFGS & FGM & CG\_PRP & LBFGS & FGM & CG\_PRP \\
       capri\_8\_22 & 2 & 0 & 0 & 2.16 & 0.63 & 0.63 \\
       capri\_7\_21 & 4 & 7 & 13 & 0.45 & 1.79 & 1.79 \\
       capri\_5\_17 & 121 & 12 & 20 & 0.82 & 0.87 & 0.87 \\
       capri\_5\_14 & 1 & 0 & 0 & 0.91 & 3.00 & 3.00 \\
       capri\_34\_104 & 1 & 0 & 1 & 0.38 & 1.88 & 1.88 \\
       capri\_32\_98 & 0 & 0 & 0 & 0.38 & 0.88 & 0.88 \\
       capri\_32\_101 & 1 & 0 & 1 & 0.84 & 0.80 & 0.80 \\
       capri\_30\_94 & 2 & 0 & 0 & 0.67 & 0.33 & 0.33 \\
       capri\_30\_88 & 2 & 1 & 0 & 0.35 & 1.20 & 1.20 \\
       capri\_30\_86 & 233 & 173 & 262 & 1.73 & 0.34 & 0.34 \\
       capri\_30\_85 & 0 & 0 & 0 & 0.93 & 0.27 & 0.27 \\
       capri\_30\_84 & 1 & 0 & 0 & 0.29 & 0.86 & 0.86 \\
       capri\_30\_79\_3 & 52 & 311 & 114 & 0.43 & 0.33 & 0.33 \\
       capri\_30\_79\_2 & 8 & 8 & 19 & 0.76 & 0.72 & 0.72 \\
       capri\_30\_79\_1 & 0 & 0 & 0 & 0.92 & 0.35 & 0.35 \\
       capri\_30\_77\_4 & 6 & 2 & 5 & 0.33 & 0.31 & 0.31 \\
       capri\_30\_77\_3 & 211 & 245 & 346 & 0.35 & 0.33 & 0.33 \\
       capri\_30\_77\_2 & 1 & 0 & 0 & 0.35 & 0.34 & 0.34 \\
       capri\_30\_77\_1 & 19 & 6 & 9 & 0.77 & 0.34 & 0.34 \\
       capri\_30\_72\_3 & 14 & 111 & 41 & 0.22 & 0.29 & 0.29 \\
       capri\_30\_72\_2 & 28 & 11 & 98 & 0.23 & 0.29 & 0.29 \\
       capri\_30\_72\_1 & 8 & 7 & 21 & 2.53 & 0.28 & 0.28 \\
       capri\_30\_69\_2 & 233 & 157 & 203 & 0.23 & 0.27 & 0.27 \\
       capri\_30\_69\_1 & 0 & 0 & 0 & 0.29 & 0.30 & 0.30 \\
       capri\_30\_68\_2 & 44 & 23 & 49 & 0.38 & 0.35 & 0.35 \\
       capri\_30\_68\_1 & 4 & 0 & 3 & 0.51 & 0.39 & 0.39 \\
       capri\_27\_58 & 0 & 0 & 1 & 2.21 & 1.04 & 1.04 \\
       capri\_26\_55 & 27 & 37 & 41 & 0.42 & 7.19 & 7.19 \\
       capri\_26\_54 & 316 & 325 & 357 & 0.31 & 0.37 & 0.37 \\
       capri\_24\_50 & 98 & 140 & 35 & 8.10 & 8.05 & 8.05 \\
       capri\_22\_46 & 469 & 485 & 466 & 0.81 & 0.80 & 0.80 \\
       capri\_19\_41 & 27 & 158 & 4 & 1.79 & 0.77 & 0.77 \\
       capri\_18\_40 & 63 & 106 & 317 & 2.35 & 2.35 & 2.35 \\
       capri\_15\_36\_min & 451 & 467 & 436 & 0.40 & 0.49 & 0.49 \\
       capri\_15\_36 & 248 & 409 & 375 & 0.50 & 0.49 & 0.49 \\
       capri\_15\_32\_min & 360 & 29 & 144 & 0.30 & 0.38 & 0.38 \\
       capri\_15\_32 & 13 & 31 & 9 & 3.05 & 3.04 & 3.04 \\
       capri\_11\_27 & 263 & 295 & 183 & 0.47 & 0.41 & 0.41 \\
       capri\_10\_26 & 175 & 81 & 27 & 8.69 & 5.57 & 5.57 \\
       4R9Y & 79 & 67 & 70 & 0.67 & 0.71 & 0.71 \\
       4G6J & 23 & 0 & 1 & 0.33 & 0.38 & 0.38 \\
       4G5Z & 30 & 64 & 87 & 3.78 & 3.75 & 3.75 \\
       3MXW & 5 & 2 & 2 & 0.44 & 0.43 & 0.43 \\
       3EO9 & 188 & 153 & 248 & 0.33 & 0.45 & 0.45 \\
       2JEL & 391 & 143 & 375 & 5.43 & 0.48 & 0.48 \\
       1VFB\_min & 130 & 27 & 3 & 1.76 & 8.77 & 8.77 \\
       1VFB & 49 & 6 & 4 & 1.76 & 8.76 & 8.76 \\
       1PPE & 0 & 8 & 0 & 0.27 & 0.60 & 0.60 \\
       1NTY & 328 & 105 & 55 & 7.61 & 7.65 & 7.65 \\
       1KXQ & 32 & 15 & 25 & 7.62 & 0.41 & 0.41 \\
       1IJK & 165 & 5 & 80 & 0.42 & 0.49 & 0.49 \\
       1I9R & 23 & 0 & 13 & 5.86 & 5.87 & 5.87 \\
       1FSK & 92 & 10 & 93 & 1.53 & 0.50 & 0.50 \\
       1E6J\_min & 178 & 347 & 28 & 1.33 & 1.37 & 1.37 \\
       1E6J & 22 & 32 & 6 & 1.34 & 0.34 & 0.34 \\
       1DQJ & 333 & 262 & 263 & 0.58 & 0.60 & 0.60 \\
       1CGI & 5 & 10 & 0 & 0.62 & 0.62 & 0.62 \\
       1BVK & 101 & 18 & 63 & 0.47 & 9.48 & 9.48 \\
       1BJ1 & 372 & 88 & 238 & 8.76 & 8.84 & 8.84 \\
       1AY7 & 72 & 52 & 77 & 6.77 & 0.52 & 0.52 \\
       1AVX & 5 & 1 & 37 & 0.51 & 0.41 & 0.41 \\
       \hline
       среднее & 100.5 & 82.8 & 88 &
    \end{tabular}
    \end{center}
    \caption{Данные о порядковом номере структуры, близкой к нативной, при ранжировании структур по значению потенциальной энергии белок-белкового комплекса. }
    \label{table_res_1}
  \end{table}

  \begin{table}[h]
    \scriptsize
    \begin{center}
    \begin{tabular}{ l | r | r | r | r | r | r }
      идентификатор 
      & \multicolumn{2}{c|}{LBFGS}
      & \multicolumn{2}{c|}{FGM} 
      & \multicolumn{2}{c}{CG\_PRP} 
      \\ 
      \hline
       & $E_{best}$ & $E_{native}$
       & $E_{best}$ & $E_{native}$
       & $E_{best}$ & $E_{native}$
       \\
       capri\_8\_22 & -50571.9 & -45926.6 & -45910.7 & -45910.7 & -46031.8 & -46031.8 \\
       capri\_7\_21 & -78483.8 & -78415.2 & -77749.9 & -77649.0 & -78406.9 & -78254.6 \\
       capri\_5\_17 & -92668.5 & -92341.4 & -93201.7 & -93108.6 & -93017.3 & -92909.6 \\
       capri\_5\_14 & -167005.4 & -137625.4 & -137974.8 & -137974.8 & -138476.5 & -138476.5 \\
       capri\_34\_104 & -55082.3 & -52252.9 & -51961.6 & -51961.6 & -52261.8 & -52181.4 \\
       capri\_32\_98 & -91536.9 & -91536.9 & -91371.0 & -91371.0 & -91809.7 & -91809.7 \\
       capri\_32\_101 & -104513.7 & -98930.8 & -98579.6 & -98579.6 & -100455.5 & -99204.5 \\
       capri\_30\_94 & -162684.0 & -160101.2 & -160394.6 & -160394.6 & -160788.5 & -160788.5 \\
       capri\_30\_88 & -79515.5 & -77559.4 & -79207.1 & -77719.3 & -77917.1 & -77917.1 \\
       capri\_30\_86 & -62108.7 & -48289.3 & -48505.1 & -48378.4 & -48692.0 & -48482.8 \\
       capri\_30\_85 & -126991.7 & -126991.7 & -127564.2 & -127564.2 & -128053.6 & -128053.6 \\
       capri\_30\_84 & -118528.3 & -110604.9 & -111089.0 & -111089.0 & -111608.4 & -111608.4 \\
       capri\_30\_79\_3 & -34419.0 & -34325.9 & -34684.9 & -34481.0 & -34799.7 & -34662.9 \\
       capri\_30\_79\_2 & -34484.8 & -34401.5 & -34691.5 & -34675.7 & -34838.9 & -34762.4 \\
       capri\_30\_79\_1 & -34537.4 & -34537.4 & -34852.6 & -34852.6 & -34913.7 & -34913.7 \\
       capri\_30\_77\_4 & -87193.1 & -84520.2 & -84456.4 & -84368.1 & -88233.9 & -84805.8 \\
       capri\_30\_77\_3 & -84664.4 & -84297.0 & -84502.2 & -84140.1 & -93717.8 & -84403.0 \\
       capri\_30\_77\_2 & -86309.6 & -84786.4 & -84639.5 & -84639.5 & -85033.2 & -85033.2 \\
       capri\_30\_77\_1 & -109759.2 & -84593.2 & -84512.3 & -84422.0 & -85030.6 & -84882.7 \\
       capri\_30\_72\_3 & -249637.5 & -202470.5 & -203271.8 & -203002.0 & -203292.8 & -203096.8 \\
       capri\_30\_72\_2 & -210381.7 & -202457.4 & -203257.2 & -203139.8 & -203323.6 & -203021.0 \\
       capri\_30\_72\_1 & -208147.4 & -202503.2 & -203316.9 & -203122.3 & -203224.0 & -203101.1 \\
       capri\_30\_69\_2 & -145319.0 & -140952.8 & -142658.1 & -141713.3 & -142782.6 & -141980.8 \\
       capri\_30\_69\_1 & -141995.4 & -141995.4 & -142747.0 & -142747.0 & -142954.4 & -142954.4 \\
       capri\_30\_68\_2 & -55235.1 & -51297.3 & -51283.9 & -51104.5 & -51750.1 & -51479.0 \\
       capri\_30\_68\_1 & -70131.8 & -51531.3 & -51239.6 & -51239.6 & -74855.2 & -51790.3 \\
       capri\_27\_58 & -72984.6 & -72984.6 & -72805.0 & -72805.0 & -87178.7 & -73048.7 \\
       capri\_26\_55 & -128419.5 & -127281.6 & -139758.6 & -127382.3 & -131501.1 & -127719.4 \\
       capri\_26\_54 & -45554.3 & -45371.6 & -45540.1 & -45374.1 & -45793.4 & -45533.7 \\
       capri\_24\_50 & -128720.1 & -128495.8 & -128904.9 & -128668.2 & -129142.6 & -128981.7 \\
       capri\_22\_46 & -87610.8 & -79320.3 & -87177.7 & -77999.9 & -88032.3 & -79731.6 \\
       capri\_19\_41 & -57530.0 & -57385.3 & -57395.9 & -57235.2 & -57712.4 & -57675.9 \\
       capri\_18\_40 & -76213.3 & -75956.5 & -76568.1 & -76466.1 & -76742.4 & -76493.8 \\
       capri\_15\_36\_min & -112675.1 & -112097.7 & -113226.7 & -112679.8 & -113655.4 & -113161.1 \\
       capri\_15\_36 & -112614.7 & -112311.3 & -113120.4 & -112760.3 & -113613.7 & -113245.4 \\
       capri\_15\_32\_min & -94423.4 & -86603.0 & -86905.2 & -86812.8 & -87088.4 & -86928.4 \\
       capri\_15\_32 & -87701.2 & -86832.0 & -86871.9 & -86759.9 & -87046.8 & -86995.9 \\
       capri\_11\_27 & -78181.6 & -77894.2 & -78157.2 & -77887.1 & -78501.1 & -78248.3 \\
       capri\_10\_26 & -107489.4 & -105406.4 & -105762.5 & -105598.2 & -105937.4 & -105844.0 \\
       4R9Y & -111822.6 & -109022.7 & -108823.6 & -108700.9 & -109864.2 & -109644.2 \\
       4G6J & -119912.2 & -119798.3 & -120160.1 & -120160.1 & -120480.3 & -120470.9 \\
       4G5Z & -156913.6 & -118867.8 & -119250.9 & -119045.6 & -119637.3 & -119498.1 \\
       3MXW & -129724.4 & -126724.2 & -228854.3 & -126604.4 & -127087.7 & -126965.9 \\
       3EO9 & -126967.6 & -126681.8 & -127066.8 & -126838.9 & -127299.4 & -127036.6 \\
       2JEL & -101452.8 & -101051.3 & -101044.3 & -100738.9 & -101482.4 & -101111.0 \\
       1VFB\_min & -74414.2 & -74294.0 & -74494.7 & -74415.6 & -74526.9 & -74517.8 \\
       1VFB & -74986.0 & -74330.7 & -74497.5 & -74441.7 & -74595.2 & -74536.9 \\
       1PPE & -49119.3 & -49119.3 & -49226.0 & -49171.3 & -49256.0 & -49256.0 \\
       1NTY & -110838.1 & -110260.9 & -110457.9 & -110222.1 & -110847.3 & -110669.8 \\
       1KXQ & -143886.8 & -132645.5 & -132951.1 & -132872.6 & -133178.7 & -133115.5 \\
       1IJK & -107683.6 & -107381.5 & -107336.6 & -107287.7 & -109706.4 & -107646.3 \\
       1I9R & -112501.7 & -112357.3 & -112660.1 & -112660.1 & -112942.5 & -112848.2 \\
       1FSK & -121052.2 & -120222.2 & -120037.7 & -119945.2 & -120533.2 & -120344.0 \\
       1E6J\_min & -122041.0 & -98979.1 & -99248.2 & -98994.0 & -99321.4 & -99260.8 \\
       1E6J & -99252.8 & -99068.8 & -99207.9 & -99133.1 & -100978.5 & -99290.5 \\
       1DQJ & -115945.9 & -110563.6 & -111170.0 & -110864.7 & -115192.8 & -111144.9 \\
       1CGI & -62803.7 & -59810.1 & -59746.3 & -59680.5 & -59927.0 & -59927.0 \\
       1BVK & -90899.6 & -72872.2 & -73015.9 & -72801.1 & -80154.7 & -73078.0 \\
       1BJ1 & -109904.1 & -104866.7 & -104420.1 & -104267.4 & -104960.2 & -104675.5 \\
       1AY7 & -40858.6 & -40730.9 & -40723.8 & -40561.6 & -40891.7 & -40739.4 \\
       1AVX & -90394.4 & -81194.0 & -80901.2 & -80891.3 & -81278.9 & -81149.8 \\
    \end{tabular}
    \end{center}
    \caption{Данные о значениях потенциальной энергии белок-белковых комплексов: лучшая энергия из всех начальных приближений ($E_{best}$, кДж/моль) и энергия структуры, близкой к нативной ($E_{native}$, кДж/моль).}
    \label{table_res_2}
  \end{table}
\fi

\FloatBarrier
