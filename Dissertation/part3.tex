\chapter{Рестарты зеркального спуска для относительно липшицевых задач с условием $\gamma$-роста}\label{ch:ch3}

\section{Введение}\label{sec:ch3/sect1}

    В предыдущих пунктах уже упоминались пессимистичные теоретические оценки скорости сходимости для негладких задач в пространствах больших размерностей. Распространенным подходом к этой проблеме служит выделение специальных классов задач, таких как условие острого минимума \cite{6, 1} и условие сильной выпуклости. Напомним определение острого минимума. Говорят, что $f$ удовлетворяет условию острого минимума, если
    \begin{gather}\label{sm}
    f(x) - f(x_*) \geq \alpha \min_{x_* \in X_*} \|x- x_*\|_2 \quad \forall x \in Q
    \end{gather}
    для некоторого фиксированного $\alpha >0$ и $f(x_*) = f^* = \min\limits_{x\in Q} f(x)$ для всякого $x_* \in X_*$, где $Q$ --- выпуклое и замкнутое подмножество $\mathbb{R}^n$, $X_*$ --- компакт и $\|\cdot\|_2$ --- евклидова норма. 

    В финале предыдущей главы было проведено сравнение 2х методов, использующих различные дополнительные условия:
    \begin{enumerate}
        \item сильной выпуклости,
        \item обобщенного условия острого минимума.
    \end{enumerate}
    На основе проведенного противопоставления указанных выше условий, в данной главе будут предложены рестарты для субградиентных методов, улучшающие полученные ранее оценки благодаря использованию аналога условия острого минимума.  Кроме того, предложена некоторая релаксация для понятия острого минимума, включающая в себя условие квадратичного роста и использующая функцию Брэгмана для оценки удаленности от решения. 
    
    Пункт \ref{sec:ch3/sect3} обобщает использованные в предыдущей главе подходы для уточнения оценок скорости сходимости для сильно выпуклых задач и вводимое условие относительного $\gamma$-роста. В рамках данного пункта получены оценки скорости сходимости по значению и по аргументу функции, улучшающие рассмотренные ранее сублинейные оценки скорости сходимости, предложенные изначально в \cite{Bach_2012}. В последующем пункте \ref{sec:ch3/sect4} предложены улучшения для полученного метода, направленные на удобство его использования. В частности проведена заменена констант, использующих расстояние от начальной точки до точного решения, на их оценки. Также предложен критерий остановки на каждом из рестартов, который может значительно сократить общее количество итераций на практике.

\section{Рестарты для метода зеркального спуска для относительных липшицевых задач оптимизации с относительным гамма-ростом}\label{sec:ch3/sect3}
    Воспользуемся следующим результатом, показанным в формуле (6) в работе \cite{Lu_2018}:
    \begin{theorem} \label{vanilla_mirror}
        Пусть $f$ --- является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$ c сильно выпуклой прокс-функцией $d(x)$. Тогда можно задать метод следующим образом:
        \begin{equation} \label{mirr_upd}
            x_{k+1} = \arg \min_{x \in Q} {\left[ f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{h_k} V(x, x_k)\right]},
        \end{equation}
        где $\{ h_k \}$ - последовательность размеров шагов.
        Для него справедлива следующая оценка скорости сходимости:
        \begin{equation} \label{general_est}
            \min_{0\leq k \leq N} f(x_k) - f(x) \leq \frac{\frac{1}{2} M^2 \sum_{k=0}^N h_k^2 + V(x, x_0)}{\sum_{k=0}^N h_k}
        \end{equation}
    \end{theorem}

    \begin{remark}
        Если в \eqref{mirr_upd} в формулировке теоремы \ref{vanilla_mirror} выбрать шаг следующим образом:
        \begin{equation} \label{mirr_step}
            h_{k} = \frac{\sqrt{2 \left[\min\limits_{x_* \in X_*}{V(x_*, x_0)}\right] }}{M\sqrt{N}},
        \end{equation}
        то можно выписать такую оценку скорости сходимости:
        \begin{equation} \label{mirr_est}
            f(\widehat{x_N}) - f(x_*) \leq \frac{M\sqrt{2 \left[\min\limits_{x_* \in X_*}{V(x_*, x_0)}\right]}}{\sqrt{N}}
        \end{equation}
    \end{remark}
    Если функция обладает дополнительными свойствами, аналогичными <<острому минимуму>>,  то становится возможным применение техники рестартов. Используем аналог данного условия и вслед за Шапиро–Немировским (см. \cite{shapiro_2005} и \cite{shapiro_2021} ) впервые введем условие относительного $\gamma$-роста ($\gamma \geq 1$). Отметим, что подобное условие впервые вводится с использованием дивергенции Брэгмана, что позволяет обобщить ряд условий, таких как условия <<острого минимума>>, <<квадратичного доминирования>> и <<$\gamma$-роста>>:
    \begin{definition}
       $f$ --- удовлетворяет условию относительного $\gamma$-роста при условии:
       \begin{equation} \label{gamma-growth}
           f(x) - f(x_*) \geq \mu_{\gamma}\left(\min_{x_* \in X_*}{V(x_*,x)}\right)^{\gamma/2} \;\;\; \forall x \in Q,
       \end{equation}
       где $X_*$ --- множество возможный решений.  
    \end{definition}
    
    В данных предположениях была сформулирована и доказана следующая теорема:
    \begin{theorem} \label{simple_restart}
        Пусть $f$ --- удовлетворяет условию относительного $\gamma$-роста \eqref{gamma-growth} и также является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$. В таком случае Алгоритм \ref{alg:rest_gamma} достигнет точности $\epsilon$ за:
        \begin{equation}
        \begin{aligned}
           N =\mathcal{O}\left(\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}}\right) \text{ при } \gamma = 1, \\
           N = \mathcal{O}\left(\frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2^{\gamma} - 2)} \left[\varepsilon^{(1 - \gamma)} - \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)}\right]\right) \text{ при } \gamma > 1,
        \end{aligned}
        \end{equation}
        обращений к субградиенту, причем будут справедливы неравенства:
        \begin{equation}
            \min_{x_* \in X_*}{V(x_*, \widehat{x_p})} \leq \varepsilon
        \end{equation}
        и
        \begin{equation}
            f(\widehat{x_p}) - f(x_*) \leq M \sqrt{2 \varepsilon}.  
        \end{equation}
    \end{theorem}

    \begin{algorithm}[htp]
        \caption{Рестарты зеркального спуска при условии относительного $\gamma$-роста.}
        \label{alg:rest_gamma}
        \KwData{$\varepsilon > 0$}
        \KwResult{$x_p$}
        $p \gets 0$\;
        $\min\limits_{x_* \in X_*}{V(x_*, x_0)} \gets \min\limits_{x_* \in X_*}{V(x_*,x_0^0)}$\;
        \While{$p < \log_2\left(\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}\right).$}{
            $x_{p}$ --- результат работы метода \eqref{mirr_upd} с шагом \eqref{mirr_step} и параметром $N_{p} = \ceil*{\frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{p(1 - \gamma)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}}$\;
            $x_0 = \widehat{x_p}$\;
            $\min\limits_{x_* \in X_*}{V(x_*, x_0)} \gets \frac{1}{2^{p+1}}\min\limits_{x_* \in X_*}{V_{0}(x_*, x_0^0)}$\;
            $p=p+1$\;
        }
    \end{algorithm}

    \begin{proof}
       Объединим в систему свойство \eqref{gamma-growth} и оценку \eqref{mirr_est}. В доказательстве используется обозначение $x_m^n$, где $m - 0...N$ соответствует номеру итерации и $n - 0...p$ соответствует номеру рестарта: 
       $$
           \mu_{\gamma}\left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{\gamma/2} \leq f(\widehat{x_N^0}) - f(x_*) \leq \frac{M\sqrt{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}}{\sqrt{N}}
       $$
       $$
           \mu_{\gamma}\left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{\gamma/2} \leq \frac{M\sqrt{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}}{\sqrt{N}}
       $$
       $$
           \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{\gamma/2} \leq \frac{M\sqrt{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}}{\mu_{\gamma}\sqrt{N}}
       $$
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})} \leq \left(\frac{M}{\mu_{\gamma}\sqrt{N}}\right)^{\frac{2}{\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\frac{1}{\gamma}}
       $$
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})} \leq \left[\min\limits_{x_* \in X_*}{V(x_*, x_0^0)} \right] \left[\frac{M}{\mu_{\gamma}\sqrt{N}}\right]^{\frac{2}{\gamma}} \left[\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right]^{\frac{1}{\gamma} - 1}
       $$
       Основываясь на полученной закономерности проведем последовательные оценки для первых нескольких запусков. Оценим необходимое количество итераций для 0 запуска:
       $$
           \left(\frac{M}{\mu_{\gamma}\sqrt{N}}\right)^{\frac{2}{\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\frac{1}{\gamma} - 1} \leq \frac{1}{2} 
       $$
       $$
           \left(\frac{M}{\mu_{\gamma}}\right)^{\frac{2}{\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\frac{1}{\gamma} - 1} \leq \frac{1}{2} N^{\frac{1}{\gamma}} 
       $$
       $$
           \left(\frac{M}{\mu_{\gamma}}\right)^{\frac{2}{\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\frac{1}{\gamma} - 1} \leq \frac{1}{2} N^{\frac{1}{\gamma}} 
       $$
       $$
           \frac{1}{2} N^{\frac{1}{\gamma}} \geq \left(\frac{M}{\mu_{\gamma}}\right)^{\frac{2}{\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\frac{1}{\gamma} - 1}  
       $$
       $$
           N \geq 2 ^ {\gamma} \frac{M^2}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}  
       $$
       $$
           N \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}  
       $$
       Аналогично рассуждая получаем для 1-го запуска $x_0^1 = \widehat{x_N^0}$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^1})} \leq \frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, x_0^1)} = \frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})} \leq \left(\frac{1}{2}\right)^2 \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}
       $$
       после:
       $$
           N_1 \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^1)}\right)^{1 - \gamma} \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{1 - \gamma} 
       $$
       при $\gamma > 1$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})} \leq \frac{1}{2}\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}  
       $$
       $$
           \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{\gamma - 1} \leq \left(\frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{\gamma - 1}
       $$
       $$
            \left(\frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} \leq \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{1 - \gamma}
       $$
       соответственно:
       $$
       \begin{aligned}
           N_1 \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^0})}\right)^{1 - \gamma} \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} =\\
           = \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{1-\gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}
       \end{aligned}
       $$
       Для 2-го запуска $x_0^2 = \widehat{x_N^1}$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^2})} \leq \frac{1}{2} \min\limits_{x_* \in X_*}{V(x_*, x_0^2)} \leq (\frac{1}{2})^3 \min\limits_{x_* \in X_*}{V(x_*, x_0^0)} 
       $$
       после:
       $$
       \begin{aligned}
           N_2 \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^2)}\right)^{1 - \gamma} = \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^1})}\right)^{1 - \gamma} \geq\\
           \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{1 - \gamma}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^1)}\right)^{1 - \gamma} \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{2(1 - \gamma)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}
       \end{aligned}
       $$
       Наконец, для $(p-1)$-го запуска:
       \begin{equation} \label{v_seq}
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^{p-1}})} \leq \frac{1}{2^p} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}
       \end{equation}
       после:
       \begin{equation} \label{n_seq}
           N_{p-1} \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 2^{(p - 1)(1 - \gamma)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}
       \end{equation}
       Используя найденную зависимость \eqref{v_seq}, проведем оценку общего числа обращений к оракулу (здесь предполагается, что $\gamma > 1$ - равенство рассмотрим далее):
       \begin{equation} \label{n_p_sum}
       \begin{aligned}
           \sum_{k=1}^{p - 1} N_k \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} (1 + 2^{(\gamma-1)} + 2^{2(\gamma - 1)} + ... + 2^{(p-1)(\gamma - 1)}) = \\
           = \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \frac{1 - 2^{(p-1)(\gamma-1)}}{1 - 2^{(\gamma-1)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma}
       \end{aligned}
       \end{equation}
       Удаленность от решения оценим, используя $\min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^{p-1}})}$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_N^{p-1}})} \leq \frac{1}{2^p} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)} \leq \varepsilon
       $$
       $$
            2^p \geq \frac{1}{\varepsilon} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}
       $$
       Откуда и получаем необходимую оценку количества рестартов:
       \begin{equation} \label{p_rest}
            p \geq \log_2{\left[\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}\right]}
       \end{equation}
       Сопоставим \eqref{p_rest} и \eqref{n_p_sum} для общей оценки количества обращений к <<оракулу>>:
       $$
           \sum_{k=1}^{p} N_k \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2} \frac{1 - 2^{p(\gamma-1)}}{1 - 2^{(\gamma-1)}} \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} \geq 
       $$
       $$
           \geq \frac{M^2 2^{\gamma}}{\mu_{\gamma}^2 (1 - 2^{(\gamma-1)})} \left(1 - \frac{\left[\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right]^{(\gamma-1)}}{\varepsilon^{(\gamma-1)}}\right) \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} =
       $$
       $$
           = \frac{2 M^2 2^{\gamma}}{\mu_{\gamma}^2 (2 - 2^{\gamma})} \left[\left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{1 - \gamma} - \frac{1}{\varepsilon^{(\gamma-1)}}\right] = 
       $$
       $$
           = \frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2 - 2^{\gamma})} \left[\left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)} - \varepsilon^{(1 - \gamma)}\right] = 
       $$
       $$
           = \frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2^{\gamma} - 2)} \left[\varepsilon^{(1 - \gamma)} - \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)}\right]
       $$
       Отдельно рассмотрим случай, когда $\gamma = 1$:
       $$
           \sum_{k=1}^{p} N_k = \ceil*{\frac{2 p M^2}{\mu_{\gamma}^2}} \geq \frac{2 M^2}{\mu_{\gamma}^2} \log_2{\left[\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}\right]}
       $$
       Таким образом получаем следующие оценки для количества итераций, необходимых чтобы достичь заданной точности:
       $$
           \mathcal{O} \left(\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}}{\varepsilon}}\right) \text{ при } \gamma = 1
       $$
       $$
           \mathcal{O} \left(\frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2^{\gamma} - 2)} \left[\varepsilon^{(1 - \gamma)} - \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)}\right]\right) \text{ при } \gamma > 1
       $$
       Соответствующая невязка по функции представляет собой следующее:
       $$
            f(\widehat{x_p}) - f(x_*) \leq  \langle \nabla f(\widehat{x_p}), \widehat{x_p} - x_* \rangle \leq M \sqrt{ 2 \left[\min_{x_* \in X_*}{V(x_*, \widehat{x_p})}\right]} \leq M \sqrt{2 \varepsilon}.  
        $$
    \end{proof}
    Также путем замены $\delta := M \sqrt{2 \varepsilon}$ в предыдущей теореме, можно сформулировать следующее замечание:
    \begin{remark}
        Пусть $f$ --- удовлетворяет условию $\gamma$-роста \eqref{gamma-growth} и также является $M$-липшицевой на $Q$ относительно некоторой дивергенции Брэгмана $V(x, y)$. В таком случае, Алгоритм \ref{alg:rest_gamma} достигнет точности $\delta$ за:
        \begin{equation}
            \begin{aligned}
               N = \mathcal{O}\left(\frac{2 M^2}{\mu_{\gamma}^2} \log_2{\frac{2 M^2 \left[\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right]}{\delta^2}}\right) \text{ при } \gamma = 1, \\
               N = \mathcal{O}\left(\frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 (2^{\gamma} - 2)} \left[\left(\frac{\delta^2}{2 M^2}\right)^{(1 - \gamma)} - \left(\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}\right)^{(1 - \gamma)}\right]\right) \text{ при } \gamma > 1,
            \end{aligned}
        \end{equation}
        обращений к субградиенту $f$, причем будут справедливы неравенства:
        \begin{equation}
           f(\widehat{x_p}) - f(x_*)  \leq \delta 
        \end{equation}
        и
        \begin{equation}
           \min\limits_{x_* \in X_*}{V(x_*, \widehat{x_p})} \leq \frac{\delta^2}{2 M^2}.
        \end{equation}
    \end{remark}

\section{Адаптивный вариант зеркального спуска для липшицевых задач с гамма-ростом}\label{sec:ch3/sect4}

    Для дальнейших рассуждений необходим адаптивный аналог оценки \eqref{general_est}. Воспользуемся результатом, полученным в лемме \ref{th:base}, а точнее в замечании \ref{remark4} к этой лемме. При помощи данного результата проведем замену глобальной константы $M$ на локальные значения в теореме \ref{vanilla_mirror}. Сформулируем и докажем следующее соответствующее данному подходу замечание:
    \begin{remark} \label{adapt_mirror}
        Пусть $f$ --- является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$ c 1-сильно выпуклой прокс-функцией $d(x)$.
        Зададим метод аналогично теореме \ref{vanilla_mirror}: 
        \begin{equation} \label{adapt_upd}
            x_{k+1} = \arg \min_{x \in Q} {\left[ f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{h_k} V(x, x_k)\right]},
        \end{equation}
        где $\{ h_k \}$ - последовательность размеров шагов. Тогда справедлива следующая оценка:
        \begin{equation} \label{adapt_est}
            \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N h_k^2 \norm{\nabla f(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)} }{\sum_{k=0}^N h_k}.
        \end{equation}
    \end{remark}

    \begin{proof}
       Пусть алгоритм \eqref{adapt_upd} отработал $N$ шагов, тогда просуммируем неравенства \eqref{base_eq} (из замечания \ref{remark4}):
       $$
       \begin{aligned}
           h_k (f(x_k) - f(x_*)) \leq h_k \langle g(x_k), x_k - x_* \rangle \leq \frac{h_k^2}{2} \norm{g(x_k)}^2 + \min\limits_{x_* \in X_*}{V(x_*, x_k)} - \\
           - \min\limits_{x_* \in X_*}{V(x_*, x_{k+1})}
       \end{aligned}
       $$
       $$
           \sum_{k=0}^N h_k (f(x_k) - f(x_*)) \leq \sum_{k=0}^N \frac{h_k^2}{2} \norm{g(x_k)}^2 + \sum_{k=0}^N (\min\limits_{x_* \in X_*}{V(x_*, x_k)} - \min\limits_{x_* \in X_*}{V(x_*, x_{k+1})})
       $$
       $$
       \begin{aligned}
           \frac{\sum_{k=0}^N h_k f(x_k)} {\sum_{k=0}^N h_k} - f(x_*) \leq & \frac{\sum_{k=0}^N h_k^2 \norm{g(x_k)}^2} {2 \sum_{k=0}^N h_k} + \\
           & + \frac{\sum_{k=0}^N \left(\min\limits_{x_* \in X_*}{V(x_*, x_k)} - \min\limits_{x_* \in X_*}{V(x_*, x_{k+1})}\right)}{\sum_{k=0}^N h_k}
       \end{aligned}
       $$
       И, наконец,
       $$
           \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N h_k^2 \norm{g(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)} - \min\limits_{x_* \in X_*}{V(x_*, x_N)} }{\sum_{k=0}^N h_k} \leq
       $$
       $$
           \leq \frac{\sum_{k=0}^N h_k^2 \norm{g(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)}}{\sum_{k=0}^N h_k}
       $$
    \end{proof}
    На практике для использования приведенной выше теоремы \ref{simple_restart} требуется знание о точном решении для оценки $\min\limits_{x_* \in X_*}{V(x_*, x_0^0)}$, что лишает данный метод практической пользы. Потому стоит провести замену расстояния Брэгмана оценкой сверху:
    \begin{equation} \label{adapt_est_theta}
    \begin{aligned}
        \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N h_k^2 \norm{\nabla f(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)} }{\sum_{k=0}^N h_k} \leq \\
        \leq \frac{\sum_{k=0}^N h_k^2 \norm{g(x_k)}^2} {2 \sum_{k=0}^N h_k} + \frac{\Theta^2}{\sum_{k=0}^N h_k}.
    \end{aligned}
    \end{equation}
    Используем данный результат и предложим следующую модификацию, использующая критерий остановки метода на каждом из рестартов.
    \begin{remark}
        Если в \eqref{adapt_upd} выбрать шаг следующим образом:
        \begin{equation} \label{eps_step}
            h_{k} = \frac{\varepsilon}{\norm{\nabla f(x_k)}^2},
        \end{equation}
        и воспользоваться критерием остановки метода на рестарте: 
        \begin{equation} \label{stop_crit}
            \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2} \geq \frac{2 \Theta^2}{\varepsilon^2}, \;\;\;\;\text{ где } \Theta \geq \min\limits_{x_* \in X_*}{V(x_*, x_0)},
        \end{equation}
        тогда для невязки по функции можно провести следующую последовательность сравнений:
        \begin{equation} 
        \begin{aligned}
            \min_{0\leq k \leq N} f(x_k) - f(x_*) \leq \frac{\sum_{k=0}^N \frac{\varepsilon^2}{\norm{\nabla f(x_k)}^4} \norm{\nabla f(x_k)}^2} {2 \sum_{k=0}^N \frac{\varepsilon}{\norm{\nabla f(x_k)}^2}} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)} }{\sum_{k=0}^N \frac{\varepsilon}{\norm{\nabla f(x_k)}^2}} = \\
            = \frac{\varepsilon} {2} \frac{ \sum_{k=0}^N \frac{1}{\norm{\nabla f(x_k)}^2}} {\sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)}}{\varepsilon \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}}  = \frac{\varepsilon}{2} + \frac{\min\limits_{x_* \in X_*}{V(x_*, x_0)} }{\varepsilon \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}} \leq \\
            \leq \frac{\varepsilon}{2} + \frac{\Theta^2}{\varepsilon \sum_{k=0}^N \frac{1} {\norm{\nabla f(x_k)}^2}} \leq \varepsilon.
        \end{aligned}
        \end{equation}
     \end{remark}
     В дальнейших рассуждениях будем обозначать 
     \[
        x_{min}^j  := \min_{0\leq k \leq N} f(x_k) \;\;\; \text{на} \;\; j\text{-м рестарте}.
     \]

     \begin{algorithm}[htp]
        \caption{Рестарты зеркального спуска при условии $\gamma$-роста с критерием остановки.}
        \label{alg:rest_criteria}
        \KwData{$\varepsilon > 0$}
        \KwResult{$x_p$}
        $p \gets 0$\;
        $\Theta_0 \geq \min\limits_{x_* \in X_*}{V(x_*,x_0^0)}$\;
        \While{$p < \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}.$}{
            $x_{p}$ --- результат работы метода \eqref{mirr_upd} с шагом \eqref{eps_step} и критерием остановки $\sum_{k=0}^{N_p} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{ 2^{(p \gamma - p + \gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} } $\;
            $x_0 = x_{min}^p$\;
            $p=p+1$\;
        }
    \end{algorithm}
    \begin{theorem}
        Пусть $f$ --- удовлетворяет условию $\gamma$-роста \eqref{gamma-growth} и также является $M$-липшицевой на $Q$ относительно некоторой функции Брэгмана $V(x, y)$. В таком случае Алгоритм \ref{alg:rest_criteria} достигнет точности $\varepsilon$ за:
        \begin{equation}
           N = \mathcal{O} \left( \frac{4 M^2}{\mu_{\gamma}^2} \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}\right) \text{ при } \gamma = 1
       \end{equation}
       или
       \begin{equation}
           N = \mathcal{O}\left( \frac{2 M^2 }{2^{\gamma - 1} - 1}\left[ \frac{2}{\mu_{\gamma}^{\frac{2}{\gamma}}}\varepsilon^{\frac{2}{\gamma} - 2} - \frac{2^{\gamma}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \right] \right) \text{ при } \gamma > 1.
       \end{equation}
    \end{theorem}

    \begin{proof}
       Оценим начальное расстояние от точного решения при помощи $\Theta_0$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, x_0^0)} \leq \Theta_0^2
       $$
       Поскольку начальный $\varepsilon_0$ является произвольным - выберем его следующим образом:
       $$
       \begin{aligned}
           \mu_{\gamma}\left(\min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)}\right)^{\gamma/2} \leq f(x_{min}^0) - f(x_*) \leq \mu_{\gamma}\left(\frac{\min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)}}{2}\right)^{\gamma/2} \leq \\
           \leq \varepsilon_0 = \mu_{\gamma}\left(\frac{\Theta_0^2}{2}\right)^{\gamma/2} = \frac{\mu_{\gamma}}{\sqrt{2^{\gamma}}}\Theta_0^{\gamma}
       \end{aligned}
       $$
       \[
           \varepsilon_0 = \frac{\mu_{\gamma}}{\sqrt{2^{\gamma}}}\Theta_0^{\gamma}
       \]
       Такой выбор $\varepsilon_0$ не нарушает требований по заданной точности $\varepsilon$. Метод выстроен так, что можно показать следующее:
       \[
           \varepsilon_0 = \varepsilon \cdot \left(\sqrt{2}\right)^{p\gamma}
       \]
       Таким образом:
       $$
           \mu_{\gamma}\left(\min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)}\right)^{\gamma/2} \leq \frac{\mu_{\gamma}}{\sqrt{2^{\gamma}}}\Theta_0^{\gamma}
       $$
       $$
           \left(\min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)}\right)^{\gamma} \leq \frac{\Theta_0^{2\gamma}}{2^{\gamma}}
       $$
       $$
           \left(\min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)}\right)^{\gamma} \leq \left(\frac{\Theta_0^2}{2}\right)^{\gamma}
       $$
       Поскольку $\gamma \geq 1$:
       $$
           \min\limits_{x_* \in X_*}{V(x_*, x_{min}^0)} \leq \frac{\Theta_0^2}{2}
       $$
       Критерий остановки:
       \begin{equation} \label{krit}
           \sum_{k=0}^{N_0} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{2 \Theta_0^2}{\varepsilon_0^2} = \frac{2 \Theta_0^2}{{\frac{\mu_{\gamma}^2 \Theta_0^{2\gamma}}{2^{\gamma}}}} = \frac{2^{\gamma + 1}}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2}}
       \end{equation}
       Если $\norm{\nabla f(x_k) }_2 \leq M$, то
       \[
           \sum_{k=0}^{N_0} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{N_0}{M^2}
       \]
       Значит критерий остановки \eqref{krit} будет заведомо выполнен при:
       \[
            \sum_{k=0}^{N_0} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{N_0}{M^2} \geq \frac{2^{\gamma + 1}}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2}}
       \]
       соответственно:
       $$
            N_0 \geq \frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2}}
       $$
       Аналогично продолжая рассуждение для второго рестарта:
       \[
           \min\limits_{x_* \in X_*}{V(x_*, x_{0}^1)} \leq \Theta_1^2
       \]
       \[
           \min\limits_{x_* \in X_*}{V(x_*, x_{0}^2)} = \min\limits_{x_* \in X_*}{V(x_*, x_{min}^1)} \leq \frac{\Theta_1^2}{2}
       \]
       и для третьего перезапуска:
       \[
           \min\limits_{x_* \in X_*}{V(x_*, x_{0}^2)} \leq \Theta_2^2
       \]
       Таким образом:
       \[
           \Theta_1 = \frac{\Theta_0}{\sqrt{2}}
       \]
       \[
           \Theta_2 = \frac{\Theta_1}{\sqrt{2}} = \frac{\Theta_0}{(\sqrt{2})^2}
       \]
       \[
           ...
       \]
       \[
           \Theta_p = \frac{\Theta_0}{(\sqrt{2})^p}
       \]
       Проведем те же рассуждения в общем виде для $p$-го рестарта:
       \[
           \varepsilon_p = \mu_{\gamma} \left(\frac{\Theta_p^2}{2}\right)^{\frac{\gamma}{2}} = \mu_{\gamma} \left(\frac{\Theta_0^2}{2^{p+1}}\right)^{\frac{\gamma}{2}}
       \]
       тогда аналогично: 
       \[
           \mu_{\gamma}\left(\min\limits_{x_* \in X_*}{V(x_*, x_{min}^p)}\right)^{\frac{\gamma}{2}} \leq f(x_{min}^p) - f(x_*) \leq \varepsilon_p
       \]
       Откуда получаем:
       \[
           \min\limits_{x_* \in X_*}{V(x_*, x_{min}^{p})} \leq \frac{\Theta_0^2}{2^{p+1}}
       \]
       Соответствующий критерий остановки:
       \[
           \sum_{k=0}^{N_p} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{2 \Theta_p^2}{\varepsilon_p^2} = \frac{2 \Theta_0^2}{{2^p \mu_{\gamma}^2 \frac{ \Theta_0^{2\gamma}}{2^{\gamma(p + 1)}}}} = \frac{1}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} 2^{(p - \gamma p - \gamma - 1)}}
       \]
       и заведомо выполнен при:
       \[
           \sum_{k=0}^{N_p} \frac{1}{\norm{\nabla f(x_k) }_2^2} \geq \frac{N_p}{M^2} \geq \frac{1}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} 2^{(p - \gamma p - \gamma - 1)}}
       \]
       \[
           N_p \geq \frac{M^2}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} 2^{(p - \gamma p - \gamma - 1)}} = \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2\gamma - 2} } 2^{p(\gamma - 1)}
       \]
       \iffalse
       Используя полученное неравенство мы получаем следующую оценку для $N_p$:
       \[
            N_p \geq \frac{2 \cdot 2^{\gamma} \cdot 2^{p\gamma} M^2}{2^p \mu_{\gamma}^2} \min\limits_{x_* \in X_*}{V(x_*, x_0^0)}^{(1 - \gamma)}
       \]
       \fi
       Используя данный подход получим следующую оценку (достаточно завышенную):
       \[
        \begin{aligned}
           \sum_{k=0}^{p} N_k \geq \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)} } (1 + 2^{(\gamma-1)} + 2^{2(\gamma - 1)} + ... + 2^{p(\gamma - 1)}) = \\
           = \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \frac{1 - 2^{p(\gamma-1)}}{1 - 2^{(\gamma-1)}}
       \end{aligned}
       \]
       Так же отдельно рассмотрим случай $\gamma = 1$:
       \[
           \sum_{k=0}^{p} N_k = \ceil*{\frac{4 p M^2}{\mu_{\gamma}^2}}
       \]
       Для финальной оценки необходимо получить оценку для количества рестартов $p$, обозначим $\varepsilon := \varepsilon_p$ - то есть финальную, необходимую точность:
       \[
           \varepsilon = \varepsilon_p = \mu_{\gamma} \left(\frac{\Theta_0^2}{2^{p+1}}\right)^{\frac{\gamma}{2}}
       \]
       \[
           \left(\frac{\varepsilon}{\mu_{\gamma}}\right)^{\frac{2}{\gamma}} =  \frac{\Theta_0^2}{2^{p+1}}
       \]
       \[
            2^p =  \left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}
       \]
       \[
            p \geq \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}
       \]
       Соответственно при $\gamma > 1$:
       \[
       \begin{aligned}
           \sum_{k=0}^{p} N_k \geq & \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \frac{1 - 2^{p(\gamma-1)}}{1 - 2^{(\gamma-1)}} \geq \\
           \geq& \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)} (1 - 2^{(\gamma-1)})} \left[1 - \left(\left[\frac{\mu_{\gamma}}{\varepsilon}\right]^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right) ^{(\gamma-1)}\right] = \\
           =& \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 (1 - 2^{(\gamma-1)}) \Theta_0^{2(\gamma - 1)}}  - \frac{M^2 2^{(\gamma + 1)}}{\mu_{\gamma}^2 (1 - 2^{(\gamma-1)}) \Theta_0^{2(\gamma - 1)}} \left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2\gamma - 2}{\gamma}} \frac{\Theta_0^{2\gamma - 2}}{2^{\gamma - 1}} = \\ 
           =& \frac{4 M^2 2^{\gamma}}{\mu_{\gamma}^2 (2 - 2^{\gamma}) \Theta_0^{2(\gamma - 1)}}  - \frac{4 M^2 \cancel{2^{\gamma}}}{\cancel{\mu_{\gamma}^2} (2 - 2^{\gamma}) \cancel{\Theta_0^{2(\gamma - 1)}}} \frac{\cancel{\mu_{\gamma}^2} \cdot \mu_{\gamma}^{- \frac{2}{\gamma}}}{\varepsilon^2 \cdot \varepsilon^{- \frac{2}{\gamma}}} \frac{2 \cancel{\Theta_0^{2\gamma - 2}}}{\cancel{2^{\gamma}}} = \\
           =& \frac{4 M^2 }{2 - 2^{\gamma}}\left( \frac{2^{\gamma}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} - \frac{2 \varepsilon^{\frac{2}{\gamma}}}{\mu_{\gamma}^{\frac{2}{\gamma}} \varepsilon^2} \right) = \frac{2 M^2 }{2^{\gamma - 1} - 1}\left( \frac{2}{\mu_{\gamma}^{\frac{2}{\gamma}}}\varepsilon^{\frac{2}{\gamma} - 2} - \frac{2^{\gamma}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \right)
       \end{aligned}
       \]
       И при $\gamma = 1$:
       \[
           \sum_{k=0}^{p} N_k = \ceil*{\frac{4 p M^2}{\mu_{\gamma}^2}} \geq \frac{4 M^2}{\mu_{\gamma}^2} \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}
       \]
       Таким образом получаем следующие оценки общего количества обращений к оракулу:
       \[
           \mathcal{O} \left( \frac{4 M^2}{\mu_{\gamma}^2} \log_2{\left[\left(\frac{\mu_{\gamma}}{\varepsilon}\right)^{\frac{2}{\gamma}} \frac{\Theta_0^2}{2}\right]}\right) \text{ при } \gamma = 1
       \]
       \[
           \mathcal{O} \left( \frac{2 M^2 }{2^{\gamma - 1} - 1}\left[ \frac{2}{\mu_{\gamma}^{\frac{2}{\gamma}}}\varepsilon^{\frac{2}{\gamma} - 2} - \frac{2^{\gamma}}{\mu_{\gamma}^2 \Theta_0^{2(\gamma - 1)}} \right] \right) \text{ при } \gamma > 1
       \]
       
       \iffalse
           Соответственно при $\gamma > 1$, степень $1 - \gamma$ является отрицательной, потому справедливо следующее изменение оценки \eqref{eq:v_sup}:
           \begin{equation}
               V(x_*, x_{min}^p)^{(1 - \gamma)} \geq (\frac{V(x_*, x_0^p)}{2})^{(1 - \gamma)} \geq \frac{V(x_*, x_0^0)^{(1 - \gamma)}}{2^{p(1 - \gamma)} 2^{(1 - \gamma)}}
           \end{equation}
           то есть: 
           \begin{equation}
               V(x_*, x_0^p)^{(1 - \gamma)} \geq \frac{V(x_*, x_0^0)^{(1 - \gamma)} 2^{(1-\gamma)}}{2^{p(1 - \gamma)} 2^{(1 - \gamma)}} = \frac{V(x_*, x_0^0)^{(1 - \gamma)}}{2^{p(1 - \gamma)}}
           \end{equation}
           Используя полученное неравенство мы получаем следующую оценку для $N_p$:
           \begin{equation}
                N_p \geq \frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2} V(x_*, x_0^p)^{(1 - \gamma)} \geq  \frac{2^{\gamma + 1} M^2}{\mu_{\gamma}^2} \frac{V(x_*, x_0^0)^{(1 - \gamma)}}{2^{p(1 - \gamma)}} = \frac{2 \cdot 2^{\gamma} \cdot 2^{p\gamma} M^2}{2^p \mu_{\gamma}^2} V(x_*, x_0^0)^{(1 - \gamma)}
           \end{equation}
           Используя данный подход получим следующую оценку (достаточно завышенную):
           $$
               \sum_{k=1}^{p} N_k \geq \frac{2 \cdot 2^{\gamma} M^2}{\mu_{\gamma}^2} V(x_*, x_0^0)^{(1 - \gamma)} (1 + 2^{(\gamma-1)} + 2^{2(\gamma - 1)} + ... + 2^{p(\gamma - 1)}) = \frac{2 \cdot 2^{\gamma} M^2}{\mu_{\gamma}^2} \frac{1 - 2^{p(\gamma-1)}}{1 - 2^{(\gamma-1)}} (V(x_*, x_0^0))^{1 - \gamma}
           $$
           Так же отдельно рассмотрим случай $\gamma = 1$:
           $$
               \sum_{k=1}^{p} N_k = \frac{4 p M^2}{\mu_{\gamma}^2} 
           $$
           Для финальной оценки необходимо получить оценку для количества рестартов $p$, обозначим $\varepsilon := \varepsilon_p$ - то есть финальную, необходимую точность:
           \begin{equation}
               \varepsilon = \varepsilon_p = \mu_{\gamma} (\frac{V(x_*, x_0^p)}{2})^{\frac{\gamma}{2}} \leq  \mu_{\gamma} (\frac{V(x_*, x_0^0)}{2^{p+1}})^{\frac{\gamma}{2}}
           \end{equation}
           \begin{equation}
               (\frac{\varepsilon}{\mu_{\gamma}})^{\frac{2}{\gamma}} \leq  \frac{V(x_*, x_0^0)}{2 \cdot 2^p}
           \end{equation}
           \begin{equation}
               (\frac{\varepsilon}{\mu_{\gamma}})^{\frac{2}{\gamma}} \leq  \frac{V(x_*, x_0^0)}{2 \cdot 2^p}
           \end{equation}
           Критерий?:
           \begin{equation}
               V(x_*, x_0^p) \leq \frac{1}{2} \sum_{k=0}^N \frac{\varepsilon_p^2}{\norm{\nabla f(x_k) }_2^2} \leq \frac{\varepsilon_p^2}{2} \frac{N_p}{M^2}
           \end{equation}
       \fi
    \end{proof}

    Полученный результат имеет схожие оценки с предыдущей теоремой \ref{simple_restart}, однако на практике критерий остановки значительно сокращает количество итераций, необходимых для достижения заданной точности $\varepsilon$. Также при помощи данного метода легко прогнозируется точность перед каждым рестартом метода, что удобно для контроля за правильностью работы метода. Также подобная информация позволяет переключаться между несколькими модификациями метода, чередуя не только <<эффективные>> и <<аналитические>> шаги, но и перезапуски.  
\FloatBarrier