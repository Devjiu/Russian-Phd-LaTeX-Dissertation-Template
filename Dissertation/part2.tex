\chapter{Методы типа зеркального спyска для относительно липшицевых и относительно сильно выпyклых задач}\label{ch:ch2}

\section{Введение}\label{sec:ch2/sec1}

    Численные методы градиентного типа достаточно часто используются для самых разнообразных постановок задач выпуклой оптимизации в пространствах больших размерностей. Это объясняется небольшими затратами памяти на итерациях, а также возможностью обоснования приемлемых оценок скорости сходимости, не содержащих (в отличие, например, от методов отсекающей гиперплоскости) параметров размерности пространства. Однако при этом существенны предположения о функциональных свойствах таких задач (гладкость, липшицевость, сильная выпуклость). Так, несколько лет назад был выделен класс относительно гладких задач оптимизации (см., например \cite{Bauschke,Drag,Lu_Nesterov_2018}). Свойство относительной $L$-гладкости ($L > 0$) обобщает ycловие $L$-гладкости ($L$-липшицевости градиента)  $f$ путём замены в известном для $L$-гладкий фyнкций $f$ неравенстве ($Q$ --- область определения $f$)
    $$
    	f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{L}{2} \|x - y \|_2^2 \quad   \forall x, y \in Q
    $$	
    выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией (расхождением) Брэгмана (см. \eqref{Brg_form} и \eqref{funct_rel_smooth} ниже), которая порождается некоторой выпуклой прокс-функцией (важно, что она не обязательно сильно выпукла). Отметим, что здесь и всюду далее $\|\cdot\|_2$ --- евклидова норма в $n$-мерном пространстве $\mathbb{R}^n$.

    Для выпуклых относительно гладких задач оценки сходимости обычных (неускоренных) методов градиентного типа оптимальны с точностью до умножения на константу, не зависящую от размерности и параметров метода (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также имеющиеся в них ссылки). В работе \cite{Lu_Nesterov_2018} введено понятие относительной сильной выпуклости функции, которое позволило расширить класс выпуклых оптимизационных задач, для которых можно доказать линейную скорость сходимости (сходимость со скоростью геометрической прогрессии) метода градиентного типа, причём соответствующая оценка не содержит параметров размерности задачи. В данной главе мы развиваем этот подход и исследуем некоторые алгоритмы уже для вариационных неравенств с аналогом относительной сильной выпуклости для операторов (относительной сильной монотонностью). Напомним, что понятие относительной сильной выпуклости \cite{Lu_Nesterov_2018} функции $f$ обобщает понятие обычной $\mu$-сильной выпуклости $f$ ($\mu > 0$) путём замены в неравенстве 
    \begin{equation}
    	f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{\mu}{2} \|x - y \|_2^2 \leq f(y) \quad   \forall x, y \in Q,
    	\end{equation}
    выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией Брэгмана (см. \eqref{Brg_form} и \eqref{eqrelativestorngconv} ниже), которая порождается некоторой выпуклой прокс-функцией. 

    В настоящей главе рассматриваются методы первого порядка для двух классов вариационных неравенств с операторами, удовлетворяющими предлагаемому аналогу условия относительной сильной выпуклости (см. ниже определение  \ref{DefRelStrongMonot} относительной сильной монотонности оператора): с аналогом ограниченности (относительная ограниченность, см. определение 2 ниже), а также с аналогом условия Липшица (относительная гладкость, см. определение 3 ниже). Проводится экспериментальное сравнение и анализ методов, использующих $\mu$-сильную выпуклость и острую выпуклость. Данное сравнение получает свое развитие в следующей главе, где используется для построения механизма рестартов. 

    Хорошо известно, что на классе липшицевых и сильно выпуклых минимизационных задач оптимальная оценка скорости сходимости достигается именно для субградиентного метода \cite{Bach_2012}. В последние годы активно исследуются задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Мы исследуем оценку скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее говоря, рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач. 

    Далее, немалую популярность в работах по оптимизации получило упомянутое выше недавно предложенное понятие относительной гладкости функций (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также приведённые в них ссылки), которое позволило существенно расширить класс задач выпуклой оптимизации по сравнению со стандартным предположением о липшицевости градиента с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$ (здесь и далее $N$ --- количество итераций), которая может считаться оптимальной для такого широкого класса задач \cite{Dragomir}. В плане приложений можно отметить подход к построению методов градиентного типа для задач распределенной оптимизации с использованием относительной гладкости и относительной сильной выпуклости \cite{Hendr}. Аналоги относительной гладкости введены в последние пару лет и для более общей постановки задачи решения вариационного неравенства (см. \cite{Inex}, а также имеющиеся там ссылки) с монотонным оператором. Оказывается, что для этого класса задач можно предложить алгоритмы экстраградиентного типа с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$. 

    В данной главе в большинстве пунктов рассматривается задача нахождения решения $x_*$ (также называемого слабым решением) вариационного неравенства: 
    \begin{equation}\label{eq:1}
        \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
    \end{equation}
    где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$,
    $g: Q \longrightarrow \mathbb{R}^n$. Предположим, что удовлетворяющее \eqref{eq:1} решение $x_*$ существует.

    Всюду далее будем предполагать, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая расстояние, а также соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
    \begin{equation}\label{Brg_form}
        V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
    \end{equation}

    Введём следующий аналог понятия относительной сильной выпуклости функции \cite{Lu_Nesterov_2018} для вариационных неравенств.
    \begin{definition}\label{DefRelStrongMonot}
        Назовём оператор $g$ относительно $\mu$-сильно монотонным, где $\mu >0$, если для всяких $x, y \in Q$ верно неравенство
            \begin{equation}\label{eq:3}
                 \mu V(y, x) + \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
             \end{equation}
    \end{definition}
    Как правило, далее мы будем использовать следующее неравенство, естественно вытекающее из \eqref{eq:3}.
    \begin{remark}
        Если оператор $g$ является  относительно $\mu$-сильно монотонным, то для всяких $x, y \in Q$ верно неравенство
        $$
            \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
        $$
    \end{remark}

    Поясним на примере, почему относительная сильная монотонность вводится именно согласно  \eqref{eq:3}.
    \begin{example}
        Если $f: Q \longrightarrow \mathbb{R}$ --- относительно $\mu$-сильно выпуклая функция
        \begin{equation}\label{eqrelativestorngconv}
            f(x) - f(y) + \mu V(x, y) \leq \langle \nabla{f(x)}, x - y \rangle \quad   \forall x, y \in Q,
        \end{equation}
        то
        \begin{equation}
            f(y) - f(x) + \mu V(y, x) \leq \langle \nabla{f(y)}, y - x \rangle \quad   \forall x, y \in Q.
        \end{equation}
        После сложения двух последних неравенств получаем
        \begin{align*}
            \mu V(x, y) + \mu V(y, x)\leq \langle \nabla{f(y)} - \nabla{f(x)}, y - x \rangle \quad  \forall x, y \in Q.
        \end{align*}
        Таким образом, неравенство \eqref{eq:3} верно при $g(x) = \nabla{f(x)}$, где $\nabla{f(x)}$ --- произвольный субградиент $f$.
    \end{example}

    Относительно сильно выпуклые функционалы возникают в самых разных ситуациях, среди которых мы упомянем задачу централизованной распределённой минимизации эмпирического риска в предположении схожести слагаемых \cite{Hendr}.

    \begin{definition}\label{DefRelBound}\cite{Main}
        Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $M$-ограниченным, где $M >0$, если для всяких $x, y \in Q$ верно неравенство
        \begin{equation}\label{rel_bound}
             \langle g(x), x - y \rangle \leq M\sqrt{2V(y,x)}.
         \end{equation}
    \end{definition}
    \begin{definition}\cite{Inex}
        \fixme{[FIXME] Мне кажется тоже нужно, но может стоит убрать} Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $L$-гладким, где $L > 0$, если для всяких $x, y, z \in Q$ верно неравенство
        \begin{equation}\label{rel_smooth}
            \langle g(y)-g(z),x-z\rangle \leq LV(x,z) + LV(z,y).
        \end{equation}
    \end{definition}
    Отметим, что если функция $f$ $L$-относительно гладкая \cite{Bauschke}, т.е.
    \begin{equation}\label{funct_rel_smooth}
        f(y) \leq f(x) + \langle \nabla f(x), y - x\rangle + LV(y, x) \quad \forall x, y \in Q,
    \end{equation}
    то оператор $g(x) = \nabla f(x)$ удовлетворяет \eqref{rel_smooth}. Однако в случае непотенциального оператора $g$ условие \eqref{rel_smooth} не сводится, вообще говоря, к \eqref{funct_rel_smooth} для какой-нибудь функции $f$.

    Приведем еще примеры задач оптимизации с относительно сильно выпуклыми функционалами. Начнём с примера относительно гладкого и относительно сильно выпуклого функционала.

    \begin{example} (\cite{Lu_Nesterov_2018}) Пусть $\widehat{f}(x):=\frac{1}{4}\|E x\|_{2}^{4}+\frac{1}{4}\|A x-b\|_{4}^{4}+$ $\frac{1}{2}\|C x-d\|_{2}^{2}$, где $A, C$ и $E$ --- положительно определённые квадратные матрицы $n \times n$, а $b, d$ --- векторы размерности $n$. Можно показать, что $\widehat{f}$ --- относительно $L$-гладкая и $\mu$-относительно сильно выпукла относительно
        $
            d(x):=\frac{1}{4}\|x\|_{2}^{4}+\frac{1}{2}\|x\|_{2}^{2}
        $
        на множестве $Q=\mathbb{R}^{n}$, где $L=3\|E\|^{4}+3\|A\|^{4}+6\|A\|^{3}\|b\|_{2}+3\|A\|^{2}\|b\|_{2}^{2}+\|C\|^{2}$ и $\mu=\min \left\{\frac{\sigma_{E}^{4}}{3}, \sigma_{C}^{2}\right\}$. 
         $\sigma_{E}$ и $\sigma_{C}$ --- наименьшие собственные значения матриц $E$ и $C$.
    \end{example}

    Теперь приведём пример относительно липшицева и относительно сильно выпуклого функционала.

    \begin{example} (\cite{Zhou_NIPS_2020}) 
        Пусть $\widehat{f}(x) := \frac{1}{p} \|x\|_2^p$ для $p \geq 2$ и $ Q = [-\alpha, \alpha]^n, \; \alpha > 0$. Заметим, что $\nabla \widehat{f}(x) = \|x\|_2^{p - 2} x$ и $\nabla^2 \widehat{f}(x) = \|x\|_2^{p - 2} I + (p-2)\|x\|_2^{p - 4} x x^{T}$. Тогда $\widehat{f}$ является относительно $M$-липшицевой при $M = 1$ относительно $d(x) := \frac{1}{2p}\|x\|_2^{2p}$. При этом $\widehat{f}$ не сильно выпукла в обычном смысле, т.к. $\nabla^2 \widehat{f}(x) - \mu I$ ($I$ --- матрица с единицами на главной диагонали) не является положительно полуопределённой в окрестности 0 ни при каком $\mu >0$. Тем не менее, при 
        \begin{equation}\label{eq_mu}
            \mu = \frac{p-1}{(2p - 1)(\sqrt{n}\alpha)^p}  
        \end{equation}
        матрица $\nabla^2 \widehat{f}(x) - \mu \nabla^2 d(x)$ уже будет  положительно полуопределена, что означает относительную сильную выпуклость $\widehat{f}$.
        \label{ex_experiments}
    \end{example}

    Данная глава посвящёна модификации метода зеркального спуска и выводу оценки его скорости сходимости для вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. В частности, полученная оценка указывает на оптимальность такого метода на выделенном классе вариационных неравенств, поскольку она оптимальна (с точностью до умножения на не зависящую от параметров метода и размерности пространства константу) даже на более узком классе задач минимизации относительно липшицевых и относительно сильно выпуклых функций \cite{Lu_2018}. В пункте \ref{sec:ch2/sec2} рассматривается класс относительно сильно монотонных и относительной гладких операторов. В пункте \ref{sec:ch2/sec3} показывается, как предложенные ранее алгоритмы для вариационных неравенств и полученные теоретические оценки их скорости сходимости могут быть применены для решения относительно сильно выпукло-вогнутых седловых задач с соответствующими предположениями о гладкости функционалов. В последнем пункте производится сравнительный анализ методов, использующих различные свойства функции, а именно сильную выпуклость и острый минимум, что находит продолжение в следующей главе при организации схемы рестартов. 

\section{Субградиентный метод для вариационных неравенств с относительно сильно монотонными и ограниченными операторами}\label{sec:ch2/sec2}

    Вслед за \cite{Bach_2012} предложим метод зеркального спуска \eqref{eq:4}, но уже для рассматриваемого в настоящей работе класса  вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами (определения \ref{DefRelStrongMonot} и \ref{DefRelBound}):
    \begin{equation} \label{eq:4}
        x_{k+1} := \arg \min_{x \in Q} \left\{ h_k \langle g(x_k), x \rangle + V(x, x_k)\right\},
    \end{equation}
    где
    $$
        h_k = \frac{2}{\mu(k+1)},\quad  \forall k= 0,1, 2, \ldots.
    $$

    Непосредственно можно проверить следующий вспомогательный результат для шага метода зеркального спуска \eqref{eq:4}.

    \begin{lemma}\label{th:base}
        Если для $g$ верно \eqref{rel_bound}, а $x_k$ и $x_{k+1}$ удовлетворяют \eqref{eq:4}, то для произвольного $x \in Q$ верно неравенство
        $$    
            h_k \langle g(x_k), x_k - x \rangle \leq \frac{h_k^2 M^2}{2} + V(x, x_k) - V(x, x_{k+1}).
        $$
    \end{lemma}
    \begin{proof}
        Если применить стандартное условие экстремума 1-го порядка ко вспомогательной минимизационной подзадаче \eqref{eq:4}, то можно проверить при $h_k >0$ для всякого $x\in Q$ справедливость неравенств
        $$
            h_k \langle g(x_k), x_k - x \rangle \leq h_k \langle g(x_k), x_k - x_{k+1} \rangle  + V(x, x_k) - V(x, x_{k+1}) -V(x_{k+1},x_k) \stackrel{\eqref{rel_bound}}{\leq}$$
        $$
            \leq h_kM\sqrt{2V(x_{k+1},x_k)}+ V(x, x_k) - V(x, x_{k+1}) -V(x_{k+1},x_k) \leq
        $$
        $$
            \leq \frac{h_k^2M^2}{2} + V(x, x_k) - V(x, x_{k+1}).
        $$%\qed
    \end{proof}
    Согласно лемме \ref{th:base}, получим, что при всяких $ k \geq 0$ и $x \in Q$ верно
    \begin{equation} 
        \langle g(x_k), x_k - x \rangle \leq \frac{h_k M^2}{2} + \frac{V(x, x_k)}{h_k} - \frac{V(x, x_{k+1})}{h_k}. 
    \end{equation}
    Далее, с учетом \eqref{eq:3}, получим 
    \begin{equation*}
        \langle g(x_k), x_k - x \rangle \geq  \langle g(x), x_k - x \rangle + \mu (V(x, x_k) + V(x_k, x)) \quad \forall x \in Q,
    \end{equation*}
    откуда при всяком $k \ge 0$ имеем:
    \[
    \begin{aligned} 
        2k\langle g(x), x_k - x \rangle +  2k\mu \left[V(x, x_k) + V(x_k, x)\right] &\leq  
        \frac{2k M^2}{\mu (k+1)} + \mu k (k+1)V(x, x_k) -  \\&
        - \mu k (k+1)V(x, x_{k+1}) \quad \forall x \in Q. 
    \end{aligned}
    \]
    Это означает, что
    \begin{equation}\label{eq:5}
    \begin{aligned} 
        2k\langle g(x), x_k - x \rangle +  2k\mu V(x_k, x) \leq   
        &\frac{2k M^2}{\mu (k+1)} + \mu k (k-1)V(x, x_k) -  \\& -
        \mu k (k+1)V(x, x_{k+1}) \quad  \forall x \in Q. 
    \end{aligned}
    \end{equation}
    Пусть алгоритм \eqref{eq:4} отработал $N$ шагов. Тогда можно просуммировать неравенства \eqref{eq:5} по $k$ от $1$ до $N$ и учесть, что $\frac{k}{k+1} \le 1$:
    \begin{equation}
        \sum_{k=1}^{N} \left[ 2k\langle g(x), x_k - x \rangle + \mu V(x_k, x) \right] \leq \frac{2NM^2}{\mu},
    \end{equation}
    откуда с учетом $2(1+2+...+N)=N(N+1)$:
    \begin{equation} \label{eq:122}
        \sum_{k=1}^{N} \left[ \frac{2k}{N(N+1)}\langle g(x), x_k - x \rangle + \mu V(x_k, x) \right] \leq \frac{2M^2}{\mu(N+1)} \quad \forall x \in Q.
    \end{equation}
    %откуда 
    %\begin{equation}\label{eq:122}
    %\sum_{k=1}^{N} \frac{2k}{N(N+1)}(\langle g(x), x_k - x \rangle) + \mu V(x_k, x)) 
    %\leq \frac{2M^2}{\mu(N+1)} \quad \forall x \in Q.
    %\end{equation}
    Если учесть, что $V(x_k, x) \geq 0 \quad \forall x \in Q, \; \forall k \ge 0$,
    то при
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k
    $$
    будет верно неравенство 
    \begin{equation} \label{eq:13}
        \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2 M^2}{\mu (N+1)} \leq \varepsilon,
    \end{equation}
    после $N = O\left(\frac{M^2}{\mu \varepsilon}\right)$ итераций алгоритма $\eqref{eq:4}$. Как известно, такая оценка сложности оптимальна даже на классе относительно липшицевых и относительно сильно выпуклых задач минимизации \cite{Lu_2018}. Это указывает на её оптимальность и для существенно более широкого класса относительно липшицевых и относительно сильно выпуклых задач минимизации, а значит и для рассматриваемого класса вариационных неравенств. 
    Таким образом, можно сформулировать следующий результат
    \begin{theorem}\label{thm_MD_VI}
        Пусть $g$ --- $\mu$-относительно сильно монотонный и $M$-относительно ограниченный оператор. Тогда после $N$ итераций алгоритма: 
        $$ 
            x_{k+1} := \arg \min_{x \in Q} \{ h_k \langle g(x_k), x\rangle + V(x, x_k)\}, \;\;\; h_k = \frac{2}{\mu (k+1)}
        $$
        будет верно неравенство:
        \begin{equation}\label{eq:2}
            \max_{x \in Q} \langle g(x), \widehat{x} - x\rangle \leq \frac{2 M^2}{\mu (N+1)},
        \end{equation}
        где 
        $$
            \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
        $$
    \end{theorem}

    \begin{remark}
        Если $x_*$ --- сильное решение рассматриваемого вариационного не\-равенства, то можно выписать оценку скорости сходимости и <<по аргументу>>, поскольку $\langle g(x_*), x_k - x_*\rangle \geq 0$. Тогда \eqref{eq:122} означает, что 
            \begin{equation} \label{eq:12}
            \begin{aligned} 
                \sum_{k=1}^{N} \frac{2k\mu V(x_k, x_*)}{N(N+1)} \leq \frac{2M^2}{\mu(N+1)} \quad  \forall x \in Q,
            \end{aligned}
            \end{equation}
        Если же прокс-функция $1$-сильно выпукла относительно нормы $\|\cdot\|$, то из \eqref{eq:12} вытекает следующая оценка:
            \begin{equation} 
            \begin{aligned} 
                \|x_* - x_k\|^2 \leq \frac{4M^2}{\mu(N+1)}.
            \end{aligned}
            \end{equation}
    \end{remark}
    \begin{remark} \label{remark4}
        Если прокс-функция $d$ является $1$-сильно выпуклой, то неравенство из леммы 1 можно уточнить:
        \begin{equation} \label{base_eq}
            h_k \langle g(x_k), x_k - x \rangle \leq \frac{h_k^2 \|g(x_k)\|_*^2}{2} + V(x, x_k) - V(x, x_{k+1}). 
        \end{equation}
        Тогда итоговая оценка \eqref{eq:13} приобретает следующий вид:
        \begin{equation}
            \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|g(x_k)\|_*^2}{k+1} \leq \varepsilon.
        \end{equation}
        Правая часть предыдущего неравенства может оказаться существенно меньшей, чем для оценки \eqref{eq:2}.
    \end{remark}

\section{Оценки скорости сходимости для относительно сильно выпукло-вогнутых седловых задач} \label{sec:ch2/sec3}

    Хорошо известно, что вариационные неравенства с монотонными операторами естественно возникают при рассмотрении выпукло-вогнутых седловых задач, важных для самых разных прикладных проблем. Поэтому в данном пункте будет показано, как полученные в предыдущих пунктах результаты о методах для вариационных неравенств можно применить к седловым задачам вида
    \begin{equation}\label{eqsedlo}
        f^* = \min_{u \in Q_1} \max_{v \in Q_2} f(u, v),
    \end{equation}
    где $f$ --- относительно сильно выпукла по $u$ и относительно сильно вогнута по $v$.

    Как известно, необходимость решения вариационных неравенств мотивируется, в частности, как раз задачами вида \eqref{eqsedlo}. В качестве примера можно рассмотреть лагранжеву седловую задачу, порожденную задачей относительно сильно выпуклого программирования.  
        \begin{example} Рассмотрим задачу относительно сильно выпуклого программирования (все функционалы $\widehat{f}, g_1, g_2, ...$ относительно сильно выпуклы):
        \begin{equation}\label{problem_with_fun_constraints}
            \left\{\begin{array}{c}
            \min_{x \in Q} \widehat{f}(x), \\
            g_{1}(x), g_{2}(x), \ldots, g_{m}(x) \leq 0.
            \end{array}\right.
        \end{equation}
            
        Рассмотрим соответствующую \eqref{problem_with_fun_constraints} лагранжеву седловую задачу следующего вида
        \begin{equation}\label{lagrange_problem}
            \min_{x \in Q} \max_{ \boldsymbol{\lambda}= (\lambda_1, \ldots, \lambda_m)^T \in \mathbb{R}_+^m} L(x, \boldsymbol{\lambda}) :=  \widehat{f}(x) + \sum_{p=1}^{m} \lambda_p g_p(x) - \varepsilon \sum_{p=1}^m \lambda_{p}^2.
        \end{equation}
    \end{example}
    Для данного типа задач можно ввести такой аналог дивергенции Брэгмана \cite{Fedor_relative_adapuniv}:
    $$
        V_{\text{new}}\left((y, \boldsymbol{\lambda}), (x, \boldsymbol{\lambda}^{'})\right) = V(y,x) + \frac{1}{2} \left\|\boldsymbol{\lambda} - \boldsymbol{\lambda}^{'}\right\|_2^2, \quad  \forall y, x \in Q, \boldsymbol{\lambda},  \boldsymbol{\lambda}^{'} \in \mathbb{R}_+^m.
    $$
    введенная таким образом дивергенция позволяет ослабить требования к ограничениям $\boldsymbol{\lambda}$.

    Перейдём теперь к методике для нахождения приближённого решения задачи \eqref{eqsedlo}. Для всякого $\varepsilon > 0$ под $\varepsilon$-точным решением задачи \eqref{eqsedlo} будем понимать пару $(\widehat{u}, \widehat{v})$ такую, что $$\max_{v \in Q_2} f(\widehat{u}, v) - \min_{u \in Q_1} f(u, \widehat{v}) \leq \varepsilon.$$ Обозначим $x = (u, v), y = (z, t)$, а также введем оператор 
    \begin{equation}\label{operator-sedlo}
        g(x) := \Bigg( 
        \begin{aligned}
            f^{'}_{u}(u,v)\\
            -f^{'}_{v}(u,v)
        \end{aligned}
        \Bigg).
    \end{equation}
    Тогда ввиду выпукло-вогнутости $f$ имеем: 
    \begin{equation}
    \begin{aligned}
        \langle g(x), x - y \rangle &=
         \Bigg( 
        \begin{aligned}
            f^{'}_{u}(u,v)\\
            -f^{'}_{v}(u,v)
        \end{aligned}
        \Bigg)
         (u - z, v - t)  = \langle f^{'}_{u}(u,v), u - z \rangle - \langle f^{'}_{v}(u,v), v - t \rangle \geq \\&
         \geq f(u, v) - f(z, v) 
        - f(u, v)+ f(u, t)=  f(u,t) - f(z, v).
    \end{aligned}
    \end{equation}
    Будем предполагать относительную ограниченность оператора \eqref{operator-sedlo}. Тогда метод \eqref{eq:4} для задач \eqref{eqsedlo} приводит к оценкам вида:
    \begin{equation} \label{eq:21}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} \langle g(x_k), x_k -x\rangle \leq \frac{2 M^2}{\mu (N+1)}.
    \end{equation}
    Если $x_k = (u_k, v_k), \;\; x = (u, v)$, то  
    \begin{equation}
        \langle g(x_k), x_k -x\rangle \geq f(u_k,v) - f(u, v_k) \quad \forall (u, v).
    \end{equation}
    Далее, \eqref{eq:21} означает, что 
    \begin{equation}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k,v) - f(u, v_k)) \leq \frac{2M^2}{\mu (N+1)}.
    \end{equation}
    Положим
    \begin{equation}
        (\widehat{u}, \widehat{v}) := \frac{1}{N(N+1)} \sum_{k=1}^{N} 2k (u_k,v_k).
    \end{equation}
    Тогда получаем, что
    \begin{equation}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k, v) - f(u, v_k)) \geq f(\widehat{u}, v) - f(u, \widehat{v}), 
    \end{equation}
    откуда ввиду \eqref{eq:2} получаем для задачи \eqref{eqsedlo} следующую оценку
    \begin{equation}
        \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)}.
    \end{equation}
    Сформулируем данный результат в виде теоремы:
    \begin{theorem}
        Пусть $f$ --- относительно сильно выпукла по $u$ и относительно сильно вогнута по $v$, а соответствующий оператор $g$, определенный как
        \[
            g(x) := \Bigg( 
                \begin{aligned}
                    f^{'}_{u}(u,v)\\
                    -f^{'}_{v}(u,v)
                \end{aligned}
            \Bigg), \text{ где } x := (u, v),
        \]
        является M-ограниченным. Тогда для задачи \eqref{eqsedlo}, решаемой методом \eqref{eq:4}, после $N$ итераций будет справедливо следующее неравенство:
        \begin{equation}
            \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)},
        \end{equation}
        где
        \[
            (\widehat{u}, \widehat{v}) := \frac{1}{N(N+1)} \sum_{k=1}^{N} 2k (u_k,v_k).
        \]
    \end{theorem}

\section{Адаптивная оценка скорости сходимости одного субградиентного метода для сильно выпуклых задач и вычислительные эксперименты для задачи о наименьшем покрывающем шаре}  \label{sec:ch2/sec4}

    Широко известно, что негладкие оптимизационные задачи обладают пессимистичными теоретическими оценками скорости сходимости в пространствах больших размерностей. Одним из подходов к этой проблеме может служить выделение специального класса задач с условием острого минимума \cite{6, 1} и дальнейшее развитие данного условия. Говорят, что $f$ удовлетворяет условию острого минимума, если
    \begin{gather}\label{sm}
    f(x) - f(x_*) \geq \alpha \min_{x_* \in X_*} \|x- x_*\|_2 \quad \forall x \in Q
    \end{gather}
    для некоторого фиксированного $\alpha >0$ и $f(x_*) = f^* = \min\limits_{x\in Q} f(x)$ для всякого $x_* \in X_*$, где $Q$ --- выпуклое и замкнутое подмножество $\mathbb{R}^n$, $X_*$ --- компакт и $\|\cdot\|_2$ --- евклидова норма. 
    При таком допущении моими соавторами в работе \cite{sharp22} в теореме 1 был предложен субградиентный метод с гарантией  линейной скорости сходимости в случае доступности информации о точном значении $f^*$ \cite{6} без использования в теоретических оценках скорости сходимости  параметра размерности пространства. Условие острого минимума верно, например, для задачи проектирования точки на выпуклый компакт. Однако требование доступности $f^*$ довольно ограничительно. В этой связи рассматривалось некоторое обобщение условия острого минимума
    \begin{gather}\label{eq_gen_sharp}
    f(x) - \overline{f} \geq \alpha \min_{x_* \in X_*} \|x - x_* \|_2 - \Delta,
    \end{gather}
    где $\overline{f}$  --- это некоторое приближение минимального значения функции $f^*$, причём $\overline{f} \geq f^*$. Такое обобщение позволяет несколько расширить класс применимости субградиентных методов для задач с острым минимумом и шагом Б.Т. Поляка. Например, оно может покрыть постановку задачи с неточной информацией о $f^*$.  

    В данном пункте предложен анализ проведенного экспериментального сравнения двух субградиентных методов, использующих различные дополнительные условия:
    \begin{enumerate}
        \item сильной выпуклости,
        \item обобщенного условия острого минимума.
    \end{enumerate}
    Напомним, что в работе рассматриваются задачи вида
    \begin{gather}\label{min_q}
        f(x)\rightarrow\min_{x\in Q},
    \end{gather}
    где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^{n}$. Для субградиентного метода вида
    \begin{gather}\label{orig}
        x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
    \end{gather}

    Приведем без доказательства результат, полученный моими коллегами, (упомянутая ранее теорема 1 из \cite{sharp22}) который необходим для дальнейшего анализа экспериментально полученных результатов.
    \begin{theorem}\label{theorem1}
        Пусть $f$ --- слабо $\beta$-квазивыпуклая функция и для задачи \eqref{min_q} с условием \eqref{eq_gen_sharp} используется метод \eqref{orig} c шагом
        $h_k = \dfrac{\beta(f(x_k) - \overline{f})}{\| \nabla f(x_k) \|_2^2}$. Пусть также $\forall i \geq 0$ верно $\alpha^2 \beta^2 \leq 2 \| \nabla f(x_i) \|_2^2$. Тогда верно неравенство:
        \begin{gather}\label{adaptive_estimate}
            \begin{aligned}
            \min_{x_* \in X_*} \|x_{k+1} - x_* \|_2^2 \leq &  \prod_{i=0}^k \left ( 1 - \frac{\alpha^2\beta^2}{2 \| \nabla f(x_i) \|_2^2} \right ) \min_{x_* \in X_*} \|x_0 - x_* \|_2^2 + \\& 
            \qquad \qquad + \sum_{i=0}^{k-1} \prod_{j=i+1}^k \left ( 1 - \frac{\alpha^2\beta^2}{2 \| \nabla f(x_j) \|_2^2} \right )\Delta_i + \Delta_k,
            \end{aligned}
        \end{gather}
        где $\Delta_k = \frac{\Delta^2}{2 \| \nabla f(x_k) \|_2^2}$ для всякого $k \geqslant 0$.
    \end{theorem}

    Для сравнения воспользуемся улучшенной версией следующей оценки скорости сходимости \cite{Bach_2012}:
    \begin{equation}\label{orig_estimation_f}
        f(\widehat{x}) - f(x_*) \leq \frac{2 M^2}{\mu (N+1)}  \; \text{  при   } \; \widehat{x} = \sum\limits_{k=1}^{N} \frac{2 k}{N (N+1)} x_k, 
    \end{equation}
    где $M$ --- константа Липщица целевой функции $f$.

    Данную оценку можно несколько улучшить на классе сильно выпуклых задач, что было описано в \cite{Stonyakin_2021}. Воспользуемся рассуждением, проведенным для вариационных неравенств в замечании \ref{remark4} и приведем соответствующий известный результат:
    \begin{theorem}\label{ThmBachAdaptive}
        Пусть $f$ --- $\mu$-сильно выпуклая функция. Тогда после $N$ итераций алгоритма:
        $$
            x_{k+1} := Pr_{Q}\{x_k - h_k \nabla f(x_k) \}, \;\; \textit{где} \; h_k = \frac{2}{\mu (k+1)}
        $$
        будет верно неравенство:
        \begin{equation}\label{adaptive_estimation_f}
            f(\widehat{x}) - f(x_*) \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1},
        \end{equation}
        где
        $$
            \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
        $$
        Если $f$ ещё и $M$-липшицева при $M >0$, то
        $$
             f(\widehat{x}) - f(x) \leq \varepsilon
        $$
        после $N = \mathcal{O}(\frac{M^2}{\mu\varepsilon})$ итераций алгоритма \eqref{orig}.
    \end{theorem}

    Отметим, что если $x_*$ --- точное решение задачи минимизации $f$, то можно получить оценку скорости сходимости по аргументу вида
    \begin{equation} \label{arg_est}
        \|\widehat{x} - x_*\|_2 \leq \frac{4}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|\nabla f(x_k)\|_2^2}{k+1} \leq \frac{4M^2}{\mu(N+1)}.
    \end{equation}

    Полученный в теореме \ref{ThmBachAdaptive} результат применим и в случаях, когда константа Липщица ($M$) --- бесконечна или её значение сложно оценить. Более того, данный подход может быть распространён на важные прикладные задачи, среди которых задача бинарной классификации методом опорных векторов (SVM) \cite{Bach_2012}. По аналогии с работой \cite{Bach_2012} можно применять стохастический вариант зеркального спуска \eqref{orig}. Также отметим, что данные рассуждения и метод аналогичны описанным в предыдущей главе обобщениям на класс вариационных неравенств, лагранжевых и седловых задач. 

    Для сравнения скорости сходимости метода \cite{Bach_2012} и полученной в теореме \ref{ThmBachAdaptive} оценки с результатом, полученным моими соавторами, в теореме \ref{theorem1} для задач с $\Delta$-острым минимумом проведены численные эксперименты для задачи о наименьшем покрытии точек шаром для $2$-сильно выпуклой функции
    \begin{gather}\label{sphere_cover_strongly}
        f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2^2, \|x - a_1\|_2^2, ..., \|x - a_m\|_2^2\right\},
    \end{gather}
    а также для не сильно выпуклой (но выпуклой) функции
    \begin{gather}\label{sphere_cover}
        f(x) := \max_{x\in Q}\left\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\right\}.
    \end{gather}

    Начнём с иллюстрации преимуществ адаптивной оценки метода \cite{Bach_2012} из теоремы \ref{ThmBachAdaptive}. Будем рассматривать множество Q, которое равно евклидову шару с центром в 0. Начальная точка выбиралась случайно, но внутри Q. На рис. \ref{res_ex_strong_r5} ниже показано поведение и характер убывания для оригинальной оценки (\ref{orig_estimation_f}) --- сплошная линия, адаптивной оценки (\ref{adaptive_estimation_f}) --- штрих-пунктирная линия и непосредственно невязки по функции и по аргументу соответственно --- штриховая линия. На рис. \ref{res_ex_strong_r5} показано поведение глобальной оценки, адаптивной и невязки по функции и аргументу в случае ограниченного $Q (R = 5)$. Данный график наглядно демонстрирует, насколько более точной может оказаться адаптивная оценка (\ref{adaptive_estimation_f}) для задачи \eqref{sphere_cover_strongly}. 

    \begin{figure}[h]
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{x_discr_rad_5_q_4_it_70_000_dim_1000.png}
        \endminipage\hfill
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{f_discr_rad_5_q_4_it_70_000_dim_1000.png}
        \endminipage\hfill
        \caption{Результаты решения задачи минимизации \eqref{sphere_cover_strongly}, учитывающей сильную выпуклость, где  $n= 1\,000, r = 5$ и  шар $Q$ радиуса 4.}
        \label{res_ex_strong_r5}
    \end{figure}

    Теперь перейдём к выпуклой постановке \eqref{sphere_cover} с целью исследования эффективности предложенного в теореме \ref{theorem1} субградиентного метода с $\Delta$-острым минимумом. К существующему набору точек, представленных для покрытия, с известным значением центра добавим дополнительную точку, которая находится вне исходного шара достаточно близко к границе (удалена не более, чем на $\Delta > 0$). Данный подход позволяет оценить <<приближённое>> значение минимума $\overline{f}$, что позволит применить указанный вариант субградиентного метода с $\Delta$-острым минимумом. При этом новое значение минимума останется внутри исходной сферы. Поскольку оптимальное значение функции --- это радиус искомого шара, покрывающего все точки, а $x_*$ всегда будет расположена внутри него, то для всякого $x$ верно неравенство $ f(x) \geq \| x - x_*\|_2$. Рассмотрим целевую функцию вида
    \begin{gather}\label{allpha_sphere_cover}
        f(x) := \alpha \max_{x\in Q}\{\|x - a_0\|_2, \|x - a_1\|_2, ..., \|x - a_m\|_2\}.
    \end{gather}
    Тогда значение $\Delta$ можно оценить  из (\ref{eq_gen_sharp}): 
        $f(x) - \overline{f} \geq \alpha\|x- x_*\|_2 - \Delta, \quad \Delta \geq \overline{f}$.

    Отметим, что данная постановка значительно влияет на величину теоретической оценки качества решения (\ref{adaptive_estimate}) для метода \eqref{orig}.
    Наиболее значимый вклад в оценку (\ref{adaptive_estimate}) дает последнее слагаемое $\frac{\Delta^2}{2\|\nabla f(x_k)\|^2_2}$, причём 
    $     \Delta \sim \overline{f} \sim \alpha \|\overline{x}-a\|_2 $ и 
    $     \|\nabla f(x_k)\|_2 = \alpha $. Поэтому последнее слагаемое пропорционально радиусу шара, соответствующему <<приближённому>> решению. Это и подтверждается экспериментально. Для сравнения, ниже на рис. \ref{res_sharp_convex} и \ref{res_strong_convex} приведены результаты работы для того же набора входных точек, которые необходимо покрыть в обоих постановках --- (\ref{allpha_sphere_cover}) и (\ref{sphere_cover_strongly}). Начальная точка также одна и та же. Сравниваются методы из теоремы \ref{theorem1} и \eqref{orig}. Первый из этих методов обеспечивает сходимость буквально за 10 итераций к <<приближённому>> решению с заданной точностью и даже позволяет эту точность повысить. Второй же метод достигает схожих (с геометрической точки зрения) результатов за значительно большее количество итераций, однако он позволяет повышать точность приближённого решения на дальнейших итерациях.

    Подтверждение данного теоретического наблюдения хорошо иллюстрируется на рис. \ref{res_sharp_convex} и \ref{res_strong_convex}. На рис. \ref{res_sharp_convex} показано поведение субградиентного спуска, использующего $\Delta$-острый минимум (теорема \ref{theorem1}), а именно --- быстрая сходимость к <<приближенному>> решению. Штрих-пунктирная линия соответствует оценке \eqref{eq_gen_sharp}, а штриховая --- невязке по функции и аргументу. На рис. \ref{res_strong_convex} показано поведение метода для той же задачи, но с использованием сильно выпуклого целевого функционала (теорема \ref{ThmBachAdaptive}). Скорость убывания уже не столь высокая, но точность получаемого решения в итоге выше. Сплошная линия --- это глобальная оценка \eqref{orig_estimation_f}, штрих-пунктирная --- адаптивная \eqref{adaptive_estimation_f}, а штриховая --- невязка по функции и аргументу.

    \begin{figure}[h]
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{sharp_convex_x.png}
        \endminipage\hfill
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{sharp_convex_f.png}
        \endminipage\hfill
        \caption{ Результаты решения задачи минимизации (\ref{allpha_sphere_cover}), учитывающей условие острого минимума, где  $n= 1\,000, r = 0.7525, \alpha = 0.6$.}
        \label{res_sharp_convex}
    \end{figure}

    \begin{figure}[h]
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{strong_convex_small_rad_x.png}
        \endminipage\hfill
        \minipage{0.49\textwidth}
        \includegraphics[width=\linewidth]{strong_convex_small_rad_f.png}
        \endminipage\hfill
        \caption{ Результаты решения задачи минимизации (\ref{sphere_cover_strongly}), учитывающей сильную выпуклость, где  $n= 1\,000, r = 0.7525$.}
        \label{res_strong_convex}
    \end{figure}

    Тем не менее, сравнение с известным точным решением $x_*$, а также график динамики значения целевой функции показывает, что за малое число шагов (значительно меньшее, чем для метода \eqref{orig}) реализация метода, описанного в теореме \ref{theorem1}, приводит к неплохому качеству приближённого решения. При этом, однако, для метода, учитывающего $\Delta$-острую постановку, после достижения такого уровня дальнейшее повышение качества выходной точки в отличие от метода \eqref{orig} уже не наблюдается. 

    Данные результаты привели к идее объединения данных подходов для достижения лучших результатов в скорости сходимости без потери возможности уточнения.

\FloatBarrier