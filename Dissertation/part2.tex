\chapter{Методы типа зеркального спyска для относительно липшицевых и относительно сильно выпyклых задач}\label{ch:ch2}

\section{Введение}\label{sec:ch2/sec1}

    Численные методы градиентного типа достаточно часто используются для самых разнообразных постановок задач выпуклой оптимизации в пространствах больших размерностей. Это объясняется небольшими затратами памяти на итерациях, а также возможностью обоснования приемлемых оценок скорости сходимости, не содержащих (в отличие, например, от методов отсекающей гиперплоскости) параметров размерности пространства. Однако при этом существенны предположения о функциональных свойствах таких задач (гладкость, липшицевость, сильная выпуклость). Так, несколько лет назад был выделен класс относительно гладких задач оптимизации (см., например \cite{Bauschke,Drag,Lu_Nesterov_2018}). Свойство относительной $L$-гладкости ($L > 0$) обобщает ycловие $L$-гладкости ($L$-липшицевости градиента)  $f$ путём замены в известном для $L$-гладкий фyнкций $f$ неравенстве ($Q$ --- область определения $f$)
    $$
    	f(y) \leq f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{L}{2} \|x - y \|_2^2 \quad   \forall x, y \in Q
    $$	
    выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией (расхождением) Брэгмана (см. \eqref{Brg_form} и \eqref{funct_rel_smooth} ниже), которая порождается некоторой выпуклой прокс-функцией (важно, что она не обязательно сильно выпукла). Отметим, что здесь и всюду далее $\|\cdot\|_2$ --- евклидова норма в $n$-мерном пространстве $\mathbb{R}^n$.

    Для выпуклых относительно гладких задач которых оценки сходимости обычных (неускоренных) методов градиентного типа оптимальны с точностью до умножения на константу, не зависящую от размерности и параметров метода (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также имеющиеся в них ссылки). В работе \cite{Lu_Nesterov_2018} введено понятие относительной сильной выпуклости функции, которое позволило расширить класс выпуклых оптимизационных задач, для которых можно доказать линейную скорость сходимости (сходимость со скоростью геометрической прогрессии) метода градиентного типа, причём соответствующая оценка не содержит параметров размерности задачи. В данной работе мы развиваем этот подход и исследуем некоторые алгоритмы уже для вариационных неравенств с аналогом относительной сильной выпуклости для операторов (относительной сильной монотонностью). Напомним, что понятие относительной сильной выпуклости \cite{Lu_Nesterov_2018} функции $f$ обобщает понятие обычной $\mu$-сильной выпуклости $f$ ($\mu > 0$) путём замены в неравенстве 
    \begin{equation}
    	f(x) + \langle \nabla{f(x)}, y - x \rangle  + \frac{\mu}{2} \|x - y \|_2^2 \leq f(y) \quad   \forall x, y \in Q,
    	\end{equation}
    выражения $\frac{1}{2} \|x - y \|_2^2 $ дивергенцией Брэгмана (см. \eqref{Brg_form} и \eqref{eqrelativestorngconv} ниже), которая порождается некоторой выпуклой прокс-функцией. 

    В настоящей статье рассматриваются методы первого порядка для двух классов вариационных неравенств с операторами, удовлетворяющими предлагаемому аналогу условия относительной сильной выпуклости (см. ниже определение  \ref{DefRelStrongMonot} относительной сильной монотонности оператора): с аналогом ограниченности (относительная ограниченность, см. определение 2 ниже), а также с аналогом условия Липшица (относительная гладкость, см. определение 3 ниже).

    Хорошо известно, что на классе липшицевых и сильно выпуклых минимизационных задач оптимальная оценка скорости сходимости достигается именно для субградиентного метода \cite{Bach_2012}. В последние годы активно исследуются задачи с аналогом условия Липшица относительно некоторой выпуклой прокс-функции (относительная липшицевость), которая, в отличие от классической постановки, не обязана удовлетворять условию сильной выпуклости относительно нормы \cite{AdaMirr_2021,Lu_2018,Zhou_NIPS_2020}. Мы исследуем оценку скорости сходимости субградиентного метода для сильно выпуклых задач с аналогичным предположением об относительной липшицевости. Точнее говоря, в настоящей статье рассматривается вариант субградиентного метода на классе относительно ограниченных и относительно сильно монотонных вариационных неравенств, а также класс относительно сильно выпукло-вогнутых седловых задач с соответствующими условиями относительной липшицевости функционалов. 

    Далее, немалую популярность в работах по оптимизации получило упомянутое выше недавно предложенное понятие относительной гладкости функций (см. работы \cite{Bauschke,Drag,Dragomir,Lu_Nesterov_2018}, а также приведённые в них ссылки), которое позволило существенно расширить класс задач выпуклой оптимизации по сравнению со стандартным предположением о липшицевости градиента с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$ (здесь и далее $N$ --- количество итераций), которая может считаться оптимальной для такого широкого класса задач \cite{Dragomir}. В плане приложений можно отметить подход к построению методов градиентного типа для задач распределенной оптимизации с использованием относительной гладкости и относительной сильной выпуклости \cite{Hendr}. Аналоги относительной гладкости введены в последние пару лет и для более общей постановки задачи решения вариационного неравенства (см. \cite{Inex}, а также имеющиеся там ссылки) с монотонным оператором. Оказывается, что для этого класса задач можно предложить алгоритмы экстраградиентного типа с гарантией оценки скорости сходимости $\mathcal{O}(N^{-1})$. Мы же рассматриваем класс относительно сильно монотонных и относительно гладких операторов и показываем, как некоторая вариация адаптивного проксимального зеркального метода \cite{UMP} со специальной организацией процедуры рестартов (перезапусков) может приводить к обоснованию лучшей оценки скорости (линейной скорости сходимости) для вариационных неравенств с такими предположениями. Стоит отметить, что метод адаптивен, т.е. в оценке скорости сходимости глобальный параметр относительной гладкости задачи можно заменить его адаптивно подбираемыми на итерациях потенциально более удобными локальными аппроксимациями. Предлагаемый нами подход, в частности, дал возможность впервые предложить метод с адаптивной настройкой на итерациях параметра относительной гладкости для выделенного в \cite{Hendr} класса задач распределённой оптимизации.

    Работа состоит из введения, трех основных частей (пунктов) и заключения. Второй пункт статьи посвящён модификации метода зеркального спуска и выводу оценки его скорости сходимости для вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами. В частности, полученная оценка указывает на оптимальность такого метода на выделенном классе вариационных неравенств, поскольку она оптимальна (с точностью до умножения на не зависящую от параметров метода и размерности пространства константу) даже на более узком классе задач минимизации относительно липшицевых и относительно сильно выпуклых функций \cite{Lu_2018}. В третьем пункте статьи рассматривается класс относительно сильно монотонных и относительной гладких операторов и анализируется возможность использования рестартованного адаптивного проксимального зеркального метода для такого класса задач с обоснованием гарантии линейной скорости сходимости. В четвертом пункте показывается, как предложенные ранее алгоритмы для вариационных неравенств и полученные теоретические оценки их скорости сходимости могут быть применены для решения относительно сильно выпукло-вогнутых седловых задач с соответствующими предположениями о гладкости функционалов.

\section{Субградиентный метод для вариационных неравенств с относительно сильно монотонными и ограниченными операторами}\label{sec:ch2/sec2}

    Будем рассматривать задачу нахождения решения $x_*$ (также называемого слабым решением) вариационного неравенства: 
    \begin{equation}\label{eq:1}
        \max_{x \in Q} \langle g(x), x_* - x \rangle \leq 0,
    \end{equation}
    где $Q$ --- выпуклое замкнутое подмножество $\mathbb{R}^n$,
    $g: Q \longrightarrow \mathbb{R}^n$. Предположим, что удовлетворяющее \eqref{eq:1} решение $x_*$ существует.

    Всюду далее будем предполагать, что нам доступна некоторая выпуклая (вообще говоря, не сильно выпуклая) дифференцируемая прокс-функция $d$, порождающая расстояние, а также соответствующая ей дивергенция (расхождение) Брэгмана \cite{Bauschke}
    \begin{equation}\label{Brg_form}
        V(y, x) = d(y) - d(x) - \langle \nabla d(x), y - x \rangle.
    \end{equation}

    Введём следующий аналог понятия относительной сильной выпуклости функции \cite{Lu_Nesterov_2018} для вариационных неравенств.
    \begin{definition}\label{DefRelStrongMonot}
        Назовём оператор $g$ относительно $\mu$-сильно монотонным, где $\mu >0$, если для всяких $x, y \in Q$ верно неравенство
            \begin{equation}\label{eq:3}
                 \mu V(y, x) + \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
             \end{equation}
    \end{definition}
    Как правило, далее мы будем использовать следующее неравенство, естественно вытекающее из \eqref{eq:3}.
    \begin{remark}
        Если оператор $g$ является  относительно $\mu$-сильно монотонным, то для всяких $x, y \in Q$ верно неравенство
        $$
            \mu V(x, y) \leq \langle g(y) - g(x), y - x \rangle.
        $$
    \end{remark}

    Поясним на примере, почему относительная сильная монотонность вводится именно согласно  \eqref{eq:3}.
    \begin{example}
        Если $f: Q \longrightarrow \mathbb{R}$ --- относительно $\mu$-сильно выпуклая функция
        \begin{equation}\label{eqrelativestorngconv}
            f(x) - f(y) + \mu V(x, y) \leq \langle \nabla{f(x)}, x - y \rangle \quad   \forall x, y \in Q,
        \end{equation}
        то
        \begin{equation}
            f(y) - f(x) + \mu V(y, x) \leq \langle \nabla{f(y)}, y - x \rangle \quad   \forall x, y \in Q.
        \end{equation}
        После сложения двух последних неравенств получаем
        \begin{align*}
            \mu V(x, y) + \mu V(y, x)\leq \langle \nabla{f(y)} - \nabla{f(x)}, y - x \rangle \quad  \forall x, y \in Q.
        \end{align*}
        Таким образом, неравенство \eqref{eq:3} верно при $g(x) = \nabla{f(x)}$, где $\nabla{f(x)}$ --- произвольный субградиент $f$.
    \end{example}

    Относительно сильно выпуклые функционалы возникают в самых разных ситуациях, среди которых мы упомянем задачу централизованной распределённой минимизации эмпирического риска в предположении схожести слагаемых \cite{Hendr}.

    \begin{example}{Минимизация эмпирического риска \cite{Hendr}.}\label{min_risk}\\
        Рассмотрим задачу минимизации эмпирического риска
        \begin{equation}\label{EmpirProbl}
            F(x)=\frac{1}{m} \sum_{j=1}^{m} f_{j}(x)=\frac{1}{n m} \sum_{j=1}^{m} \sum_{i=1}^{n} \ell\left(x, z_{i}^{(j)}\right)
            \rightarrow\min\limits_{x\in Q},
        \end{equation}
        \begin{equation}
            F(x)=\frac{1}{N}\sum\limits_{i=1}^N \ell(x,z_i),
        \end{equation}
        в предположении, что исходные данные представляют из себя набор $n$ выборок, каждая из которых хранится на одном из $m$ серверов. При этом для достаточно большого $n$ все $f_j$ есть $\mu$-сильно выпуклые и $L$-гладкие (удовлетворяют условию Липшица градиента с константой $L > 0$) функционалы, которые можно считать статистически схожими \cite{Hendr}. Такая схожесть может быть описана в виде предположения \cite{Hendr} о том, что для всякого $x$
        $$
            \|\nabla^2 F(x) - \nabla^2 f_j (x)\|_2 \leq \delta
        $$
        при всяком $j$ для некоторого достаточно малого $\delta >0$, причём здесь и всюдy далее $\|A\|_2 = \max\limits_{\|x\|_2 \leq 1}\|Ax\|_2$. При этом предполагается, что существует центральный сервер (ему соответствует функционал $\overline{f}$), на который передаётся информация о градиентах $f_j$ в текущей точке, но не передаётся информация о значениях $f_j$. В \cite{Hendr} показано, что при таком допущении можно ввести прокс-функцию
        \begin{equation}\label{prox_risk}
            d(x):= \overline{f}(x) + \frac{\delta}{2}\|x\|_2^2 
        \end{equation}
        и для соответствующей дивергенции Брэгмана $V(y, x) = d(y) - d(x) - \langle \nabla d(x), y -x \rangle$ фyнкция $F$ будет относительно $1$-гладкой и относительно $\frac{\mu}{\mu + 2\delta}$-сильно выпуклой, т.е. для всяких $x, y \in Q$ верны неравенства
        $$
            F(y) \leq F(x) + \langle \nabla F(x), y - x\rangle + V(y, x)
        $$
        и
        $$
            F(y) \geq F(x) + \langle \nabla F(x), y - x\rangle + \frac{\mu}{\mu + 2\delta} V(y, x).
        $$
            
        Это означает, что при $\delta \ll L$ можно улучшить оценку скорости сходимости неускоренного градиентного метода $$\mathcal{O}\left(\frac{L}{\mu}\log\frac{1}{\varepsilon}\right)$$
        для задач минимизации эмпирического риска (целевой функционал $L$-гладкий и $\mu$-сильно выпуклый) методами первого порядка до $$\mathcal{O}\left(\left[1+\frac{\delta}{\mu}\right]\log\frac{1}{\varepsilon}\right).$$
        При этом на каждой итерации метода, выполняемого центральным узлом, доступна информация о градиенте целевого функционала $F$, но не доступна информация о значении функционала $F$. Доступность информации о градиенте позволяет рассматривать поставленную задачу минимизации эмпирического риска как задачу отыскания решения вариационного неравенства с относительно гладким и относительно сильно монотонным оператором $g = \nabla F$.
    \end{example}

    В данной главе мы рассмотрим численные методы решения вариационных неравенств с операторами, удовлетворяющими условиям относительной ограниченности, а также относительной гладкости.
    \begin{definition}\label{DefRelBound}\cite{Main}
        Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $M$-ограниченным, где $M >0$, если для всяких $x, y \in Q$ верно неравенство
        \begin{equation}\label{rel_bound}
             \langle g(x), x - y \rangle \leq M\sqrt{2V(y,x)}.
         \end{equation}
    \end{definition}
    \begin{definition}\cite{Inex}
        Назовём оператор $g: Q \longrightarrow \mathbb{R}^n$ относительно $L$-гладким, где $L > 0$, если для всяких $x, y, z \in Q$ верно неравенство
        \begin{equation}\label{rel_smooth}
            \langle g(y)-g(z),x-z\rangle \leq LV(x,z) + LV(z,y).
        \end{equation}
    \end{definition}
    Отметим, что если функция $f$ $L$-относительно гладкая \cite{Bauschke}, т.е.
    \begin{equation}\label{funct_rel_smooth}
        f(y) \leq f(x) + \langle \nabla f(x), y - x\rangle + LV(y, x) \quad \forall x, y \in Q,
    \end{equation}
    то оператор $g(x) = \nabla f(x)$ удовлетворяет \eqref{rel_smooth}. Однако в случае непотенциального оператора $g$ условие \eqref{rel_smooth} не сводится, вообще говоря, к \eqref{funct_rel_smooth} для какой-нибудь функции $f$.

    Приведем еще примеры задач оптимизации с относительно сильно выпуклыми функционалами. Начнём с примера относительно гладкого и относительно сильно выпуклого функционала.

    \begin{example} (\cite{Lu_Nesterov_2018}) Пусть $\widehat{f}(x):=\frac{1}{4}\|E x\|_{2}^{4}+\frac{1}{4}\|A x-b\|_{4}^{4}+$ $\frac{1}{2}\|C x-d\|_{2}^{2}$, где $A, C$ и $E$ --- положительно определённые квадратные матрицы $n \times n$, а $b, d$ --- векторы размерности $n$. Можно показать, что $\widehat{f}$ --- относительно $L$-гладкая и $\mu$-относительно сильно выпукла относительно
        $
            d(x):=\frac{1}{4}\|x\|_{2}^{4}+\frac{1}{2}\|x\|_{2}^{2}
        $
        на множестве $Q=\mathbb{R}^{n}$, где $L=3\|E\|^{4}+3\|A\|^{4}+6\|A\|^{3}\|b\|_{2}+3\|A\|^{2}\|b\|_{2}^{2}+\|C\|^{2}$ и $\mu=\min \left\{\frac{\sigma_{E}^{4}}{3}, \sigma_{C}^{2}\right\}$. 
         $\sigma_{E}$ и $\sigma_{C}$ --- наименьшие собственные значения матриц $E$ и $C$.
    \end{example}

    Теперь приведём пример относительно липшицева и относительно сильно выпуклого функционала.

    \begin{example} (\cite{Zhou_NIPS_2020}) 
        Пусть $\widehat{f}(x) := \frac{1}{p} \|x\|_2^p$ для $p \geq 2$ и $ Q = [-\alpha, \alpha]^n, \; \alpha > 0$. Заметим, что $\nabla \widehat{f}(x) = \|x\|_2^{p - 2} x$ и $\nabla^2 \widehat{f}(x) = \|x\|_2^{p - 2} I + (p-2)\|x\|_2^{p - 4} x x^{T}$. Тогда $\widehat{f}$ является относительно $M$-липшицевой при $M = 1$ относительно $d(x) := \frac{1}{2p}\|x\|_2^{2p}$. При этом $\widehat{f}$ не сильно выпукла в обычном смысле, т.к. $\nabla^2 \widehat{f}(x) - \mu I$ ($I$ --- матрица с единицами на главной диагонали) не является положительно полуопределённой в окрестности 0 ни при каком $\mu >0$. Тем не менее, при 
        \begin{equation}\label{eq_mu}
            \mu = \frac{p-1}{(2p - 1)(\sqrt{n}\alpha)^p}  
        \end{equation}
        матрица $\nabla^2 \widehat{f}(x) - \mu \nabla^2 d(x)$ уже будет  положительно полуопределена, что означает относительную сильную выпуклость $\widehat{f}$.
        \label{ex_experiments}
    \end{example}



    Вслед за \cite{Bach_2012} предложим метод зеркального спуска \eqref{eq:4}, но уже для рассматриваемого в настоящей работе класса  вариационных неравенств с относительно сильно монотонными и относительно ограниченными операторами (определения \ref{DefRelStrongMonot} и \ref{DefRelBound}):
    \begin{equation} \label{eq:4}
        x_{k+1} := \arg \min_{x \in Q} \left\{ h_k \langle g(x_k), x \rangle + V(x, x_k)\right\},
    \end{equation}
    где
    $$
        h_k = \frac{2}{\mu(k+1)},\quad  \forall k= 0,1, 2, \ldots.
    $$

    Непосредственно можно проверить следующий вспомогательный результат для шага метода зеркального спуска \eqref{eq:4}.

    \begin{lemma}\label{th:base}
        Если для $g$ верно \eqref{rel_bound}, а $x_k$ и $x_{k+1}$ удовлетворяют \eqref{eq:4}, то для произвольного $x \in Q$ верно неравенство
        $$    
            h_k \langle g(x_k), x_k - x \rangle \leq \frac{h_k^2 M^2}{2} + V(x, x_k) - V(x, x_{k+1}).
        $$
    \end{lemma}
    \begin{proof}
        Если применить стандартное условие экстремума 1-го порядка ко вспомогательной минимизационной подзадаче \eqref{eq:4}, то можно проверить при $h_k >0$ для всякого $x\in Q$ справедливость неравенств
        $$
            h_k \langle g(x_k), x_k - x \rangle \leq h_k \langle g(x_k), x_k - x_{k+1} \rangle  + V(x, x_k) - V(x, x_{k+1}) -V(x_{k+1},x_k) \stackrel{\eqref{rel_bound}}{\leq}$$
        $$
            \leq h_kM\sqrt{2V(x_{k+1},x_k)}+ V(x, x_k) - V(x, x_{k+1}) -V(x_{k+1},x_k) \leq
        $$
        $$
            \leq \frac{h_k^2M^2}{2} + V(x, x_k) - V(x, x_{k+1}).
        $$%\qed
    \end{proof}
    Согласно лемме \ref{th:base}, получим, что при всяких $ k \geq 0$ и $x \in Q$ верно
    \begin{equation} 
        \langle g(x_k), x_k - x \rangle \leq \frac{h_k M^2}{2} + \frac{V(x, x_k)}{h_k} - \frac{V(x, x_{k+1})}{h_k}. 
    \end{equation}
    Далее, с учетом \eqref{eq:3}, получим 
    \begin{equation*}
        \langle g(x_k), x_k - x \rangle \geq  \langle g(x), x_k - x \rangle + \mu (V(x, x_k) + V(x_k, x)) \quad \forall x \in Q,
    \end{equation*}
    откуда при всяком $k \ge 0$ имеем:
    \[
    \begin{aligned} 
        2k\langle g(x), x_k - x \rangle +  2k\mu \left[V(x, x_k) + V(x_k, x)\right] &\leq  
        \frac{2k M^2}{\mu (k+1)} + \mu k (k+1)V(x, x_k) -  \\&
        - \mu k (k+1)V(x, x_{k+1}) \quad \forall x \in Q. 
    \end{aligned}
    \]
    Это означает, что
    \begin{equation}\label{eq:5}
    \begin{aligned} 
        2k\langle g(x), x_k - x \rangle +  2k\mu V(x_k, x) \leq   
        &\frac{2k M^2}{\mu (k+1)} + \mu k (k-1)V(x, x_k) -  \\& -
        \mu k (k+1)V(x, x_{k+1}) \quad  \forall x \in Q. 
    \end{aligned}
    \end{equation}
    Пусть алгоритм \eqref{eq:4} отработал $N$ шагов. Тогда можно просуммировать неравенства \eqref{eq:5} по $k$ от $1$ до $N$ и учесть, что $\frac{k}{k+1} \le 1$:
    \begin{equation}
        \sum_{k=1}^{N} \left[ 2k\langle g(x), x_k - x \rangle + \mu V(x_k, x) \right] \leq \frac{2NM^2}{\mu},
    \end{equation}
    откуда с учетом $2(1+2+...+N)=N(N+1)$:
    \begin{equation} \label{eq:122}
        \sum_{k=1}^{N} \left[ \frac{2k}{N(N+1)}\langle g(x), x_k - x \rangle + \mu V(x_k, x) \right] \leq \frac{2M^2}{\mu(N+1)} \quad \forall x \in Q.
    \end{equation}
    %откуда 
    %\begin{equation}\label{eq:122}
    %\sum_{k=1}^{N} \frac{2k}{N(N+1)}(\langle g(x), x_k - x \rangle) + \mu V(x_k, x)) 
    %\leq \frac{2M^2}{\mu(N+1)} \quad \forall x \in Q.
    %\end{equation}
    Если учесть, что $V(x_k, x) \geq 0 \quad \forall x \in Q, \; \forall k \ge 0$,
    то при
    $$
        \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k
    $$
    будет верно неравенство 
    \begin{equation} \label{eq:13}
        \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2 M^2}{\mu (N+1)} \leq \varepsilon,
    \end{equation}
    после $N = O\left(\frac{M^2}{\mu \varepsilon}\right)$ итераций алгоритма $\eqref{eq:4}$. Как известно, такая оценка сложности оптимальна даже на классе относительно липшицевых и относительно сильно выпуклых задач минимизации \cite{Lu_2018}. Это указывает на её оптимальность и для существенно более широкого класса относительно липшицевых и относительно сильно выпуклых задач минимизации, а значит и для рассматриваемого класса вариационных неравенств. 
    Таким образом, можно сформулировать следующий результат:
    \begin{theorem}\label{thm_MD_VI}
        Пусть $g$ --- $\mu$-относительно сильно монотонный и $M$-относительно ограниченный оператор. Тогда после $N$ итераций алгоритма: 
        $$ 
            x_{k+1} := \arg \min_{x \in Q} \{ h_k \langle g(x_k), x\rangle + V(x, x_k)\}, \;\;\; h_k = \frac{2}{\mu (k+1)}
        $$
        будет верно неравенство:
        \begin{equation}\label{eq:2}
            \max_{x \in Q} \langle g(x), \widehat{x} - x\rangle \leq \frac{2 M^2}{\mu (N+1)},
        \end{equation}
        где 
        $$
            \widehat{x} = \sum_{k=1}^{N} \frac{2 k}{N (N+1)} x_k.
        $$
    \end{theorem}

    \begin{remark}
        Если $x_*$ --- сильное решение рассматриваемого вариационного не\-равенства, то можно выписать оценку скорости сходимости и <<по аргументу>>, поскольку $\langle g(x_*), x_k - x_*\rangle \geq 0$. Тогда \eqref{eq:122} означает, что 
            \begin{equation} \label{eq:12}
            \begin{aligned} 
                \sum_{k=1}^{N} \frac{2k\mu V(x_k, x_*)}{N(N+1)} \leq \frac{2M^2}{\mu(N+1)} \quad  \forall x \in Q,
            \end{aligned}
            \end{equation}
        Если же прокс-функция $1$-сильно выпукла относительно нормы $\|\cdot\|$, то из \eqref{eq:12} вытекает следующая оценка:
            \begin{equation} 
            \begin{aligned} 
                \|x_* - x_k\|^2 \leq \frac{4M^2}{\mu(N+1)}.
            \end{aligned}
            \end{equation}
    \end{remark}
    \begin{remark} \label{remark4}
        Если прокс-функция $d$ является $1$-сильно выпуклой, то неравенство из леммы 1 можно уточнить:
        \begin{equation} \label{base_eq}
            h_k \langle g(x_k), x_k - x \rangle \leq \frac{h_k^2 \|g(x_k)\|_*^2}{2} + V(x, x_k) - V(x, x_{k+1}). 
        \end{equation}
        Тогда итоговая оценка \eqref{eq:13} приобретает следующий вид:
        \begin{equation}
            \max_{x \in Q} \langle g(x), \widehat{x} - x \rangle \leq \frac{2}{\mu N (N+1)} \sum_{k=1}^{N} \frac{k \|g(x_k)\|_*^2}{k+1} \leq \varepsilon.
        \end{equation}
        Правая часть предыдущего неравенства может оказаться существенно меньшей, чем для оценки \eqref{eq:2}.
    \end{remark}

\section{Оценки скорости сходимости для относительно сильно выпукло-вогнутых седловых задач} \label{sec:ch2/sec3}

    Хорошо известно, что вариационные неравенства с монотонными операторами естественно возникают при рассмотрении выпукло-вогнутых седловых задач, важных для самых разных прикладных проблем. Поэтому в данном пункте статьи мы покажем, как полученные в предыдущих пунктах результаты о методах для вариационных неравенств можно применить к седловым задачам вида
    \begin{equation}\label{eqsedlo}
        f^* = \min_{u \in Q_1} \max_{v \in Q_2} f(u, v),
    \end{equation}
    где $f$ --- относительно сильно выпукла по $u$ и относительно сильно вогнута по $v$.

    Как известно, необходимость решения вариационных неравенств мотивируется, в частности, как раз задачами вида \eqref{eqsedlo}. В качестве примера можно рассмотреть лагранжеву седловую задачу, порожденную задачей относительно сильно выпуклого программирования.  
        \begin{example} Рассмотрим задачу относительно сильно выпуклого программирования (все функционалы $\widehat{f}, g_1, g_2, ...$ относительно сильно выпуклы):
        \begin{equation}\label{problem_with_fun_constraints}
            \left\{\begin{array}{c}
            \min_{x \in Q} \widehat{f}(x), \\
            g_{1}(x), g_{2}(x), \ldots, g_{m}(x) \leq 0.
            \end{array}\right.
        \end{equation}
            
        Рассмотрим соответствующую \eqref{problem_with_fun_constraints} лагранжеву седловую задачу следующего вида
        \begin{equation}\label{lagrange_problem}
            \min_{x \in Q} \max_{ \boldsymbol{\lambda}= (\lambda_1, \ldots, \lambda_m)^T \in \mathbb{R}_+^m} L(x, \boldsymbol{\lambda}) :=  \widehat{f}(x) + \sum_{p=1}^{m} \lambda_p g_p(x) - \varepsilon \sum_{p=1}^m \lambda_{p}^2.
        \end{equation}
    \end{example}
    Для данного типа задач можно ввести такой аналог дивергенции Брэгмана \cite{Fedor_relative_adapuniv}:
    $$
        V_{\text{new}}\left((y, \boldsymbol{\lambda}), (x, \boldsymbol{\lambda}^{'})\right) = V(y,x) + \frac{1}{2} \left\|\boldsymbol{\lambda} - \boldsymbol{\lambda}^{'}\right\|_2^2, \quad  \forall y, x \in Q, \boldsymbol{\lambda},  \boldsymbol{\lambda}^{'} \in \mathbb{R}_+^m.
    $$
    введенная таким образом дивергенция позволяет ослабить требования к ограничениям $\boldsymbol{\lambda}$.

    Перейдём теперь к методике для нахождения приближённого решения задачи \eqref{eqsedlo}. Для всякого $\varepsilon > 0$ под $\varepsilon$-точным решением задачи \eqref{eqsedlo} будем понимать пару $(\widehat{u}, \widehat{v})$ такую, что $$\max_{v \in Q_2} f(\widehat{u}, v) - \min_{u \in Q_1} f(u, \widehat{v}) \leq \varepsilon.$$ Обозначим $x = (u, v), y = (z, t)$, а также введем оператор 
    \begin{equation}\label{operator-sedlo}
        g(x) := \Bigg( 
        \begin{aligned}
            f^{'}_{u}(u,v)\\
            -f^{'}_{v}(u,v)
        \end{aligned}
        \Bigg).
    \end{equation}
    Тогда ввиду выпукло-вогнутости $f$ имеем: 
    \begin{equation}
    \begin{aligned}
        \langle g(x), x - y \rangle &=
         \Bigg( 
        \begin{aligned}
            f^{'}_{u}(u,v)\\
            -f^{'}_{v}(u,v)
        \end{aligned}
        \Bigg)
         (u - z, v - t)  = \langle f^{'}_{u}(u,v), u - z \rangle - \langle f^{'}_{v}(u,v), v - t \rangle \geq \\&
         \geq f(u, v) - f(z, v) 
        - f(u, v)+ f(u, t)=  f(u,t) - f(z, v).
    \end{aligned}
    \end{equation}
    Будем предполагать относительную ограниченность оператора \eqref{operator-sedlo}. Тогда метод \eqref{eq:4} для задач \eqref{eqsedlo} приводит к оценкам вида:
    \begin{equation} \label{eq:21}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} \langle g(x_k), x_k -x\rangle \leq \frac{2 M^2}{\mu (N+1)}.
    \end{equation}
    Если $x_k = (u_k, v_k), \;\; x = (u, v)$, то  
    \begin{equation}
        \langle g(x_k), x_k -x\rangle \geq f(u_k,v) - f(u, v_k) \quad \forall (u, v).
    \end{equation}
    Далее, \eqref{eq:21} означает, что 
    \begin{equation}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k,v) - f(u, v_k)) \leq \frac{2M^2}{\mu (N+1)}.
    \end{equation}
    Положим
    \begin{equation}
        (\widehat{u}, \widehat{v}) := \frac{1}{N(N+1)} \sum_{k=1}^{N} 2k (u_k,v_k).
    \end{equation}
    Тогда получаем, что
    \begin{equation}
        \sum_{k=1}^{N} \frac{2k}{N(N+1)} (f(u_k, v) - f(u, v_k)) \geq f(\widehat{u}, v) - f(u, \widehat{v}), 
    \end{equation}
    откуда ввиду \eqref{eq:2} получаем для задачи \eqref{eqsedlo} следующую оценку
    \begin{equation}
        \max_{v} f(\widehat{u}, v) - \min_{u} f(u, \widehat{v}) \leq \frac{2M^2}{\mu (N+1)}.
    \end{equation}

\section{Численные эксперименты} \label{sec:ch2/sec4}

\FloatBarrier